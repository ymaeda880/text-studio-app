# -*- coding: utf-8 -*-
# pages/72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢.py
from __future__ import annotations

import io, re
from pathlib import Path
from typing import List, Iterable

import pandas as pd
import streamlit as st

# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(page_title="ğŸ” ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” 72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢")
st.caption("ãƒ•ã‚©ãƒ«ãƒ€: data/redlist/{fukushima, MOE, chiba} ã‚’èª­ã¿è¾¼ã¿ã€å’Œå/ç¨®åã‚’è¤‡æ•°èªã¾ã¨ã‚ã¦æ¨ªæ–­æ¤œç´¢ã—ã¾ã™ã€‚")

# =========================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆlib/redlist/loaders ã‹ã‚‰ï¼‰
# =========================================================
from lib.redlist.loaders import load_all, get_column_templates

DATA_ROOT = Path("data/redlist").resolve()
moe_df, fuku_df, chiba_df = load_all(DATA_ROOT)

# --- å„ã‚½ãƒ¼ã‚¹ã®ã‚«ãƒ©ãƒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
COL_TEMPLATES = get_column_templates()

with st.sidebar:
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    st.code(str(DATA_ROOT))
    st.markdown("**èª­ã¿è¾¼ã¿ä»¶æ•°ï¼ˆå‚è€ƒï¼‰**")
    st.write({
        "ç’°å¢ƒçœ": 0 if moe_df is None or moe_df.empty else int(moe_df.shape[0]),
        "ç¦å³¶çœŒ": 0 if fuku_df is None or fuku_df.empty else int(fuku_df.shape[0]),
        "åƒè‘‰çœŒ": 0 if chiba_df is None or chiba_df.empty else int(chiba_df.shape[0]),
    })

# =========================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼
# =========================================================
def normalize_name(s: str) -> str:
    if pd.isna(s):
        return ""
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’ã€å‰å¾Œç©ºç™½é™¤å»
    return str(s).replace("\u3000", " ").strip()

def parse_queries(raw: str) -> List[str]:
    """
    ã‚«ãƒ³ãƒãƒ»ç©ºç™½ï¼ˆåŠ/å…¨ï¼‰ãƒ»ã‚¿ãƒ–ãƒ»æ”¹è¡Œãƒ»ç©ºç™½è¡Œã§åˆ†å‰²ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆé †åºä¿æŒï¼‰ã€‚
    """
    if not raw:
        return []
    raw = raw.replace("\u3000", " ")
    parts = re.split(r"[,\s]+", raw)
    parts = [normalize_name(p) for p in parts if p and normalize_name(p)]
    seen = set()
    uniq: List[str] = []
    for p in parts:
        if p not in seen:
            seen.add(p)
            uniq.append(p)
    return uniq

def mask_for_queries(series: pd.Series, qs: Iterable[str], exact: bool) -> pd.Series:
    """
    series ã‚’æ­£è¦åŒ–ã—ãŸä¸Šã§ã€è¤‡æ•°ã‚¯ã‚¨ãƒªã® OR ãƒãƒƒãƒã‚’è¿”ã™ã€‚
    exact=True: å®Œå…¨ä¸€è‡´ï¼ˆ== ã® ORï¼‰ã€False: éƒ¨åˆ†ä¸€è‡´ï¼ˆcontains ã® ORã€å¤§å°ç„¡è¦–ï¼‰
    """
    s_norm = series.fillna("").map(normalize_name)
    if exact:
        return s_norm.isin(list(qs))
    s_low = s_norm.str.lower()
    mask = pd.Series(False, index=series.index)
    for q in qs:
        if not q:
            continue
        ql = q.lower()
        mask = mask | s_low.str.contains(re.escape(ql), na=False)
    return mask

# =========================================================
# å…¥åŠ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ï¼šæ¤œç´¢ãƒœã‚¿ãƒ³ã§å®Ÿè¡Œï¼‰
# =========================================================
st.subheader("ğŸ§­ å’Œå/ç¨®å æ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰")

with st.form("search_form"):
    col_q1, col_q2 = st.columns([3, 1])
    with col_q1:
        multi_query_raw = st.text_area(
            "å’Œåï¼ˆç’°å¢ƒçœ/ç¦å³¶ï¼‰ãƒ»ç¨®åï¼ˆåƒè‘‰ï¼‰ã‚’æ”¹è¡Œ/ã‚«ãƒ³ãƒ/ã‚¹ãƒšãƒ¼ã‚¹/ã‚¿ãƒ–ã§è¤‡æ•°å…¥åŠ›ã§ãã¾ã™",
            value="",
            height=120,
            placeholder="ä¾‹ï¼‰\nãƒ‹ãƒ›ãƒ³ã‚¶ãƒ«\nãƒ¤ãƒãƒ, ãƒˆã‚­\n  ãƒ¤ãƒ³ãƒãƒ«ã‚¯ã‚¤ãƒŠ"
        )
    with col_q2:
        mode_exact = st.toggle("å®Œå…¨ä¸€è‡´", value=True, help="OFFã«ã™ã‚‹ã¨éƒ¨åˆ†ä¸€è‡´ï¼ˆåŒ…å«ï¼‰ã§æ¤œç´¢ã—ã¾ã™ã€‚", key="mode_exact")
    do_search = st.form_submit_button("ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œ")

# ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸæ™‚ã ã‘ã‚¯ã‚¨ãƒªã‚’ç¢ºå®š
queries = parse_queries(multi_query_raw) if do_search else []
if do_search and queries:
    st.caption(f"ğŸ” æ¤œç´¢èªï¼š{', '.join(queries)}ï¼ˆ{len(queries)} èªï¼‰")

# =========================================================
# å„ã‚½ãƒ¼ã‚¹ã®ãƒ’ãƒƒãƒˆåé›†
# =========================================================
results: List[pd.DataFrame] = []
if do_search and queries:
    with st.spinner("ğŸ” æ¤œç´¢ä¸­ã§ã™â€¦"):
        # ç’°å¢ƒçœï¼šå’Œå
        if moe_df is not None and not moe_df.empty and "å’Œå" in moe_df.columns:
            mask = mask_for_queries(moe_df["å’Œå"], queries, mode_exact)
            hit = moe_df.loc[mask].copy()
            if not hit.empty:
                hit.insert(0, "ã‚½ãƒ¼ã‚¹", "ç’°å¢ƒçœ")
                cols = COL_TEMPLATES["moe"]
                for c in cols:
                    if c not in hit.columns: hit[c] = ""
                results.append(hit[cols])

        # ç¦å³¶çœŒï¼šå’Œå
        if fuku_df is not None and not fuku_df.empty and "å’Œå" in fuku_df.columns:
            mask = mask_for_queries(fuku_df["å’Œå"], queries, mode_exact)
            hit = fuku_df.loc[mask].copy()
            if not hit.empty:
                hit.insert(0, "ã‚½ãƒ¼ã‚¹", "ç¦å³¶çœŒ")
                cols = COL_TEMPLATES["fukushima"]
                for c in cols:
                    if c not in hit.columns: hit[c] = ""
                results.append(hit[cols])

        # åƒè‘‰çœŒï¼šç¨®å
        if chiba_df is not None and not chiba_df.empty and "ç¨®å" in chiba_df.columns:
            mask = mask_for_queries(chiba_df["ç¨®å"], queries, mode_exact)
            hit = chiba_df.loc[mask].copy()
            if not hit.empty:
                hit.insert(0, "ã‚½ãƒ¼ã‚¹", "åƒè‘‰çœŒ")
                cols = COL_TEMPLATES["chiba"]
                for c in cols:
                    if c not in hit.columns: hit[c] = ""
                results.append(hit[cols])
# =========================================================
# çµæœè¡¨ç¤º & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå†…éƒ¨ä¿å­˜ï¼‹æ¤œç´¢èªã®å…¥åŠ›é †ã§æ•´åˆ—ï¼‰
#   â€» æ¤œç´¢èªåˆ—ã¯ä½¿ç”¨ã›ãšã€ã€Œå’Œå/ç¨®åã€ã‚’çµ±ä¸€ã—ãŸâ€œå’Œåâ€ã‚’å…ˆé ­ã«è¡¨ç¤º
# =========================================================
if do_search and queries:
    def _norm(s: pd.Series) -> pd.Series:
        return s.fillna("").astype(str).str.replace("\u3000", " ").str.strip()

    # å…¥åŠ›é †ã®ç¶­æŒï¼ˆé‡è¤‡ã¯æœ€åˆã®ã¿æ¡ç”¨ï¼‰
    queries_ordered: list[str] = []
    seen = set()
    for q in queries:
        qn = q.replace("\u3000", " ").strip()
        if qn and qn not in seen:
            queries_ordered.append(qn); seen.add(qn)

    # ---- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆåˆ—ã‚†ã‚Œãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬è£œå®Œï¼‰ ----
    def first_existing_col(df: pd.DataFrame, candidates: list[str]) -> str | None:
        for c in candidates:
            if c in df.columns:
                return c
        return None

    def ensure_cols(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
        h = df.copy()
        for c in cols:
            if c not in h.columns:
                h[c] = ""
        return h[cols]

    # ---- å…±é€šå‡¦ç†ï¼šã‚½ãƒ¼ã‚¹ã”ã¨ã®ãƒ’ãƒƒãƒˆæŠ½å‡ºï¼ˆæ¤œç´¢èªåˆ—ã¯ä½œã‚‰ãªã„ï¼‰ ----
    def _hits_for_source(df: pd.DataFrame | None,
                         source_label: str,
                         name_candidates: list[str],
                         cols_template_key: str,
                         q: str) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()

        name_col = first_existing_col(df, name_candidates)
        if name_col is None:
            return pd.DataFrame()

        m = _norm(df[name_col]) == q
        h = df.loc[m].copy()
        if h.empty:
            return pd.DataFrame()

        # ã‚½ãƒ¼ã‚¹åˆ—ã‚’è¿½åŠ ï¼ˆå…ˆé ­å´ã«ä»˜ã‘ã‚‹ï¼‰
        h.insert(0, "ã‚½ãƒ¼ã‚¹", source_label)

        # ãƒ†ãƒ³ãƒ—ãƒ¬åˆ—ã‚’æº€ãŸã—ã¦ä¸¦ã¹æ›¿ãˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬å¤–ã®åˆ—ã¯ã“ã®æ™‚ç‚¹ã§è½ã¡ã‚‹ï¼‰
        cols = COL_TEMPLATES[cols_template_key]
        h = ensure_cols(h, cols)
        return h

    def _hits_for_query(q: str) -> pd.DataFrame:
        parts = []
        # ç’°å¢ƒçœï¼šå’Œåå„ªå…ˆï¼ˆå°†æ¥ã®ã‚†ã‚Œå¯¾å¿œã§å€™è£œã«â€œç¨®åâ€ã‚‚å…¥ã‚Œã‚‹ï¼‰
        parts.append(_hits_for_source(
            moe_df, "ç’°å¢ƒçœ", ["å’Œå", "ç¨®å"], "moe", q
        ))
        # ç¦å³¶çœŒï¼šå’Œåå„ªå…ˆ
        parts.append(_hits_for_source(
            fuku_df, "ç¦å³¶çœŒ", ["å’Œå", "ç¨®å"], "fukushima", q
        ))
        # åƒè‘‰çœŒï¼šç¨®åå„ªå…ˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ãŒç¨®åä¸­å¿ƒï¼‰
        parts.append(_hits_for_source(
            chiba_df, "åƒè‘‰çœŒ", ["ç¨®å", "å’Œå"], "chiba", q
        ))
        parts = [p for p in parts if p is not None and not p.empty]
        return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()

    with st.spinner("ğŸ“š æ¤œç´¢çµæœã‚’æ•´å½¢ä¸­â€¦"):
        blocks = [_hits_for_query(q) for q in queries_ordered]
        results_df = pd.concat([b for b in blocks if not b.empty], ignore_index=True) \
                      if any(not b.empty for b in blocks) else pd.DataFrame()

        # ä¿å­˜ï¼ˆå†…éƒ¨ï¼‰
        st.session_state["redlist_last_queries"] = queries_ordered
        st.session_state["redlist_last_results"] = results_df

    if results_df.empty:
        st.warning("ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.success(f"æ¤œç´¢ãƒ’ãƒƒãƒˆä»¶æ•°: {len(results_df):,} ä»¶ï¼ˆ{len(queries_ordered)}èªï¼‰")

        # ========= åˆ—ã®çµ±ä¸€ã¨ä¸¦ã¹æ›¿ãˆ =========
        # 1) ã€Œå’Œå/ç¨®åã€ã‚’â€œå’Œåâ€ã«çµ±ä¸€
        name_candidates = ["å’Œå", "ç¨®å"]
        exist_names = [c for c in name_candidates if c in results_df.columns]
        if exist_names:
            results_df["å’Œå"] = results_df[exist_names].bfill(axis=1).iloc[:, 0]
        else:
            results_df["å’Œå"] = ""

        # 2) ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ã‚’çµ±åˆï¼ˆç’°å¢ƒçœ/ç¦å³¶/åƒè‘‰/è¨˜å·ã®ã©ã‚Œã‹ï¼‰
        symbol_candidates = ["ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "è¨˜å·"]
        exist_symbols = [c for c in symbol_candidates if c in results_df.columns]
        if exist_symbols:
            results_df["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = results_df[exist_symbols].bfill(axis=1).iloc[:, 0]
        else:
            results_df["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = ""

        # 3) å…¥åŠ›èªã®é †ï¼ˆqueries_orderedï¼‰ã§æ•´åˆ—
        #    å’Œåã‚’æ­£è¦åŒ–ã—ã¦ order_map ã«ã‚ˆã‚‹ã‚­ãƒ¼ã§ä¸¦ã¹ã€å®‰å®šã‚½ãƒ¼ãƒˆ
        order_map = {q: i for i, q in enumerate(queries_ordered)}
        results_df["__name_norm"] = _norm(results_df["å’Œå"])
        results_df["__ord"] = results_df["__name_norm"].map(order_map)
        # "__ord" ãŒ NaNï¼ˆï¼å…¥åŠ›ã—ã¦ã„ãªã„å’Œåï¼‰ã®è¡ŒãŒã‚‚ã—ã‚ã‚Œã°å¾Œã‚ã¸
        # â€» ãƒ’ãƒƒãƒˆã¯ã™ã¹ã¦ä¸€è‡´æ¤œç´¢ãªã®ã§é€šå¸¸ã¯ NaN ã¯å‡ºã¾ã›ã‚“ãŒå®‰å…¨ç­–
        results_df["__ord"] = results_df["__ord"].fillna(len(queries_ordered) + 1)
        results_df = results_df.sort_values(["__ord"], kind="stable").drop(columns=["__ord", "__name_norm"])

        # 4) åŒã˜â€œå’Œåâ€ã®2è¡Œç›®ä»¥é™ã‚’ç©ºç™½åŒ–ï¼ˆè¦–èªæ€§å‘ä¸Šï¼‰
        dup_mask = results_df["å’Œå"].duplicated(keep="first")
        results_df.loc[dup_mask, "å’Œå"] = ""

        # 5) åˆ—é †ã‚’å†æ§‹æˆï¼šå’Œå â†’ ã‚½ãƒ¼ã‚¹ â†’ ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å· â†’ ãã®ä»–
        first_cols = [c for c in ["å’Œå", "ã‚½ãƒ¼ã‚¹", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] if c in results_df.columns]
        other_cols = [c for c in results_df.columns if c not in first_cols]
        results_df = results_df[first_cols + other_cols]

        # ========= è¡¨ç¤º =========
        st.dataframe(results_df, use_container_width=True)

        # ========= Excelï¼ˆ1ã‚·ãƒ¼ãƒˆï¼‹Countsã‚·ãƒ¼ãƒˆï¼‰ =========
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
            # ãƒ¡ã‚¤ãƒ³ã‚·ãƒ¼ãƒˆï¼ˆç¾åœ¨ã®åˆ—é †ã®ã¾ã¾ï¼‰
            results_df.to_excel(writer, sheet_name="Search_Results", index=False)

            # é›†è¨ˆã‚·ãƒ¼ãƒˆï¼ˆå’Œå Ã— ã‚½ãƒ¼ã‚¹ï¼‰
            if not results_df.empty and {"ã‚½ãƒ¼ã‚¹","å’Œå"}.issubset(results_df.columns):
                cnt = results_df.copy()
                # ç©ºç™½åŒ–ã•ã‚ŒãŸå’Œåã¯å‰ã®å€¤ã§åŸ‹ã‚ã¦ã‹ã‚‰é›†è¨ˆ
                cnt["å’Œå"] = cnt["å’Œå"].replace("", pd.NA).fillna(method="ffill")
                cnt = cnt.groupby(["å’Œå","ã‚½ãƒ¼ã‚¹"]).size().reset_index(name="ä»¶æ•°")

                # queries_ordered ã®é †ã«ä¸¦ã¹æ›¿ãˆï¼ˆå’Œåã‚’æ­£è¦åŒ–ã—ã¦ order_mapï¼‰
                cnt["__name_norm"] = cnt["å’Œå"].astype(str).str.replace("\u3000", " ").str.strip()
                cnt["__ord"] = cnt["__name_norm"].map(order_map).fillna(len(queries_ordered) + 1)
                cnt = cnt.sort_values(["__ord","ã‚½ãƒ¼ã‚¹"]).drop(columns=["__ord","__name_norm"])

                cnt.to_excel(writer, sheet_name="Counts", index=False)

        st.download_button(
            "ğŸ“¥ æ¤œç´¢çµæœã‚’Excelã§ä¿å­˜",
            data=buf.getvalue(),
            file_name="redlist_search_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
else:
    st.info("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ã€ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


# =========================================================
# å‚™è€ƒ
# =========================================================
with st.expander("â„¹ï¸ å‚™è€ƒ", expanded=False):
    st.markdown("""
- **è¤‡æ•°èªå…¥åŠ›**ï¼šã‚«ãƒ³ãƒãƒ»ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆåŠ/å…¨ï¼‰ãƒ»ã‚¿ãƒ–ãƒ»æ”¹è¡Œãƒ»ç©ºç™½è¡Œã§åŒºåˆ‡ã‚Œã¾ã™ã€‚  
- **å®Œå…¨ä¸€è‡´**ï¼ˆæ—¢å®šï¼‰/ **éƒ¨åˆ†ä¸€è‡´** ã‚’ãƒˆã‚°ãƒ«ã§åˆ‡æ›¿ã§ãã¾ã™ã€‚  
- ç’°å¢ƒçœãƒ»ç¦å³¶ã¯ **ã€Œå’Œåã€**ã€åƒè‘‰ã¯ **ã€Œç¨®åã€** ã‚’æ¤œç´¢å¯¾è±¡ã«ã—ã¦ã„ã¾ã™ã€‚  
- è¡¨ç¤ºãƒ»ä¿å­˜ã®åˆ—åã¯ **å‚ç…§ãƒ©ãƒ™ãƒ«** ã§çµ±ä¸€ã—ã¦ã„ã¾ã™ã€‚  
- å–ã‚Šè¾¼ã¿ãƒ»æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã¯ `lib/redlist/loaders.py` ã«ä¾å­˜ã—ã¾ã™ã€‚  
    """)
