# pages/20_æ ¡æ­£.py â€” è§£æï¼ˆæ ¡æ­£æ–¹é‡ï¼šãƒšãƒ¼ã‚¸/è¡Œ/ç†ç”±ï¼‰â†’ æœ¬æ ¡æ­£
# ï¼ˆPDFãŒç”»åƒPDFãªã‚‰è­¦å‘Šã—ã¦åœæ­¢ï¼OCRæ©Ÿèƒ½ã¯å‰Šé™¤ï¼‰
from __future__ import annotations
from typing import List, Tuple, Dict, Iterable
from pathlib import Path
import sys
import io
import datetime as _dt

import streamlit as st
from openai import OpenAI

# ===== å…±æœ‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆcommon_libï¼‰ã‚’ãƒ‘ã‚¹ã«è¿½åŠ  =====
PROJECTS_ROOT = Path(__file__).resolve().parents[3]  # or 3 for pages
if str(PROJECTS_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECTS_ROOT))

# æ–™é‡‘è¦‹ç©ã‚Šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
from common_lib.openai.costs import (
    ChatUsage,
    estimate_chat_cost,
    render_chat_only_summary,
    DEFAULT_USDJPY,
)

# === lib ã®åˆ©ç”¨ï¼ˆèª­è¾¼ / è²¼ã‚Šä»˜ã‘æ•´å½¢ï¼‰ â€»OCRãªã— ===
from lib.text_loaders import (
    load_text_generic, extract_pdf_text, load_text_from_paste
)

# ------------------------------------------------------------
# ãƒ¢ãƒ¼ãƒ‰å®šç¾©
# ------------------------------------------------------------

MODE_DEFS: Dict[str, Dict[str, str]] = {
    "å³æ ¼æ ¡æ­£": {
        "desc": "åŠ©è©ãƒ»ä¸»è¿°ä¸€è‡´ãƒ»å†—é•·/é‡è¤‡ãƒ»èªé †ãƒ»èª¤å­—è„±å­—ãƒ»ç”¨èªèª¤ç”¨ãƒ»æ–‡ä½“ä¸çµ±ä¸€ã¾ã§åºƒãå¯¾è±¡ã€‚æ„å‘³ã¯å¤‰ãˆãšæœ€é©åŒ–ã€‚",
        "analyze_inst": (
            "ã‚ãªãŸã¯å³å¯†ãªæ—¥æœ¬èªæ ¡æ­£ãƒªãƒ¼ãƒ€ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ç•ªå·ä»˜ããƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ã€"
            "ã€ä½•ã‚’ã©ã®ã‚ˆã†ã«ç›´ã™ã¹ãã‹ã€ã‚’ã€å…·ä½“çš„ãªç†ç”±ã¨ã¨ã‚‚ã«ä¸€è¦§åŒ–ã—ã¦ãã ã•ã„ã€‚\n"
            "è¡Œé ­ã® [page:line] ã‚’å¿…ãšå‚ç…§ã—ã¦ä½ç½®ã‚’ç¤ºã—ã€éåº¦ãªæ„è¨³ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\n"
            "åŠ©è©ï¼ˆã¦ã«ãŠã¯ï¼‰ã€ä¸»è¿°ä¸€è‡´ã€å†—é•·ã€é‡è¤‡ã€èªé †ã€èª¤å­—è„±å­—ã€ç”¨èªèª¤ç”¨ã€æ–‡ä½“ã®ä¸çµ±ä¸€ã«ç‰¹ã«æ³¨æ„ã€‚\n"
            "ã€ŒåŸæ–‡ã€ ã¯è©²å½“ç®‡æ‰€ã®çŸ­ã„æŠœç²‹ï¼ˆæœ€å¤§20å­—ï¼‰ã«ç•™ã‚ã¦ãã ã•ã„ã€‚"
        ),
        "proofread_inst": (
            "ã‚ãªãŸã¯å³å¯†ãªæ—¥æœ¬èªæ ¡æ­£è€…ã§ã™ã€‚ä»¥ä¸‹ã‚’å¾¹åº•ã—ã¦ãã ã•ã„ï¼š\n"
            "- ã¦ã«ãŠã¯ã€åŠ©è©ã€ä¸»è¿°ä¸€è‡´ã€èª¤å­—è„±å­—ã€è¡¨è¨˜ã‚†ã‚Œã‚’ä¿®æ­£\n"
            "- ç”¨èªèª¤ç”¨ã®æ˜¯æ­£ãƒ»èªé †ã®è‡ªç„¶åŒ–ï¼ˆæ„å‘³ã¯å¤‰ãˆãªã„ï¼‰\n"
            "- å†—é•·ãƒ»é‡è¤‡ã‚’åœ§ç¸®ã—ã€æ–‡ä½“ã‚’çµ±ä¸€ï¼ˆå°Šæ•¬/ä¸å¯§/å¸¸ä½“ã®æ··åœ¨ã‚’è§£æ¶ˆï¼‰\n"
            "- å‡ºåŠ›ã¯æ ¡æ­£å¾Œã®æœ¬æ–‡ã®ã¿ï¼ˆå‰ç½®ãä¸è¦ï¼‰"
        ),
    },
    "ç°¡æ˜“æ ¡æ­£ï¼ˆãƒŸã‚¹æœ€å°ä¿®æ­£ï¼‰": {
        "desc": "æ˜ç™½ãªãƒŸã‚¹ã®ã¿ï¼ˆã¦ã«ãŠã¯ãƒ»åŠ©è©ãƒ»èª¤å­—è„±å­—ãƒ»æ˜ç¢ºãªå¤‰æ›ãƒŸã‚¹ï¼‰ã€‚èªé †ã‚„è¨€ã„æ›ãˆã¯æ¥µåŠ›ã—ãªã„ã€‚",
        "analyze_inst": (
            "ã‚ãªãŸã¯æ—¥æœ¬èªã®è»½å¾®æ ¡æ­£ãƒªãƒ¼ãƒ€ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ç•ªå·ä»˜ããƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€"
            "ã€æ˜ç™½ãªãƒŸã‚¹ï¼ˆã¦ã«ãŠã¯ãƒ»åŠ©è©ã®èª¤ã‚Šã€èª¤å­—è„±å­—ã€æ˜ç¢ºãªå¤‰æ›ãƒŸã‚¹ï¼‰ã€ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"
            "èªé †å¤‰æ›´ã‚„ã‚¹ã‚¿ã‚¤ãƒ«çµ±ä¸€ãªã©ã®è£é‡çš„å¤‰æ›´ã¯ææ¡ˆã—ãªã„ã§ãã ã•ã„ã€‚"
            "ã€ŒåŸæ–‡ã€ ã¯çŸ­ã„æŠœç²‹ï¼ˆæœ€å¤§20å­—ï¼‰ã€‚ã€Œç†ç”±ã€ ã¯â€œæ˜ç™½ãªèª¤ã‚Šâ€ã§ã‚ã‚‹æ ¹æ‹ ã‚’ç°¡æ½”ã«ã€‚"
        ),
        "proofread_inst": (
            "ã‚ãªãŸã¯æ—¥æœ¬èªã®è»½å¾®æ ¡æ­£è€…ã§ã™ã€‚ä»¥ä¸‹ã‚’å¾¹åº•ã—ã¦ãã ã•ã„ï¼š\n"
            "- æ˜ç™½ãªãƒŸã‚¹ã®ã¿ä¿®æ­£ï¼ˆã¦ã«ãŠã¯ãƒ»åŠ©è©ãƒ»èª¤å­—è„±å­—ãƒ»æ˜ç¢ºãªå¤‰æ›ãƒŸã‚¹ï¼‰\n"
            "- èªé †ã®å¤§ããªå¤‰æ›´ã‚„è¨€ã„æ›ãˆã¯é¿ã‘ã‚‹ï¼ˆæ„å‘³ãƒ»æ–‡ä½“ã¯æœ€å¤§é™ç¶­æŒï¼‰\n"
            "- å‡ºåŠ›ã¯æ ¡æ­£å¾Œã®æœ¬æ–‡ã®ã¿ï¼ˆå‰ç½®ãä¸è¦ï¼‰"
        ),
    },
}

# ------------------------------------------------------------
# ä¸¡ãƒ¢ãƒ¼ãƒ‰å…±é€šã§å¸¸ã«ä»˜ä¸ã•ã‚Œã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã“ã“ã«å›ºå®šè¨˜è¿°ï¼‰
# ------------------------------------------------------------
COMMON_PROMPT = (
    "ã€å…±é€šæ–¹é‡ï¼ˆå³å®ˆï¼‰ã€‘\n"
    "ãã‚Œãã‚Œã®æ ¡æ­£ã®å¿…è¦æ€§ã‚’0ã‹ã‚‰10ã¾ã§ã®æ•°å­—ã§è¡¨ã—ã¦æ˜è¨˜ã—ã¦ãã ã•ã„ï¼š\n"
    "- å¿…è¦æ€§ãŒé«˜ã„æ§‹æˆã«ã¯å¤§ãã„æ•°å­—ã‚’ã¤ã‘ã¦ãã ã•ã„ï¼\n"
    "- èª¤å­—ï¼è„±å­—ãªã©ï¼Œæ ¡æ­£ãŒå¿…ãšå¿…è¦ãªã‚‚ã®ã‚’10ã¨è©•ä¾¡ã—ã¦ãã ã•ã„ï¼\n"
    "- æ ¡æ­£ã™ã‚‹å¿…è¦ã®ç„¡ã„ã‚‚ã®ã‚’0ã¨è©•ä¾¡ã—ã¦ãã ã•ã„ï¼"
    "- å‡ºåŠ›ã¯ **Markdownã®è¡¨** ã§ã€åˆ—ã¯æ¬¡ã®é †ï¼šé  | è¡Œ | é‡è¦åº¦  | åŸæ–‡ | ä¿®æ­£æ¡ˆ | ç†ç”±  "
)

# ------------------------------------------------------------
# UIå®šæ•°
# ------------------------------------------------------------
st.set_page_config(page_title="Text Studio / æ ¡æ­£", page_icon="ğŸ“", layout="wide")

MODEL_OPTIONS = ["gpt-5-mini", "gpt-5-nano"]
DEFAULT_MODEL = "gpt-5-mini"
DEFAULT_MODE = "å³æ ¼æ ¡æ­£"
LINES_PER_PAGE = 40  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®è¡¨ç¤ºè¡Œæ•°

# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
if "chat_model" not in st.session_state:
    st.session_state["chat_model"] = DEFAULT_MODEL
if "proof_mode" not in st.session_state:
    st.session_state["proof_mode"] = DEFAULT_MODE
if "tok_in_total" not in st.session_state:
    st.session_state["tok_in_total"] = 0
if "tok_out_total" not in st.session_state:
    st.session_state["tok_out_total"] = 0

# ------------------------------------------------------------
# è¡¨ç¤ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ------------------------------------------------------------
def display_pdf_bytes(data: bytes, height: int = 600):
    """Streamlit PDFè¡¨ç¤ºï¼ˆstreamlit[pdf] ãŒã‚ã‚Œã° st.pdfï¼‰ã€‚ãªã‘ã‚Œã° iframe åŸ‹ã‚è¾¼ã¿ã€‚"""
    try:
        st.pdf(data, height=height)  # Streamlit 1.31+ / streamlit[pdf]
    except Exception:
        import base64
        b64 = base64.b64encode(data).decode()
        st.markdown(
            f'<iframe src="data:application/pdf;base64,{b64}" width="100%" height="{height}px"></iframe>',
            unsafe_allow_html=True
        )

def to_numbered_lines(raw: str) -> List[str]:
    return raw.replace("\r\n", "\n").replace("\r", "\n").split("\n")

def page_and_line(idx: int, lines_per_page: int) -> Tuple[int, int]:
    page = idx // lines_per_page + 1
    line_in_page = idx % lines_per_page + 1
    return page, line_in_page

def render_preview_with_numbers(lines: List[str], lines_per_page: int) -> str:
    return "\n".join(f"[{(i//lines_per_page)+1}:{(i%lines_per_page)+1:02d}] {t}" for i, t in enumerate(lines))

# ------------------------------------------------------------
# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ä½¿ç”¨é‡æŠ½å‡º
# ------------------------------------------------------------
def openai_client() -> OpenAI:
    return OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

def _add_usage_from_resp(resp) -> Tuple[int, int]:
    u = getattr(resp, "usage", None) or {}
    prompt = getattr(u, "prompt_tokens", None) if hasattr(u, "prompt_tokens") else u.get("prompt_tokens", 0)
    completion = getattr(u, "completion_tokens", None) if hasattr(u, "completion_tokens") else u.get("completion_tokens", 0)
    try:
        prompt = int(prompt or 0)
    except Exception:
        prompt = 0
    try:
        completion = int(completion or 0)
    except Exception:
        completion = 0
    return prompt, completion

# ------------------------------------------------------------
# è¿½åŠ ï¼šæ–¹é‡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼†è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# ------------------------------------------------------------
def get_analyze_instruction(mode: str) -> str:
    return MODE_DEFS.get(mode, MODE_DEFS["å³æ ¼æ ¡æ­£"])["analyze_inst"]

def get_proofread_instruction(mode: str, keep_layout: bool) -> str:
    base = MODE_DEFS.get(mode, MODE_DEFS["å³æ ¼æ ¡æ­£"])["proofread_inst"]
    if keep_layout:
        base = base + "\n- æ”¹è¡Œãƒ»æ®µè½ã¯å¯èƒ½ãªé™ã‚Šç¶­æŒ"
    return base

def build_sys_inst(base: str, extra: str) -> str:
    """
    baseï¼ˆãƒ¢ãƒ¼ãƒ‰å›ºæœ‰ï¼‰ â†’ extraï¼ˆUIå…¥åŠ›ï¼‰ â†’ COMMON_PROMPTï¼ˆå…±é€šæŒ‡ç¤ºï¼‰
    ã®é †ã§çµåˆã—ã¦ã€1æœ¬ã® System ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã™ã‚‹ã€‚
    """
    parts = [base.strip()]
    if (extra or "").strip():
        parts.append("ã€è¿½åŠ æŒ‡ç¤ºï¼ˆå³å®ˆï¼‰ã€‘\n" + extra.strip())
    if (globals().get("COMMON_PROMPT") or "").strip():
        parts.append(COMMON_PROMPT.strip())
    return "\n\n".join(parts)

def render_policy_preview(*, mode: str, keep_layout: bool) -> str:
    analyze_base = get_analyze_instruction(mode)
    proofread_base = get_proofread_instruction(mode, keep_layout=keep_layout)

    st.subheader("ğŸ§­ æ ¡æ­£æ–¹é‡ã®ç¢ºèª")
    st.caption(MODE_DEFS[mode]["desc"])
    
    # â†“â†“ è§£æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆSystemï¼‰ â†“â†“
    with st.expander("è§£æã§ä½¿ã†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆSystemï¼‰", expanded=False):
        st.code(analyze_base, language="markdown")

    # â†“â†“ æœ¬æ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆSystemï¼‰ â†“â†“
    with st.expander("æœ¬æ ¡æ­£ã§ä½¿ã†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆSystemï¼‰", expanded=False):
        st.code(proofread_base, language="markdown")

     # â†“â†“ å…±é€šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¸¸æ™‚ä»˜ä¸ï¼‰ â†“â†“
    with st.expander("å…±é€šæ–¹é‡ï¼ˆSystemã«æ¯å›ä»˜ä¸ï¼‰", expanded=False):
        st.code(COMMON_PROMPT.strip(), language="markdown")

    st.markdown("**âœï¸ è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»»æ„ï¼‰**")
    st.caption("ç‰¹è¨˜äº‹é …ï¼ˆä¾‹ï¼šå¤–æ¥èªã¯ã‚«ã‚¿ã‚«ãƒŠå„ªå…ˆï¼è£½å“åã¯åŸæ–‡ã©ãŠã‚Šï¼å…¨è§’è‹±æ•°å­—ã¯åŠè§’ã« ãªã©ï¼‰ã‚’è¿½è¨˜ã§ãã¾ã™ã€‚")

    extra = st.text_area(
        "è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        key="extra_user_prompt",
        placeholder="ä¾‹ï¼‰å¤–æ¥èªã®è¡¨è¨˜ã¯ã‚«ã‚¿ã‚«ãƒŠå„ªå…ˆã€‚è£½å“åã‚„å›ºæœ‰åè©ã¯åŸæ–‡ã©ãŠã‚Šã«ä¿æŒã€‚",
        height=100,
    )

    return extra or ""

# ------------------------------------------------------------
# Wordï¼ˆdocxï¼‰ç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆåŸæ–‡ã¯è¡Œç•ªå·ã¤ãï¼æ–¹é‡ã¯æ•´å½¢è¡¨ç¤ºï¼‰
# ------------------------------------------------------------
def _parse_plan_md_tables(md: str) -> List[Dict[str, str]]:
    """
    æ ¡æ­£æ–¹é‡ï¼ˆMarkdownã®è¡¨ï¼‰ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€å„è¡Œã‚’ dict åŒ–ã—ã¦è¿”ã™ã€‚
    æœŸå¾…ã‚«ãƒ©ãƒ : é  | è¡Œ | é‡è¦åº¦ | åŸæ–‡ | ä¿®æ­£æ¡ˆ | ç†ç”±
    â€» è‹±èª/æ—¥æœ¬èªãƒ˜ãƒƒãƒ€ã‚’å—ã‘ä»˜ã‘ã€è¿”å´ã¯æ—¥æœ¬èªã‚­ãƒ¼ã«çµ±ä¸€ã™ã‚‹ã€‚
    """
    items: List[Dict[str, str]] = []
    if not md:
        return items

    # è‹±->æ—¥ ã®æ­£è¦åŒ–ãƒãƒƒãƒ—
    to_jp = {
        "page": "é ",
        "line": "è¡Œ",
        "issue": "é‡è¦åº¦",
        "original": "åŸæ–‡",
        "suggestion": "ä¿®æ­£æ¡ˆ",
        "reason": "ç†ç”±",
        "é ": "é ", "ãƒšãƒ¼ã‚¸": "é ",
        "è¡Œ": "è¡Œ",
        "é‡è¦åº¦": "é‡è¦åº¦", "å•é¡Œç‚¹": "é‡è¦åº¦",
        "åŸæ–‡": "åŸæ–‡",
        "ä¿®æ­£æ¡ˆ": "ä¿®æ­£æ¡ˆ", "ææ¡ˆ": "ä¿®æ­£æ¡ˆ",
        "ç†ç”±": "ç†ç”±", "æ ¹æ‹ ": "ç†ç”±",
    }

    expected = ["é ", "è¡Œ", "é‡è¦åº¦", "åŸæ–‡", "ä¿®æ­£æ¡ˆ", "ç†ç”±"]

    def _norm_head_cell(s: str) -> str:
        key = s.strip().lower()
        return to_jp.get(key, to_jp.get(s.strip(), s.strip()))

    def _row_to_dict(ln: str, cols_jp: List[str]) -> Dict[str, str] | None:
        cells = [c.strip() for c in ln.strip("|").split("|")]
        if len(cells) < len(cols_jp):
            return None
        row = dict(zip(cols_jp, cells[:len(cols_jp)]))
        for k in expected:
            row.setdefault(k, "")
        return row

    lines = [ln for ln in (l.strip() for l in md.splitlines()) if ln]
    header_seen = False
    cols_jp: List[str] = []

    for ln in lines:
        if ln.startswith("|") and ("|" in ln):
            raw_cells = [c.strip() for c in ln.strip("|").split("|")]

            # åŒºåˆ‡ã‚Šè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
            if all(set(c.replace("-", "")) == set() for c in raw_cells):
                continue

            if not header_seen:
                norm = [_norm_head_cell(c) for c in raw_cells]
                if len(set(norm) & set(expected)) >= 3:
                    cols_jp = [c if c in expected else "" for c in norm]
                    if len(cols_jp) < 6:
                        cols_jp += [""] * (6 - len(cols_jp))
                    cols_jp = [cols_jp[i] or expected[i] for i in range(6)]
                    header_seen = True
                    continue

            if header_seen and cols_jp:
                row = _row_to_dict(ln, cols_jp)
                if row:
                    items.append(row)

    return items

def build_policy_docx_bytes(
    *,
    original_numbered_preview: str,
    plan_md: str,
    model: str,
    mode: str,
    extra_prompt: str,
    src_name: str
) -> Tuple[bytes, str]:
    """
    python-docx ãŒå­˜åœ¨ã™ã‚Œã° .docx ã‚’ã€ç„¡ã‘ã‚Œã° .txt ã‚’è¿”ã™ã€‚
    æˆ»ã‚Šå€¤: (ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒˆåˆ—, æ‹¡å¼µå­ ".docx" or ".txt")
    """
    plan_items_raw = _parse_plan_md_tables(plan_md)

    def _clean(s: str) -> str:
        s = (s or "").strip()
        if set(s) <= set("-â€”:ãƒ» ã€€"):
            return ""
        return s

    def _is_meaningful(row: Dict[str, str]) -> bool:
        keys = ["é ", "è¡Œ", "åŸæ–‡", "ä¿®æ­£æ¡ˆ"]
        return any(_clean(row.get(k, "")) for k in keys)

    plan_items = []
    for it in plan_items_raw:
        row = {
            "é ": _clean(it.get("é ", "")),
            "è¡Œ": _clean(it.get("è¡Œ", "")),
            "é‡è¦åº¦": _clean(it.get("é‡è¦åº¦", "")),
            "åŸæ–‡": _clean(it.get("åŸæ–‡", "")),
            "ä¿®æ­£æ¡ˆ": _clean(it.get("ä¿®æ­£æ¡ˆ", "")),
            "ç†ç”±": _clean(it.get("ç†ç”±", "")),
        }
        if _is_meaningful(row):
            plan_items.append(row)

    try:
        from docx import Document
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH

        doc = Document()

        # ã‚¿ã‚¤ãƒˆãƒ«
        title = doc.add_heading("æ ¡æ­£æ–¹é‡ãƒ¬ãƒãƒ¼ãƒˆ", 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # ãƒ¡ã‚¿æƒ…å ±
        p = doc.add_paragraph()
        p.add_run("ç”Ÿæˆæ—¥æ™‚: ").bold = True
        p.add_run(_dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        p.add_run("\nãƒ¢ãƒ‡ãƒ«: ").bold = True
        p.add_run(model)
        p.add_run("\nãƒ¢ãƒ¼ãƒ‰: ").bold = True
        p.add_run(mode)
        if (extra_prompt or "").strip():
            p.add_run("\nè¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ").bold = True
            p.add_run(extra_prompt.strip())
        p.add_run("\nå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: ").bold = True
        p.add_run(src_name or "-")

        # åŸæ–‡ï¼ˆè¡Œç•ªå·ã¤ãï¼‰
        doc.add_heading("åŸæ–‡ï¼ˆè¡Œç•ªå·ã¤ãï¼‰", level=1)
        para = doc.add_paragraph()
        run = para.add_run(original_numbered_preview if original_numbered_preview else "(ç©º)")
        run.font.name = "Courier New"
        run.font.size = Pt(10)

        # æ ¡æ­£æ–¹é‡ï¼ˆæ•´å½¢ç‰ˆï¼‰
        doc.add_heading("æ ¡æ­£æ–¹é‡ï¼ˆèª­ã¿ã‚„ã™ã„æ•´å½¢ç‰ˆï¼‰", level=1)
        if plan_items:
            for k, it in enumerate(plan_items, 1):
                if k > 1:
                    doc.add_paragraph("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

                head = doc.add_paragraph()
                head_run = head.add_run(
                    f"{k}. é  {it.get('é ','-')}  è¡Œ {it.get('è¡Œ','-')} â€” é‡è¦åº¦ {it.get('é‡è¦åº¦','')}"
                )
                head_run.bold = True

                if it.get("åŸæ–‡"):
                    p1 = doc.add_paragraph()
                    p1.add_run("åŸæ–‡: ").bold = True
                    p1.add_run(it.get("åŸæ–‡", ""))

                if it.get("ä¿®æ­£æ¡ˆ"):
                    p2 = doc.add_paragraph()
                    p2.add_run("ä¿®æ­£æ¡ˆ: ").bold = True
                    p2.add_run(it.get("ä¿®æ­£æ¡ˆ", ""))

                if it.get("ç†ç”±"):
                    p3 = doc.add_paragraph()
                    p3.add_run("ç†ç”±: ").bold = True
                    p3.add_run(it.get("ç†ç”±", ""))
        else:
            doc.add_paragraph("(è§£æè¡¨ã®æ¤œå‡ºã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“)")

        # ä»˜éŒ²ï¼šå…ƒMarkdownï¼ˆä»»æ„ï¼‰
        doc.add_heading("ä»˜éŒ²ï¼šå…ƒMarkdownï¼ˆãã®ã¾ã¾ï¼‰", level=2)
        para2 = doc.add_paragraph()
        r2 = para2.add_run(plan_md if plan_md else "(ãªã—)")
        r2.font.name = "Courier New"
        r2.font.size = Pt(10)

        bio = io.BytesIO()
        doc.save(bio)
        return bio.getvalue(), ".docx"

    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: .txt
        lines: List[str] = []
        lines.append("=== æ ¡æ­£æ–¹é‡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆTXTãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ===")
        lines.append(f"ç”Ÿæˆæ—¥æ™‚: {_dt.datetime.now():%Y-%m-%d %H:%M:%S}")
        lines.append(f"ãƒ¢ãƒ‡ãƒ«: {model}")
        lines.append(f"ãƒ¢ãƒ¼ãƒ‰: {mode}")
        if (extra_prompt or "").strip():
            lines.append(f"è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {extra_prompt.strip()}")
        lines.append(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {src_name or '-'}")

        lines.append("\n--- åŸæ–‡ï¼ˆè¡Œç•ªå·ã¤ãï¼‰ ---\n")
        lines.append(original_numbered_preview or "(ç©º)")

        lines.append("\n--- æ ¡æ­£æ–¹é‡ï¼ˆæ•´å½¢ç‰ˆï¼‰ ---\n")
        if plan_md and plan_items:
            for i, it in enumerate(plan_items, 1):
                if i > 1:
                    lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                lines.append(
                    f"{i}. é  {it.get('é ','-')}  è¡Œ {it.get('è¡Œ','-')} â€” é‡è¦åº¦ {it.get('é‡è¦åº¦','')}"
                )
                if it.get("åŸæ–‡"):
                    lines.append(f"   åŸæ–‡   : {it.get('åŸæ–‡','')}")
                if it.get("ä¿®æ­£æ¡ˆ"):
                    lines.append(f"   ä¿®æ­£æ¡ˆ : {it.get('ä¿®æ­£æ¡ˆ','')}")
                if it.get("ç†ç”±"):
                    lines.append(f"   ç†ç”±   : {it.get('ç†ç”±','')}")
        else:
            lines.append("(è§£æè¡¨ã®æ¤œå‡ºã«å¤±æ•—)")

        lines.append("\n--- ä»˜éŒ²ï¼šå…ƒMarkdown ---\n")
        lines.append(plan_md or "(ãªã—)")
        data = "\n".join(lines).encode("utf-8")
        return data, ".txt"

# ------------------------------------------------------------
# è¿½åŠ ï¼šæ ¡æ­£çµæœã® PDF/Word ç”Ÿæˆãƒ˜ãƒ«ãƒ‘
# ------------------------------------------------------------
def build_plain_docx_bytes(text: str, title: str = "æ ¡æ­£çµæœ") -> bytes:
    """æ ¡æ­£çµæœãƒ†ã‚­ã‚¹ãƒˆã‚’ .docx ã«å˜ç´”å‡ºåŠ›"""
    from docx import Document
    from docx.shared import Pt
    doc = Document()
    doc.add_heading(title, 0)
    p = doc.add_paragraph()
    run = p.add_run(text if text else "(ç©º)")
    run.font.size = Pt(11)
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

def build_pdf_from_text(text: str, title: str = "æ ¡æ­£çµæœ") -> bytes:
    """
    æ—¥æœ¬èªå¯¾å¿œã®ç°¡æ˜“PDFç”Ÿæˆã€‚reportlab ãŒã‚ã‚‹å ´åˆã®ã¿ã€‚
    ç„¡ã„å ´åˆã¯ç©ºbytesã‚’è¿”ã™ï¼ˆUIå´ã§æ¡ˆå†…ï¼‰ã€‚
    """
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.pdfgen import canvas
        from reportlab.lib.units import mm
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.cidfonts import UnicodeCIDFont

        buf = io.BytesIO()
        c = canvas.Canvas(buf, pagesize=A4)
        width, height = A4

        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆï¼ˆCIDãƒ•ã‚©ãƒ³ãƒˆï¼‰
        pdfmetrics.registerFont(UnicodeCIDFont('HeiseiMin-W3'))

        # ã‚¿ã‚¤ãƒˆãƒ«
        c.setFont('HeiseiMin-W3', 14)
        c.drawString(20*mm, height - 20*mm, title)

        # æœ¬æ–‡
        textobj = c.beginText(20*mm, height - 30*mm)
        textobj.setFont('HeiseiMin-W3', 10)

        for line in (text or "").splitlines():
            textobj.textLine(line if line else " ")
            # ä½™ç™½ç¢ºä¿ã—ã¦æ”¹ãƒšãƒ¼ã‚¸
            if textobj.getY() < 20*mm:
                c.drawText(textobj)
                c.showPage()
                c.setFont('HeiseiMin-W3', 14)
                c.drawString(20*mm, height - 20*mm, title)
                textobj = c.beginText(20*mm, height - 30*mm)
                textobj.setFont('HeiseiMin-W3', 10)

        c.drawText(textobj)
        c.showPage()
        c.save()
        pdf_bytes = buf.getvalue()
        buf.close()
        return pdf_bytes
    except Exception:
        return b""

# ------------------------------------------------------------
# è§£æï¼ˆæ ¡æ­£æ–¹é‡ï¼‰
# ------------------------------------------------------------
def analyze_issues(model: str, lines: List[str], lines_per_page: int, mode: str, extra: str) -> Tuple[str, ChatUsage]:
    client = openai_client()
    md_tables: List[str] = []
    used_in = used_out = 0

    total_pages = (len(lines) + lines_per_page - 1) // lines_per_page
    sys_inst_template = build_sys_inst(get_analyze_instruction(mode), extra)

    for pg in range(total_pages):
        start = pg * lines_per_page
        end = min((pg + 1) * lines_per_page, len(lines))
        page_chunk = [f"[{(i//lines_per_page)+1}:{(i%lines_per_page)+1:02d}] {lines[i]}" for i in range(start, end)]
        page_text = "\n".join(page_chunk)

        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": sys_inst_template},
                {"role": "user", "content": f"æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã“ã®ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰ã‚’è§£æã—ã¦ãã ã•ã„ï¼š\n---\n{page_text}"},
            ],
        )
        md_tables.append(resp.choices[0].message.content.strip())
        pin, pout = _add_usage_from_resp(resp)
        used_in += pin; used_out += pout

    out = []
    for i, tbl in enumerate(md_tables, 1):
        out.append(f"#### é  {i}\n\n{tbl}\n")
    return "\n".join(out), ChatUsage(input_tokens=used_in, output_tokens=used_out)

# ------------------------------------------------------------
# æœ¬æ ¡æ­£
# ------------------------------------------------------------
def proofread(model: str, content: str, keep_layout: bool, want_report: bool, mode: str, extra: str) -> Tuple[str, ChatUsage]:
    client = openai_client()
    CHUNK = 6000
    chunks = [content[i:i+CHUNK] for i in range(0, len(content), CHUNK)] or [content]

    fixed_parts: List[str] = []
    used_in = used_out = 0
    sys_inst = build_sys_inst(get_proofread_instruction(mode, keep_layout=keep_layout), extra)

    for chunk in chunks:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": sys_inst},
                {"role": "user", "content": f"æ¬¡ã®æœ¬æ–‡ã‚’æ ¡æ­£ã—ã¦ãã ã•ã„ï¼š\n---\n{chunk}"},
            ],
        )
        fixed_parts.append(resp.choices[0].message.content.strip())
        pin, pout = _add_usage_from_resp(resp)
        used_in += pin; used_out += pout

    fixed_text = ("\n\n" if keep_layout else "\n").join(fixed_parts).strip()

    if want_report:
        rep_inst = (
            "æ¬¡ã®åŸæ–‡ã¨æ ¡æ­£å¾Œæœ¬æ–‡ã®å·®åˆ†è¦³ç‚¹ã§ã€ä¸»ãªä¿®æ­£ãƒã‚¤ãƒ³ãƒˆã‚’æœ€å¤§8é …ç›®ã§ç®‡æ¡æ›¸ãã«ã€‚\n"
            "ä¾‹ï¼šåŠ©è©/æ´»ç”¨ã€èªé †ã€å†—é•·/é‡è¤‡ã€èª¤å­—è„±å­—ã€æ–‡ä½“çµ±ä¸€ã€ç”¨èªèª¤ç”¨ãªã©ã€‚"
        )
        rep_user = f"åŸæ–‡:\n{content[:4000]}\n\næ ¡æ­£å¾Œ:\n{fixed_text[:4000]}"
        rep = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": rep_inst},
                {"role": "user", "content": rep_user},
            ],
        )
        fixed_text += f"\n\n---\nã€ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼ˆè¦ç´„ï¼‰ã€‘\n{rep.choices[0].message.content.strip()}"
        pin, pout = _add_usage_from_resp(rep)
        used_in += pin; used_out += pout

    return fixed_text, ChatUsage(input_tokens=used_in, output_tokens=used_out)

# ------------------------------------------------------------
# ç”»é¢
# ------------------------------------------------------------
st.title("ğŸ“ æ ¡æ­£ â€” è§£æï¼ˆãƒšãƒ¼ã‚¸/è¡Œ/ç†ç”±ï¼‰ â†’ æœ¬æ ¡æ­£")
st.write("æœ€åˆã«**æ–¹é‡**ã‚’ç¢ºèª/è¿½è¨˜ã—ã¦ã‹ã‚‰ã€å…¥åŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ« or è²¼ã‚Šä»˜ã‘ï¼‰ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")

with st.sidebar:
    st.header("è¨­å®š")
    st.radio(
        "ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«",
        MODEL_OPTIONS,
        index=MODEL_OPTIONS.index(st.session_state["chat_model"]),
        key="chat_model",
    )

    from common_lib.openai.costs import MODEL_PRICES_USD
    sel_model = st.session_state["chat_model"]
    if sel_model in MODEL_PRICES_USD:
        price_in = MODEL_PRICES_USD[sel_model]["in"] / 1000
        price_out = MODEL_PRICES_USD[sel_model]["out"] / 1000
        st.markdown(
            f"""
            **ğŸ’² å˜ä¾¡ (USD / 1K tokens)**  
            - å…¥åŠ›: `${price_in:.5f}`  
            - å‡ºåŠ›: `${price_out:.5f}`
            """
        )
    else:
        st.info("å˜ä¾¡æƒ…å ±ãŒæœªç™»éŒ²ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚")

    st.selectbox(
        "ğŸ›  æ ¡æ­£ãƒ¢ãƒ¼ãƒ‰",
        list(MODE_DEFS.keys()),
        index=list(MODE_DEFS.keys()).index(st.session_state.get("proof_mode", DEFAULT_MODE)),
        key="proof_mode",
        help="\n\n".join([f"ãƒ»{k}: {v['desc']}" for k, v in MODE_DEFS.items()]),
    )
    st.caption(f"èª¬æ˜ï¼š{MODE_DEFS[st.session_state['proof_mode']]['desc']}")

    keep_formatting = st.checkbox("æ”¹è¡Œãƒ»æ®µè½ã‚’ä¿æŒï¼ˆæœ¬æ ¡æ­£æ™‚ï¼‰", value=True)
    show_report = st.checkbox("æœ¬æ ¡æ­£å¾Œã«ã€ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆè¦ç´„ã€ã‚‚ä»˜ä¸", value=False)
    lpp = st.number_input("ãƒšãƒ¼ã‚¸è¡Œæ•°ï¼ˆè¡¨ç¤ºç”¨ï¼‰", min_value=20, max_value=100, value=LINES_PER_PAGE, step=5)

    # è¿½åŠ ï¼šæ ¡æ­£çµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼
    dl_choice = st.radio(
        "ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼ï¼ˆæ ¡æ­£çµæœï¼‰",
        ("Word (.docx)", "PDF (.pdf)", "ä¸¡æ–¹"),
        index=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Word
        help="æ ¡æ­£çµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼ã‚’é¸ã³ã¾ã™ã€‚æ–¹é‡ãƒ¬ãƒãƒ¼ãƒˆã® Word å‡ºåŠ›ã¯å¾“æ¥ã©ãŠã‚Šå›ºå®šã§ã™ã€‚"
    )

extra_prompt = render_policy_preview(
    mode=st.session_state["proof_mode"],
    keep_layout=keep_formatting,
)
st.markdown("---")

# ===== å…¥åŠ›æ–¹æ³•ã®é¸æŠï¼ˆãƒ•ã‚¡ã‚¤ãƒ« / è²¼ã‚Šä»˜ã‘ï¼‰ =====
tab_file, tab_paste = st.tabs(["ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰", "ğŸ“ è²¼ã‚Šä»˜ã‘ãƒ†ã‚­ã‚¹ãƒˆ"])

src_text = ""
used_file_name = None

# ---------- ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ› ----------
with tab_file:
    col_u, col_btn1, col_btn2 = st.columns([3, 1, 1])
    with col_u:
        up = st.file_uploader(".docx / .txt / .pdf ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["docx", "txt", "pdf"])
    with col_btn1:
        do_analyze_file = st.button("â‘  è§£æï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰", type="primary", use_container_width=True, disabled=not up, key="btn_analyze_file")
    with col_btn2:
        do_fix_file = st.button("â‘¡ æœ¬æ ¡æ­£ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰", type="primary", use_container_width=True, disabled=not up, key="btn_fix_file")

    if up:
        used_file_name = up.name
        name = up.name.lower()
        if name.endswith(".pdf"):
            data = up.read()
            try:
                stats = extract_pdf_text(data)
            except RuntimeError as e:
                st.error(str(e)); st.stop()

            st.subheader("ğŸ“„ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            display_pdf_bytes(data, height=600)

            if int(stats.get("visible", 0)) < 20:
                st.warning("ã“ã®PDFã¯ç”»åƒPDFï¼ˆãƒ†ã‚­ã‚¹ãƒˆå±¤ãªã—ï¼‰ã¨åˆ¤å®šã—ã¾ã—ãŸã€‚OCRãƒ„ãƒ¼ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                st.stop()
            else:
                src_text = (stats.get("text") or "").strip()
        else:
            try:
                src_text = load_text_generic(up)
            except RuntimeError as e:
                st.error(str(e)); st.stop()

# ---------- è²¼ã‚Šä»˜ã‘å…¥åŠ› ----------
with tab_paste:
    pasted = st.text_area(
        "ã“ã“ã«æœ¬æ–‡ã‚’è²¼ã‚Šä»˜ã‘",
        height=260,
        placeholder="ã“ã“ã«æœ¬æ–‡ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ï¼ˆæ”¹è¡Œã¯ä¿æŒã•ã‚Œã¾ã™ï¼‰ã€‚"
    )
    col_p1, col_p2, col_opts = st.columns([1, 1, 2])
    with col_opts:
        normalize = st.checkbox("æ”¹è¡Œã¨BOMã‚’æ­£è¦åŒ–", value=True, key="opt_norm")
        collapse = st.checkbox("é€£ç¶šç©ºè¡Œã‚’åœ§ç¸®", value=False, key="opt_collapse")
        keep_blanks = st.number_input("ç©ºè¡Œã®ä¸Šé™ï¼ˆåœ§ç¸®æ™‚ï¼‰", 1, 5, 1, key="opt_keep_blanks")
        trim_tail = st.checkbox("è¡Œæœ«ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»", value=True, key="opt_trim_tail")

    with col_p1:
        do_analyze_paste = st.button("â‘  è§£æï¼ˆè²¼ã‚Šä»˜ã‘ï¼‰", type="secondary", use_container_width=True,
                                 disabled=not pasted.strip(), key="btn_analyze_paste")

    with col_p2:
        do_fix_paste = st.button("â‘¡ æœ¬æ ¡æ­£ï¼ˆè²¼ã‚Šä»˜ã‘ï¼‰", type="primary", use_container_width=True,
                             disabled=not pasted.strip(), key="btn_fix_paste")

    if pasted:
        src_text = load_text_from_paste(
            pasted,
            normalize=normalize,
            collapse_blanks=collapse,
            keep_blank_lines=int(keep_blanks),
            trim_trailing=trim_tail,
        )
        used_file_name = "pasted_text.txt"

# ===== å…±é€šï¼šã“ã“ã‹ã‚‰è§£æ/æœ¬æ ¡æ­£ã®å®Ÿè¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ« or è²¼ã‚Šä»˜ã‘ï¼‰ =====
if src_text:
    lines = to_numbered_lines(src_text)
    st.subheader("ğŸ‘€ è¡Œç•ªå·ä»˜ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    st.caption(f"è¡¨ç¤ºä¸Šã®ãƒšãƒ¼ã‚¸è¡Œæ•°: {LINES_PER_PAGE} è¡Œ/ãƒšãƒ¼ã‚¸ï¼ˆæ“¬ä¼¼å‰²ã‚Šä»˜ã‘ï¼‰")
    st.text_area("åŸæ–‡ï¼ˆç•ªå·ä»˜ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰", value=render_preview_with_numbers(lines, LINES_PER_PAGE), height=260)

    run_in = run_out = 0
    want_analyze = (locals().get("do_analyze_file") or locals().get("do_analyze_paste"))
    want_fix     = (locals().get("do_fix_file") or locals().get("do_fix_paste"))

    if want_analyze:
        with st.spinner("è§£æä¸­ï¼ˆæ ¡æ­£æ–¹é‡ã‚’æŠ½å‡ºï¼‰â€¦"):
            plan_md, usage = analyze_issues(
                st.session_state["chat_model"], lines, LINES_PER_PAGE,
                mode=st.session_state["proof_mode"], extra=extra_prompt
            )
        st.success("è§£æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸/è¡Œ/ç†ç”±ã¤ãã§æ–¹é‡ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        st.subheader("ğŸ“‹ æ ¡æ­£æ–¹é‡ï¼ˆã¾ãšä½•ã‚’ã©ã†ç›´ã™ã‹ï¼‰")
        st.markdown(plan_md, unsafe_allow_html=False)
        run_in += usage.input_tokens; run_out += usage.output_tokens

        # â–¼â–¼ Wordï¼ˆdocxï¼‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåŸæ–‡=è¡Œç•ªå·ã¤ãã€æ–¹é‡=æ•´å½¢ï¼‰ â–¼â–¼
        st.markdown("### â¤µï¸ åŸæ–‡ï¼‹æ ¡æ­£æ–¹é‡ã‚’ Word ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        numbered_preview = render_preview_with_numbers(lines, LINES_PER_PAGE)
        data, ext = build_policy_docx_bytes(
            original_numbered_preview=numbered_preview,
            plan_md=plan_md,
            model=st.session_state["chat_model"],
            mode=st.session_state["proof_mode"],
            extra_prompt=extra_prompt,
            src_name=used_file_name or "pasted_text.txt",
        )
        file_base = (used_file_name or "pasted_text").rsplit(".", 1)[0]
        dl_name = f"policy_{file_base}{ext}"
        st.download_button(
            "Wordï¼ˆ.docxï¼‰ã¨ã—ã¦ä¿å­˜" if ext == ".docx" else "ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ.txtï¼‰ã¨ã—ã¦ä¿å­˜",
            data=data,
            file_name=dl_name,
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document" if ext == ".docx" else "text/plain",
            use_container_width=True,
        )
        st.caption("â€» æ–¹é‡ãƒ¬ãƒãƒ¼ãƒˆã¯ Wordï¼ˆpython-docxï¼‰å›ºå®šã§ã™ã€‚python-docx ãŒç„¡ã„ç’°å¢ƒã§ã¯ .txt ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    if want_fix:
        with st.spinner("æœ¬æ ¡æ­£ã®å®Ÿè¡Œä¸­â€¦"):
            result, usage = proofread(
                model=st.session_state["chat_model"],
                content=src_text,
                keep_layout=keep_formatting,
                want_report=show_report,
                mode=st.session_state["proof_mode"],
                extra=extra_prompt,
            )
        st.success("æ ¡æ­£å®Œäº†ï¼ä¸‹ã®çµæœã‚’ã‚³ãƒ”ãƒ¼/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
        st.subheader("ğŸ§¾ æ ¡æ­£çµæœ")
        st.text_area("æ ¡æ­£çµæœ", value=result, height=420)

        file_stub = f"proofread_{(used_file_name or 'output').rsplit('.',1)[0]}"

        # â–¼â–¼ æ ¡æ­£çµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®é¸æŠã«å¾“ã† â–¼â–¼
        if dl_choice in ("Word (.docx)", "ä¸¡æ–¹"):
            try:
                docx_bytes = build_plain_docx_bytes(result, title="æ ¡æ­£çµæœ")
                st.download_button(
                    "æ ¡æ­£çµæœã‚’ Word (.docx) ã§ä¿å­˜",
                    data=docx_bytes,
                    file_name=f"{file_stub}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    use_container_width=True,
                )
            except Exception as e:
                st.warning(f"Word å‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        if dl_choice in ("PDF (.pdf)", "ä¸¡æ–¹"):
            pdf_bytes = build_pdf_from_text(result, title="æ ¡æ­£çµæœ")
            if pdf_bytes:
                st.download_button(
                    "æ ¡æ­£çµæœã‚’ PDF (.pdf) ã§ä¿å­˜",
                    data=pdf_bytes,
                    file_name=f"{file_stub}.pdf",
                    mime="application/pdf",
                    use_container_width=True,
                )
            else:
                st.warning("PDF ç”Ÿæˆã«å¿…è¦ãª reportlab ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚`pip install reportlab` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        # å‚è€ƒã¨ã—ã¦å¾“æ¥ã® .txt ã‚‚æ®‹ã™å ´åˆã¯ä¸‹ã‚’æœ‰åŠ¹åŒ–
        # st.download_button(
        #     "æ ¡æ­£çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆ (.txt) ã§ä¿å­˜",
        #     data=result.encode("utf-8"),
        #     file_name=f"{file_stub}.txt",
        #     mime="text/plain",
        # )

        run_in += usage.input_tokens; run_out += usage.output_tokens

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆã¸åæ˜  & æ–™é‡‘è¡¨ç¤º
    if want_analyze or want_fix:
        st.session_state["tok_in_total"] += run_in
        st.session_state["tok_out_total"] += run_out

        st.markdown("## ğŸ’° ä½¿ç”¨é‡ã¨æ¦‚ç®—è²»ç”¨")
        render_chat_only_summary(
            title="ä»Šå›ãƒ©ãƒ³ã®æ¦‚ç®—",
            model=st.session_state["chat_model"],
            in_tokens=run_in,
            out_tokens=run_out,
        )
        usage_cost = estimate_chat_cost(
            st.session_state["chat_model"],
            ChatUsage(input_tokens=run_in, output_tokens=run_out),
        )
        st.caption(
            f"ä»Šå›ãƒ©ãƒ³ tokens: in={run_in:,} / out={run_out:,} â†’ "
            f"USD ${usage_cost['usd']:.6f} â‰ˆ JPY {usage_cost['jpy']:.2f} "
            f"(rate={DEFAULT_USDJPY:.2f})"
        )
        st.divider()
        st.markdown("### ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆï¼ˆã“ã®ãƒšãƒ¼ã‚¸ã§ã®åˆç®—ï¼‰")
        render_chat_only_summary(
            title="ç´¯è¨ˆã®æ¦‚ç®—",
            model=st.session_state["chat_model"],
            in_tokens=st.session_state["tok_in_total"],
            out_tokens=st.session_state["tok_out_total"],
        )
else:
    st.info("å…¥åŠ›ã‚¿ãƒ–ï¼ˆğŸ“/ğŸ“ï¼‰ã‹ã‚‰æœ¬æ–‡ã‚’æŒ‡å®šã—ã¦ã€â‘  è§£æã€â†’ã€â‘¡ æœ¬æ ¡æ­£ã€ã®é †ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
