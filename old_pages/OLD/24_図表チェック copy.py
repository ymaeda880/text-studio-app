# -*- coding: utf-8 -*-
# pages/24_å›³è¡¨ãƒã‚§ãƒƒã‚¯.py â€” GPTä¸ä½¿ç”¨ï¼šå›³è¡¨è¦‹å‡ºã— â†” æœ¬æ–‡å‚ç…§ã®ç…§åˆï¼ˆå…¨ä»¶æ–‡è„ˆã¤ãï¼‰
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ§¾ å›³è¡¨ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‰", page_icon="ğŸ§¾", layout="wide")
st.title("ğŸ§¾ å›³è¡¨ãƒã‚§ãƒƒã‚¯ï¼ˆè¦‹å‡ºã— â†” å‚ç…§ ã®ç…§åˆï¼‰")
st.caption("è¦‹å‡ºã—ã¯è¡Œé ­ã®ã€Œå›³/è¡¨/å›³è¡¨ + ç•ªå· + ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ1è¡Œï¼‰ã€ã€å‚ç…§ã¯æœ¬æ–‡è¡Œä¸­ã®ã€Œå›³/è¡¨/å›³è¡¨ + ç•ªå·ã€ã€‚è¦‹å‡ºã—è¡Œã¯å‚ç…§ã‹ã‚‰é™¤å¤–ã€‚å‚ç…§æ–‡è„ˆã¯å…¨ä»¶ã‚’ page_labelï¼‹è¡Œç•ªå·ã¤ãã§åé›†ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
run = st.button("â–¶ è§£æãƒ»ç…§åˆã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    context_radius = st.number_input("å‚ç…§å‰å¾Œã®è¡Œæ•°ï¼ˆæ–‡è„ˆï¼‰", min_value=0, max_value=10, value=2, step=1)
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ï¼ˆæŠ½å‡ºä¸­é–“ï¼‰ã‚’è¡¨ç¤º", value=False)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts

with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")


# =========================
# æ­£è¦åŒ–ãƒ»ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«æŠ½å‡ºï¼ˆç›®æ¬¡ãƒã‚§ãƒƒã‚¯æº–æ‹ ï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"   # å„ç¨®ãƒã‚¤ãƒ•ãƒ³
LEADER_CHARS_CLASS = r"[\.ï¼ãƒ»ï½¥â€¦â€§ï½¡]"
LEADERS_SPACED = rf"(?:\s*{LEADER_CHARS_CLASS}\s*){{3,}}"
ALPHAJP = r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]+"  # è‹±/ã‹ãª/æ¼¢

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰ï¼»ï¼½ï½›ï½", "0123456789()[]{}"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    s = z2h_numhy(s)
    s = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

# ã€Œè¡Œå˜ç‹¬ã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã€æ¤œå‡ºï¼ˆé€£ç•ª / ç« -ãƒšãƒ¼ã‚¸ / ã‚·ãƒªãƒ¼ã‚º-ç•ªå·ï¼‰
def build_label_line_regex_mixed() -> re.Pattern:
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"
    core_series = rf"{ALPHAJP}{HY}[0-9ï¼-ï¼™]+"
    core = rf"(?:{core_seq}|{core_chap}|{core_series})"
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

LABEL_LINE_RE = build_label_line_regex_mixed()

def split_segments_by_label(all_text: str) -> List[Dict[str, Any]]:
    """
    å…¨æ–‡ï¼ˆé€£çµï¼‰ã‚’ã€å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§åˆ†å‰²ã—ã€[{'page_label','body'}] ã‚’è¿”ã™ã€‚
    """
    txt = normalize_strict(all_text.replace("\r\n","\n").replace("\r","\n"))
    matches = list(LABEL_LINE_RE.finditer(txt))
    if not matches:
        return []

    def next_nonempty_pos(pos: int) -> int:
        n = pos
        while n < len(txt) and txt[n] == "\n":
            n += 1
        return n

    segs: List[Dict[str, Any]] = []
    for i, m in enumerate(matches):
        label = z2h_numhy(m.group("label"))
        start = next_nonempty_pos(m.end())
        end   = matches[i+1].start() if i+1 < len(matches) else len(txt)
        body  = txt[start:end].lstrip("\n ")
        segs.append({"page_label": label, "body": body})
    return segs

all_text_joined = "\n".join(pages_text)
segments = split_segments_by_label(all_text_joined)

st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆå˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§åŒºåˆ‡ã‚Šï¼‰â€” æ¦‚è¦³")
if not segments:
    st.warning("å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãšã€æŠ½å‡ºãƒšãƒ¼ã‚¸ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFç‰ˆé¢ã«ã‚ˆã‚Šè¡Œå˜ç‹¬ã¨ãªã‚‰ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰ã€‚")
else:
    df_segments_overview = pd.DataFrame([{
        "page_label": s["page_label"],
        "char_count": len(s["body"]),
        "preview": s["body"][:120].replace("\n"," ") + ("â€¦" if len(s["body"])>120 else "")
    } for s in segments])
    st.dataframe(df_segments_overview, use_container_width=True)

# ãƒ©ãƒ™ãƒ«ã®é€£ç•ªå¦¥å½“æ€§ï¼ˆç°¡æ˜“ï¼‰
def _parse_label_kind(label: str) -> Tuple[str, Any]:
    lab = z2h_numhy(label)
    if re.fullmatch(r"[0-9]+", lab):
        return "seq", int(lab)
    parts = lab.split("-")
    if len(parts) >= 2 and all(p.isdigit() for p in parts):
        return "chap", [int(p) for p in parts]
    m = re.fullmatch(rf"({ALPHAJP})-([0-9]+)", lab)
    if m:
        return "series", (m.group(1), int(m.group(2)))
    return "unknown", None

def valid_and_reason_auto(label: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    k, cur = _parse_label_kind(label)
    if k == "unknown":
        return False, "ä¸æ˜ãªãƒ©ãƒ™ãƒ«å½¢å¼"
    if prev_ok is None:
        return True, ""
    pk, prev = _parse_label_kind(prev_ok)
    if pk == "unknown":
        return True, ""
    if k != pk:
        return True, "å½¢å¼åˆ‡æ›¿"
    if k == "seq":
        return (cur == prev + 1, "" if cur == prev + 1 else "éé€£ç•ª")
    if k == "chap":
        c, p = (cur + [1, 1])[:2]; pc, pp = (prev + [1, 1])[:2]
        ok = (c == pc and p == pp + 1) or (c == pc + 1 and p == 1)
        return (ok, "" if ok else "éé€£ç•ª")
    if k == "series":
        s, n = cur; ps, pn = prev
        if s != ps:
            return True, "å½¢å¼åˆ‡æ›¿"
        return (n == pn + 1, "" if n == pn + 1 else "éé€£ç•ª")
    return True, ""

rows_check: List[Dict[str, Any]] = []
prev_ok: Optional[str] = None
for s in segments:
    ok, reason = valid_and_reason_auto(s["page_label"], prev_ok)
    if ok:
        prev_ok = s["page_label"]
    rows_check.append({
        "page_label": s["page_label"],
        "valid": ok,
        "reason": "" if ok else reason,
        "char_count": len(s["body"]),
        "preview": s["body"][:100].replace("\n"," ") + ("â€¦" if len(s["body"])>100 else "")
    })
df_check = pd.DataFrame(rows_check)
st.dataframe(df_check, use_container_width=True)

valid_segments = [s for s in segments if any((r["page_label"]==s["page_label"] and r["valid"]) for r in rows_check)]

# --- ã“ã“ã¯ rows_check/df_check ã®å¾Œã€valid_segments ã‚’ä½œã£ãŸç›´å¾Œã«è¿½åŠ  ---
# valid ãƒ©ãƒ™ãƒ«é›†åˆ
valid_label_set = {s["page_label"] for s in segments
                   if any((r["page_label"] == s["page_label"] and r["valid"]) for r in rows_check)}

# ==== PDFãƒšãƒ¼ã‚¸ â†’ validãª page_label ã‚’å¼•ã‘ã‚‹è¾æ›¸ã‚’ç”¨æ„ ====
# å„ãƒšãƒ¼ã‚¸æœ¬æ–‡å†…ã‚’å€‹åˆ¥ã«èµ°æŸ»ã—ã¦ã€Œè¡Œå˜ç‹¬ã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«(LABEL_LINE_RE)ã€ã‚’æ‹¾ã„ã€
# é€£ç•ªæ¤œè¨¼(valid_and_reason_auto)ã«é€šã£ãŸã‚‚ã®ã ã‘ã‚’ page_to_valid_label[pidx] ã«ç™»éŒ²
page_to_valid_label: Dict[int, str] = {}
_prev_ok: Optional[str] = None
for pidx, ptxt in enumerate(pages_text, start=1):
    txt = normalize_strict(ptxt.replace("\r\n","\n").replace("\r","\n"))
    m = LABEL_LINE_RE.search(txt)
    if not m:
        continue
    cand = z2h_numhy(m.group("label"))
    ok, _ = valid_and_reason_auto(cand, _prev_ok)
    if ok:
        _prev_ok = cand
        page_to_valid_label[pidx] = cand



# PDFãƒšãƒ¼ã‚¸é † â†’ page_label ã®å¯¾å¿œï¼ˆç·©ã„å¯¾å¿œï¼šsegmentsé †ã‚’æµç”¨ã€è¶³ã‚Šãªã‘ã‚Œã°é€£ç•ªï¼‰
page_labels = [s["page_label"] for s in segments] if segments else [str(i+1) for i in range(len(pages_text))]
if len(page_labels) < len(pages_text):
    # è¶³ã‚Šãªã„åˆ†ã¯æœ«å°¾ã«é€£ç•ªã‚’è£œå®Œ
    page_labels = page_labels + [str(i+1) for i in range(len(page_labels), len(pages_text))]


# =========================
# å›³è¡¨ è¦‹å‡ºã—æŠ½å‡º ï¼† å‚ç…§æŠ½å‡ºï¼ˆè¦‹å‡ºã—è¡Œã¯å‚ç…§ã‹ã‚‰é™¤å¤–ï¼‰
# =========================
# ãƒ‰ãƒƒãƒˆé¡ãƒ»æ•°å€¤ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ2. 2-1 / ï¼’ï¼ï¼‘ï¼ï¼‘ / ï¼ˆï¼‘ï¼‰ ã‚’è¨±å®¹ï¼‰
DOT = r"[\.ï¼ãƒ»ï½¥]"
NUM = r"[0-9ï¼-ï¼™]+"
NUM_TOKEN = rf"""
(?:                                     # â‘  ãƒ‰ãƒƒãƒˆåŒºåˆ‡ã‚Š + ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Š
    {NUM}
    (?:\s*{DOT}\s*{NUM})*               # 2.2 ã‚„ 2ï¼ 2 ãªã©
    (?:\s*-\s*{NUM})*                   # -1 ã‚„ -ï¼‘
  |
    [ï¼ˆ(]\s*{NUM}\s*[ï¼‰)]               # â‘¡ ï¼ˆï¼‘ï¼‰/ (12)
)
"""
NUM_TOKEN_RE = re.compile(NUM_TOKEN, re.X)

# è¦‹å‡ºã—ï¼ˆè¡Œé ­ï¼‰: å›³/è¡¨/å›³è¡¨ + ç•ªå· + ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ1è¡Œï¼‰
HEADING_RE = re.compile(
    rf"^\s*(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})\s*[:ï¼š.\-ï¼ã€]?\s*(?P<title>.+?)\s*$",
    re.X
)

# æœ¬æ–‡å‚ç…§ï¼ˆè¡Œä¸­ï¼‰: å›³/è¡¨/å›³è¡¨ + ç•ªå·
REF_RE = re.compile(
    rf"(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})(?![0-9])",
    re.X
)

def canon_num(num: str) -> str:
    """ç•ªå·ã‚’ã‚­ãƒ¼ç”¨ã«æ­£è¦åŒ–ï¼šå…¨è§’â†’åŠè§’ã€ï¼ˆ1ï¼‰â†’1ã€ãƒ‰ãƒƒãƒˆé¡â†’'.'ã€ãƒã‚¤ãƒ•ãƒ³é¡â†’'-'ã€ç©ºç™½é™¤å»ã€‚"""
    s = z2h_numhy(num)
    s = re.sub(r"[()ï¼ˆï¼‰]", "", s)     # ï¼ˆ1ï¼‰â†’1
    s = re.sub(DOT, ".", s)           # ï¼ãƒ» â†’ .
    s = re.sub(r"\s*\.\s*", ".", s)   # '2 . 2' â†’ '2.2'
    s = re.sub(r"\s*-\s*", "-", s)    # '2 - 1' â†’ '2-1'
    s = re.sub(r"\s+", "", s)         # æ®‹ã‚Šã®ç©ºç™½é™¤å»
    return s

def canon_label(kind: str, num: str) -> str:
    return f"{kind}{canon_num(num)}"

# ---- è¦‹å‡ºã—æŠ½å‡ºï¼ˆè¡Œç•ªå·ä¿æŒãƒ»page_labelä»˜ä¸ï¼‰
heading_rows: List[Dict[str, Any]] = []
heading_line_index: set[Tuple[int,int]] = set()  # {(pidx, line_no)} è¦‹å‡ºã—è¡Œã®åº§æ¨™ï¼ˆå‚ç…§é™¤å¤–ã«ä½¿ã†ï¼‰

for pidx, page_text in enumerate(pages_text, start=1):
    #page_label = page_labels[pidx-1] if pidx-1 < len(page_labels) else str(pidx)
    page_label = page_to_valid_label.get(pidx, "-")
    lines = page_text.replace("\r\n","\n").replace("\r","\n").split("\n")
    for ln_no, raw_line in enumerate(lines, start=1):
        line = normalize_strict(raw_line)
        if not line:
            continue
        m = HEADING_RE.match(line)
        if not m:
            continue
        kind = m.group("kind")
        num  = m.group("num")
        title= m.group("title").strip()
        heading_rows.append({
            "å›³è¡¨ç¨®é¡": kind,
            "å›³è¡¨ç•ªå·": f"{kind}{z2h_numhy(num)}",
            "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
            "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«": title,
            "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)": pidx,
            "è¦‹å‡ºã—page_label": page_label,
            "è¦‹å‡ºã—è¡Œç•ªå·": ln_no,
        })
        heading_line_index.add((pidx, ln_no))

df_heads = pd.DataFrame(heading_rows) if heading_rows else pd.DataFrame(columns=[
    "å›³è¡¨ç¨®é¡","å›³è¡¨ç•ªå·","å›³è¡¨ã‚­ãƒ¼","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«","è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)","è¦‹å‡ºã—page_label","è¦‹å‡ºã—è¡Œç•ªå·"
])

# ---- æœ¬æ–‡å‚ç…§æŠ½å‡ºï¼ˆè¦‹å‡ºã—è¡Œã‚’é™¤å¤–ï¼‰ï¼‹ æ–‡è„ˆï¼ˆÂ±Nè¡Œï¼‰
ref_rows: List[Dict[str, Any]] = []
for pidx, page_text in enumerate(pages_text, start=1):
    # page_label = page_labels[pidx-1] if pidx-1 < len(page_labels) else str(pidx)
    page_label = page_to_valid_label.get(pidx, "-")
    lines = page_text.replace("\r\n","\n").replace("\r","\n").split("\n")

    for ln_no, raw_line in enumerate(lines, start=1):
        # è¦‹å‡ºã—è¡Œã¯æœ¬æ–‡å‚ç…§ã‹ã‚‰é™¤å¤–
        if (pidx, ln_no) in heading_line_index:
            continue

        line = raw_line  # æ–‡è„ˆã¯å…ƒã®æ”¹è¡Œãƒ»ç©ºç™½æ„Ÿã‚’ä¿æŒã—ãŸã„ã®ã§ normalize ã—ãªã„
        for m in REF_RE.finditer(line):
            kind = m.group("kind")
            num  = m.group("num")
            # æ–‡è„ˆæŠ½å‡ºï¼ˆÂ±context_radius è¡Œï¼‰
            i0 = max(1, ln_no - context_radius)
            i1 = min(len(lines), ln_no + context_radius)
            ctx = "\n".join(lines[i0-1:i1])

            ref_rows.append({
                "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": m.group(0),
                "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
                "å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)": pidx,
                "å‚ç…§page_label": page_label,   # â† valid ç”±æ¥ã® page_label
                "å‚ç…§è¡Œç•ªå·": ln_no,
                "å‚ç…§å‰å¾Œï¼ˆè¡Œï¼‰": ctx,
            })

df_refs = pd.DataFrame(ref_rows) if ref_rows else pd.DataFrame(columns=[
    "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å›³è¡¨ã‚­ãƒ¼","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§page_label","å‚ç…§è¡Œç•ªå·","å‚ç…§å‰å¾Œï¼ˆè¡Œï¼‰"
])

col1, col2 = st.columns(2)
with col1:
    st.subheader("æŠ½å‡ºã•ã‚ŒãŸå›³è¡¨è¦‹å‡ºã—")
    st.dataframe(df_heads, use_container_width=True)
with col2:
    st.subheader("æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡å†…å‚ç…§ï¼ˆè¦‹å‡ºã—è¡Œã¯é™¤å¤–æ¸ˆã¿ï¼‰")
    st.dataframe(df_refs, use_container_width=True)

if show_debug:
    st.caption("æ­£è¦è¡¨ç¾")
    st.code(f"HEADING_RE = {HEADING_RE.pattern}\nREF_RE = {REF_RE.pattern}")


# =========================
# ç…§åˆï¼ˆè¦‹å‡ºã— â†” å‚ç…§ï¼‰ â€” é‡è¤‡å®šç¾©ãƒ»å…¨ä»¶æ–‡è„ˆã¤ã
# =========================
# è¦‹å‡ºã—ã®é‡è¤‡å®šç¾©ãƒã‚§ãƒƒã‚¯ï¼ˆ0ä»¶ã§ã‚‚åˆ—ã‚’æŒãŸã›ã‚‹ï¼‰
# ---- è¦‹å‡ºã—ã®é‡è¤‡å®šç¾©ï¼ˆpage_labelã‚‚é›†ç´„ï¼‰----
if df_heads.empty:
    dup_heads = pd.DataFrame(columns=["å›³è¡¨ã‚­ãƒ¼","è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)","è¦‹å‡ºã—page_label","å®šç¾©ä»¶æ•°"])
else:
    dup_heads = (
        df_heads.groupby("å›³è¡¨ã‚­ãƒ¼", as_index=False)
        .agg(**{
            "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)": ("è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)", list),
            "è¦‹å‡ºã—page_label": ("è¦‹å‡ºã—page_label", lambda s: sorted({x for x in s if x and x != "-"}))
        })
    )
    dup_heads["å®šç¾©ä»¶æ•°"] = dup_heads["è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)"].str.len()

# ---- å‚ç…§ã®é›†ç´„ï¼ˆpage_labelã‚‚é›†ç´„ï¼‰----
if df_refs.empty:
    refs_grouped = pd.DataFrame(columns=["å›³è¡¨ã‚­ãƒ¼","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§page_label"])
else:
    refs_grouped = (
        df_refs.groupby("å›³è¡¨ã‚­ãƒ¼", as_index=False)
        .agg(**{
            "å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)": ("å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)", lambda x: sorted(set(x))),
            "å‚ç…§page_label": ("å‚ç…§page_label", lambda s: sorted({x for x in s if x and x != "-"})),
        })
    )

# ---- çµåˆï¼ˆãƒ¡ã‚¿ä»˜ä¸ï¼‰----
meta = df_heads.drop_duplicates(subset=["å›³è¡¨ã‚­ãƒ¼"])[
    ["å›³è¡¨ã‚­ãƒ¼","å›³è¡¨ç¨®é¡","å›³è¡¨ç•ªå·","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"]
]
df_merge = pd.merge(dup_heads, refs_grouped, on="å›³è¡¨ã‚­ãƒ¼", how="outer")
df_merge = pd.merge(meta, df_merge, on="å›³è¡¨ã‚­ãƒ¼", how="right")

# ---- æ¬ æã‚’ãƒªã‚¹ãƒˆåŒ–ï¼ˆå®‰å…¨åŒ–ï¼‰----
for col in ["è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)","è¦‹å‡ºã—page_label","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§page_label"]:
    if col not in df_merge.columns:
        df_merge[col] = [[]] * len(df_merge)
    else:
        df_merge[col] = df_merge[col].apply(lambda v: v if isinstance(v, list) else ([] if pd.isna(v) else [v]))

# ---- è¡¨ç¤ºç”¨ã®æ–‡å­—åˆ—åŒ–ï¼ˆpage_labelã‚’å„ªå…ˆçš„ã«å‡ºã™ï¼‰----
def list_to_str(v): 
    return ",".join(str(x) for x in v) if isinstance(v, list) and len(v)>0 else "-"

df_merge["å®šç¾©page_label"] = df_merge["è¦‹å‡ºã—page_label"].apply(list_to_str)
df_merge["å‚ç…§page_label"] = df_merge["å‚ç…§page_label"].apply(list_to_str)

# çŠ¶æ…‹
def judge(row) -> str:
    defs = row.get("å®šç¾©ä»¶æ•°", 0) or 0
    has_def = defs > 0
    has_ref = isinstance(row.get("å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)"), list) and len(row["å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)"])>0
    if has_def and has_ref:
        return "ä¸€è‡´ï¼ˆå‚ç…§ã‚ã‚Šï¼‰" if defs == 1 else "ä¸€è‡´ï¼ˆé‡è¤‡å®šç¾©+å‚ç…§ã‚ã‚Šï¼‰"
    if has_def and not has_ref:
        return "æœªå‚ç…§ï¼ˆè¦‹å‡ºã—ã®ã¿ï¼‰" if defs == 1 else "é‡è¤‡å®šç¾©ï¼ˆå‚ç…§ãªã—ï¼‰"
    if (not has_def) and has_ref:
        return "æœªå®šç¾©å‚ç…§ï¼ˆè¦‹å‡ºã—ãªã—ï¼‰"
    return "ä¸æ˜"

df_merge["çŠ¶æ…‹"] = df_merge.apply(judge, axis=1)

# ---- æœ€çµ‚å‡ºåŠ›ï¼ˆExcel ã§ã‚‚ page_label åˆ—ã‚’ä½¿ã†ï¼‰----
df_result = df_merge[[
    "å›³è¡¨ç¨®é¡","å›³è¡¨ç•ªå·","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«","å›³è¡¨ã‚­ãƒ¼",
    "å®šç¾©page_label","å‚ç…§page_label","çŠ¶æ…‹"
]].sort_values(["å›³è¡¨ç¨®é¡","å›³è¡¨ã‚­ãƒ¼"], ignore_index=True)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆpage_labelè¡¨ç¤ºï¼‰")
st.dataframe(df_result, use_container_width=True)

# === Excel ===
xlsx_buf = io.BytesIO()
with pd.ExcelWriter(xlsx_buf, engine="xlsxwriter") as writer:
    sheet_name = "fig_table_check"
    df_result.to_excel(writer, index=False, sheet_name=sheet_name)
    wb = writer.book
    ws = writer.sheets[sheet_name]
    text_fmt   = wb.add_format({"num_format": "@"})
    header_fmt = wb.add_format({"bold": True})
    wrap_fmt   = wb.add_format({"text_wrap": True})

    cols = list(df_result.columns)
    col_idx = {name: i for i, name in enumerate(cols)}

    # æ–‡å­—åˆ—å›ºå®šï¼ˆèª¤æ—¥ä»˜åŒ–å¯¾ç­–ï¼‰
    for name in ["å›³è¡¨ã‚­ãƒ¼","å›³è¡¨ç•ªå·","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«","çŠ¶æ…‹","å®šç¾©page_label","å‚ç…§page_label"]:
        if name in col_idx:
            ws.set_column(col_idx[name], col_idx[name], 26, text_fmt if name!="è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«" else wrap_fmt)

    # ãƒ˜ãƒƒãƒ€/ãƒ•ã‚£ãƒ«ã‚¿/å›ºå®š
    for j, name in enumerate(cols): ws.write(0, j, name, header_fmt)
    ws.autofilter(0, 0, len(df_result), len(cols)-1)
    ws.freeze_panes(1, 0)

st.download_button(
    "ğŸ“¥ ç…§åˆçµæœã‚’Excelã§ä¿å­˜ (.xlsx)",
    data=xlsx_buf.getvalue(),
    file_name="fig_table_check.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)
