# -*- coding: utf-8 -*-
# pages/26_ç›®æ¬¡ãƒã‚§ãƒƒã‚¯.py â€” GPT API ä¸ä½¿ç”¨ç‰ˆï¼šç›®æ¬¡å€™è£œ â†” æœ¬æ–‡ï¼ˆè¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰ç…§åˆ
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‰", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆ / è¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰")
st.caption("GPT ã‚’ä½¿ã‚ãšã€ç›®æ¬¡å€™è£œï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’æœ¬æ–‡ã«å¯¾ã—ã¦ **è¡Œã”ã¨ã«é †ç•ªã«** ç…§åˆã—ã¾ã™ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2, c3 = st.columns([1.3, 1.1, 1.2])
with c1:
    scheme = "auto"
with c2:
    toc_join_front = st.checkbox("ç›®æ¬¡æŠ½å‡ºã¯å†’é ­10pã‚’é€£çµ", value=True)
with c3:
    run = st.button("â–¶ è§£æãƒ»ç…§åˆã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    max_toc_lines = st.number_input("ç›®æ¬¡å€™è£œã®ä¸Šé™è¡Œæ•°", min_value=10, max_value=500, value=120, step=10)
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", value=False)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts


with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")


# =========================
# æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå·®ã—æ›¿ãˆï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"   # å„ç¨®ãƒã‚¤ãƒ•ãƒ³

# ãƒªãƒ¼ãƒ€ãƒ¼ã«ä½¿ã‚ã‚Œã†ã‚‹è¨˜å·é›†åˆï¼ˆãƒ‰ãƒƒãƒˆ/ä¸­é»’/ä¸‰ç‚¹ãƒªãƒ¼ãƒ€/å…¨è§’ãªã©ï¼‰
LEADER_CHARS_CLASS = r"[\.ï¼ãƒ»ï½¥â€¦â€§ï½¡]"
# ã€Œãƒ‰ãƒƒãƒˆ(é¡) + ä»»æ„ç©ºç™½ã€ã‚’1å˜ä½ã¨ã—ã¦è¤‡æ•°å›ç¹°ã‚Šè¿”ã™ = ç©ºç™½å…¥ã‚Šãƒªãƒ¼ãƒ€ãƒ¼åˆ—ã«ã‚‚ãƒãƒƒãƒ
LEADERS_SPACED = rf"(?:\s*{LEADER_CHARS_CLASS}\s*){{3,}}"

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")  # å…¨è§’ç©ºç™½â†’åŠè§’
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    s = z2h_numhy(s)
    # è¡Œæœ«ã®ãƒªãƒ¼ãƒ€ãƒ¼å¡Šï¼ˆç©ºç™½æŒŸã¿ã‚‚ï¼‰ã‚’ä¸¸ã”ã¨é™¤å»
    s = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", s)
    # é€£ç¶šç©ºç™½ã‚’1ã¤ã«
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_loose(s: str) -> str:
    s = z2h_numhy(s)
    s = re.sub(rf"{LEADERS_SPACED}$", "", s)
    return re.sub(r"\s+", "", s)


# =========================
# ç›®æ¬¡å€™è£œã®æŠ½å‡ºï¼ˆæœ«å°¾ãƒ©ãƒ™ãƒ«æ¤œå‡ºã‚’æ‹¡å¼µï¼‰â†å·®ã—æ›¿ãˆ
# =========================
ALPHAJP = r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]+"  # è‹±å­—/ã‹ãª/æ¼¢å­—ã®é€£ç¶š

def build_label_tail_regex_mixed() -> re.Pattern:
    # è¨±å®¹ã™ã‚‹æœ«å°¾ãƒ©ãƒ™ãƒ«:
    #  1) é€£ç•ª:          12
    #  2) ç« -ãƒšãƒ¼ã‚¸:     1-2, 3-10-2
    #  3) ã‚·ãƒªãƒ¼ã‚º-æ•°å­—: åº-1, è³‡-2, ä»˜-3, A-10 ç­‰
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"
    core_series = rf"{ALPHAJP}{HY}[0-9ï¼-ï¼™]+"
    tail = rf"(?P<label>(?:{core_seq}|{core_chap}|{core_series}))"

    pat = rf"""
        ^(?P<head>.*?)                            # å·¦å´æœ¬æ–‡
        (?:{LEADERS_SPACED}|\s{{2,}})?            # ãƒ‰ãƒƒãƒˆ/ä¸­é»’/â€¦ + ç©ºç™½ã®ãƒªãƒ¼ãƒ€ãƒ¼åˆ— or 2é€£ä»¥ä¸Šã®ç©ºç™½
        {tail}\s*$                                # è¡Œæœ«ãƒ©ãƒ™ãƒ«
    """
    return re.compile(pat, re.X)

def build_label_line_regex_mixed() -> re.Pattern:
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"
    core_series = rf"{ALPHAJP}{HY}[0-9ï¼-ï¼™]+"
    core = rf"(?:{core_seq}|{core_chap}|{core_series})"
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

# ç½®ãæ›ãˆ
LABEL_TAIL_RE = build_label_tail_regex_mixed()
LABEL_LINE_RE = build_label_line_regex_mixed()

# =========================
# ç›®æ¬¡å€™è£œæŠ½å‡ºï¼ˆå…ˆé ­åˆ¤å®šã‚’å°‘ã—åºƒã’ã‚‹ï¼‰â†è»½å¾®ä¿®æ­£
# =========================
def extract_toc_lines(fulltext: str, limit: int) -> List[str]:
    lines = [l.rstrip() for l in fulltext.replace("\r\n","\n").replace("\r","\n").split("\n")]

    # å…ˆé ­ãŒã€Œåº/è³‡æ–™/ä»˜éŒ²/ç¬¬/æ•°å­—/[æ™¯1]ç­‰ã®è§’æ‹¬å¼§ã€ã‹ã‚‰å§‹ã¾ã‚‹ã‚‚ã®ã‚’å€™è£œåŒ–
    head_ok   = re.compile(r"^(åº|è³‡æ–™|ä»˜éŒ²|ç¬¬|[0-9ï¼-ï¼™]|\[|ï¼»)")
    text_char = re.compile(r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]")
    out: List[str] = []

    for ln in lines:
        s = ln.strip()
        if not s or not head_ok.match(s) or not text_char.search(s):
            continue

        m = LABEL_TAIL_RE.match(s)
        if not m:
            continue

        head  = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", m.group("head")).strip()
        label = z2h_numhy(m.group("label"))

        if len(head) <= 1:
            continue

        out.append(f"{head} ::: {label}")
        if len(out) >= limit:
            break
    return out



# ç›®æ¬¡å€™è£œæŠ½å‡ºï¼ˆå‰åŠãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
front_n = min(10, len(pages_text))
sample_text = "\n".join(pages_text[:front_n]) if toc_join_front else pages_text[0]
toc_lines = extract_toc_lines(sample_text, limit=max_toc_lines)

st.subheader("æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡å€™è£œï¼ˆä¸Šä½ï¼‰")
if not toc_lines:
    st.warning("ç›®æ¬¡å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()
st.code("\n".join(toc_lines[:80]))


# =========================
# å˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§æœ¬æ–‡ã‚’åˆ†å‰²ï¼ˆæŠ½å‡ºãƒšãƒ¼ã‚¸ã‚’ä½œã‚‹ï¼‰
# =========================
# def build_label_line_regex(scheme: str) -> re.Pattern:
#     if scheme.startswith("(1)"):
#         core = r"[0-9ï¼-ï¼™]{1,6}"                 # é€£ç•ª
#     else:
#         core = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"  # ç« -ãƒšãƒ¼ã‚¸
#     return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

# LABEL_LINE_RE = build_label_line_regex(scheme)

def split_segments_by_label(all_text: str) -> List[Dict[str, Any]]:
    """
    å…¨æ–‡ï¼ˆé€£çµï¼‰ã‚’ã€å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§åˆ†å‰²ã—ã€[{'page_label','body'}] ã‚’è¿”ã™ã€‚
    """
    txt = normalize_strict(all_text.replace("\r\n","\n").replace("\r","\n"))
    matches = list(LABEL_LINE_RE.finditer(txt))
    if not matches:
        return []

    def next_nonempty_pos(pos: int) -> int:
        n = pos
        while n < len(txt) and txt[n] == "\n":
            n += 1
        return n

    segs: List[Dict[str, Any]] = []
    for i, m in enumerate(matches):
        label = z2h_numhy(m.group("label"))
        start = next_nonempty_pos(m.end())
        end   = matches[i+1].start() if i+1 < len(matches) else len(txt)
        body  = txt[start:end].lstrip("\n ")
        segs.append({"page_label": label, "body": body})
    return segs


all_text_joined = "\n".join(pages_text)
segments = split_segments_by_label(all_text_joined)

st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆå˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§åŒºåˆ‡ã‚Šï¼‰â€” æ¦‚è¦³")
if not segments:
    st.warning("å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãšã€æŠ½å‡ºãƒšãƒ¼ã‚¸ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    df_segments_overview = pd.DataFrame([{
        "page_label": s["page_label"],
        "char_count": len(s["body"]),
        "preview": s["body"][:120].replace("\n"," ") + ("â€¦" if len(s["body"])>120 else "")
    } for s in segments])
    st.dataframe(df_segments_overview, use_container_width=True)


# =========================
# ãƒ©ãƒ™ãƒ«å¦¥å½“æ€§ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰â†’ valid=True ã®ã¿ã‚’æ¡ç”¨
# =========================
st.subheader("ğŸ“‘ ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«æ¤œè¨¼ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ï¼‰")
def _parse_label_kind(label: str) -> Tuple[str, Any]:
    lab = z2h_numhy(label)
    if re.fullmatch(r"[0-9]+", lab):
        return "seq", int(lab)
    parts = lab.split("-")
    if len(parts) >= 2 and all(p.isdigit() for p in parts):
        return "chap", [int(p) for p in parts]
    m = re.fullmatch(rf"({ALPHAJP})-([0-9]+)", lab)
    if m:
        return "series", (m.group(1), int(m.group(2)))
    return "unknown", None

def valid_and_reason_auto(label: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    k, cur = _parse_label_kind(label)
    if k == "unknown":
        return False, "ä¸æ˜ãªãƒ©ãƒ™ãƒ«å½¢å¼"
    if prev_ok is None:
        return True, ""
    pk, prev = _parse_label_kind(prev_ok)
    if pk == "unknown":
        return True, ""
    if k != pk:
        return True, "å½¢å¼åˆ‡æ›¿"
    if k == "seq":
        return (cur == prev + 1, "" if cur == prev + 1 else "éé€£ç•ª")
    if k == "chap":
        c, p = (cur + [1, 1])[:2]; pc, pp = (prev + [1, 1])[:2]
        ok = (c == pc and p == pp + 1) or (c == pc + 1 and p == 1)
        return (ok, "" if ok else "éé€£ç•ª")
    if k == "series":
        s, n = cur; ps, pn = prev
        if s != ps:
            return True, "å½¢å¼åˆ‡æ›¿"
        return (n == pn + 1, "" if n == pn + 1 else "éé€£ç•ª")
    return True, ""


rows_check: List[Dict[str, Any]] = []
prev_ok: Optional[str] = None
for s in segments:
    ok, reason = valid_and_reason_auto(s["page_label"], prev_ok)
    if ok:
        prev_ok = s["page_label"]
    rows_check.append({
        "page_label": s["page_label"],
        "valid": ok,
        "reason": "" if ok else reason,
        "char_count": len(s["body"]),
        "preview": s["body"][:100].replace("\n"," ") + ("â€¦" if len(s["body"])>100 else "")
    })
df_check = pd.DataFrame(rows_check)
st.dataframe(df_check, use_container_width=True)

# valid=True ã®æŠ½å‡ºãƒšãƒ¼ã‚¸ã®ã¿æ¡ç”¨
valid_segments = [s for s in segments if any((r["page_label"]==s["page_label"] and r["valid"]) for r in rows_check)]
seg_index: Dict[str, str] = {s["page_label"]: s["body"] for s in valid_segments}

# æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalidã®ã¿ï¼‰TXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
if valid_segments:
    txt_buf = io.StringIO()
    for s in valid_segments:
        header = f"==== page_label={s['page_label']} (chars={len(s['body'])}) ====\n"
        txt_buf.write(header)
        txt_buf.write(s["body"].rstrip("\n") + "\n\n")
    st.download_button(
        "ğŸ“¥ æŠ½å‡ºãƒšãƒ¼ã‚¸TXTã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆvalid=True ã®ã¿ï¼‰",
        data=txt_buf.getvalue().encode("utf-8"),
        file_name="extracted_pages_valid.txt",
        mime="text/plain"
    )


# =========================
# ç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ« â†” æœ¬æ–‡ï¼ˆè¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰ç…§åˆ
# =========================
def scan_lines_for_match(title_raw: str, body: str) -> Tuple[str, str]:
    """
    æœ¬æ–‡ body ã‚’æ”¹è¡Œã§åˆ†å‰²ã—ã¦ä¸Šã‹ã‚‰é †ã«ãƒã‚§ãƒƒã‚¯ã€‚
    æˆ»ã‚Šå€¤: (åˆ¤å®š, ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ)
      - "ä¸€è‡´"                    : å³ã—ã‚ã®å®Œå…¨ä¸€è‡´ï¼ˆstrictï¼‰
      - "ä¸€è‡´ï¼ˆç©ºç™½å·®å¸åï¼‰"      : ç©ºç™½å…¨é™¤å»ä¸€è‡´ï¼ˆlooseï¼‰
      - "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰"    : è¡Œã«ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹
      - "éƒ¨åˆ†ä¸€è‡´ï¼ˆNæ–‡å­—ï¼‰"        : ã‚¿ã‚¤ãƒˆãƒ«å…ˆé ­ N=5/4/3 ã®éƒ¨åˆ†ä¸€è‡´
      - "æœªæ¤œå‡º"
    """
    title_strict = normalize_strict(title_raw)
    title_loose  = normalize_loose(title_raw)

    lines = [ln for ln in body.split("\n")]  # æ”¹è¡Œã¯ä¿æŒã—ãŸã¾ã¾æ¯”è¼ƒç”¨ã«åˆ¥ã§æ­£è¦åŒ–
    for ln in lines:
        if not ln.strip():
            continue
        ln_strict = normalize_strict(ln)
        if ln_strict == title_strict:
            return "ä¸€è‡´", ln.rstrip("\n")
        ln_loose = normalize_loose(ln)
        if ln_loose == title_loose:
            return "ä¸€è‡´ï¼ˆç©ºç™½å·®å¸åï¼‰", ln.rstrip("\n")
        if title_raw in ln:
            return "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰", ln.rstrip("\n")

        for klen in (5, 4, 3):
            if len(title_raw) >= klen and title_raw[:klen] in ln:
                return f"éƒ¨åˆ†ä¸€è‡´ï¼ˆ{klen}æ–‡å­—ï¼‰", ln.rstrip("\n")

    return "æœªæ¤œå‡º", "-"


def check_toc_by_order(toc_lines: List[str],
                       seg_index: Dict[str, str],
                       pages_text: List[str]) -> pd.DataFrame:
    """
    1) ç›®æ¬¡ãƒ©ãƒ™ãƒ«ã¨ä¸€è‡´ã™ã‚‹æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=Trueï¼‰å†…ã®è¡Œã‚’ä¸Šã‹ã‚‰é †ã«ã‚¹ã‚­ãƒ£ãƒ³
    2) è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸æœ¬æ–‡ã‚’é †ã«è¡Œã‚¹ã‚­ãƒ£ãƒ³
    """
    out_rows: List[Dict[str, Any]] = []

    for toc in toc_lines:
        if " ::: " not in toc:
            continue
        title_raw, label = toc.split(" ::: ", 1)
        title_raw = title_raw.strip()
        label     = label.strip()

        # 1) ãƒ©ãƒ™ãƒ«ä¸€è‡´ãƒšãƒ¼ã‚¸ã§æ¢ç´¢
        body = seg_index.get(label, "")
        status = "æœªæ¤œå‡º"
        matched = "-"
        found_page_num: Optional[int] = None

        if body:
            status, matched = scan_lines_for_match(title_raw, body)

        # 2) ã¾ã æœªæ¤œå‡ºãªã‚‰å…¨ãƒšãƒ¼ã‚¸ã‚’é †ã«æ¢ç´¢
        if status == "æœªæ¤œå‡º":
            for i, ptxt in enumerate(pages_text):
                stt, m = scan_lines_for_match(title_raw, ptxt)
                if stt != "æœªæ¤œå‡º":
                    status, matched = stt, m
                    found_page_num = i + 1
                    break

        out_rows.append({
            "ã‚¿ã‚¤ãƒˆãƒ«": title_raw,
            "ç›®æ¬¡ãƒ©ãƒ™ãƒ«": label,
            "æœ¬æ–‡å†…ãƒ©ãƒ™ãƒ«": label if body else "-",
            "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸": found_page_num if found_page_num is not None else "-",
            "åˆ¤å®š": status,
            "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ": matched,
        })

    return pd.DataFrame(out_rows)


df_result = check_toc_by_order(toc_lines, seg_index, pages_text)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆè¡Œãƒ™ãƒ¼ã‚¹ï¼‰")
st.dataframe(df_result, use_container_width=True)
st.caption("â€» ç›®æ¬¡ã®å„è¡Œã‚’é †ã«è¾¿ã‚Šã€ã¾ãšåŒãƒ©ãƒ™ãƒ«ã®æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=Trueï¼‰ã§è¡Œã‚¹ã‚­ãƒ£ãƒ³ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸ã‚’è¡Œã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã™ã€‚")

# é›†è¨ˆ
summary = df_result["åˆ¤å®š"].value_counts().to_dict()
st.markdown(f"**çµæœæ¦‚è¦**: {summary}")

# # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSVï¼‰
# buf = io.StringIO()
# df_result.to_csv(buf, index=False)
# st.download_button(
#     "ğŸ“¥ ç…§åˆçµæœCSVã‚’ä¿å­˜",
#     data=buf.getvalue().encode("utf-8-sig"),
#     file_name="toc_check_local_result.csv",
#     mime="text/csv"
# )

# === Excel(.xlsx) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šæ—¥ä»˜åŒ–ã‚’å®Œå…¨é˜²æ­¢ï¼ˆåˆ—ã‚’ã€Œæ–‡å­—åˆ—(@)ã€ã«æŒ‡å®šï¼‰ ===
xlsx_buf = io.BytesIO()

# ã“ã“ã§ df_result ã‚’ä½¿ã„ã¾ã™ï¼ˆä¸Šã§ä½œæˆæ¸ˆã¿ï¼‰
with pd.ExcelWriter(xlsx_buf, engine="xlsxwriter") as writer:
    sheet_name = "result"
    df_result.to_excel(writer, index=False, sheet_name=sheet_name)

    wb = writer.book
    ws = writer.sheets[sheet_name]

    # æ–‡å­—åˆ—å›ºå®šã®æ›¸å¼ï¼ˆ@ï¼‰
    text_fmt = wb.add_format({"num_format": "@"})
    # è¦‹å‡ºã—ï¼ˆå¤ªå­—ï¼‰
    header_fmt = wb.add_format({"bold": True})
    # é•·æ–‡ã‚»ãƒ«ï¼ˆæŠ˜è¿”ã—ï¼‰ç”¨
    wrap_fmt = wb.add_format({"text_wrap": True})

    # åˆ—ä½ç½®ã‚’å–å¾—
    cols = list(df_result.columns)
    col_idx = {name: i for i, name in enumerate(cols)}  # 0-based

    # æ—¥ä»˜åŒ–ã•ã‚Œã‚„ã™ã„ã€Œãƒ©ãƒ™ãƒ«ã€åˆ—ã‚’æ–‡å­—åˆ—ã«å›ºå®š
    for name in ["ç›®æ¬¡ãƒ©ãƒ™ãƒ«", "æœ¬æ–‡å†…ãƒ©ãƒ™ãƒ«"]:
        if name in col_idx:
            j = col_idx[name]
            ws.set_column(j, j, 16, text_fmt)  # å¹…ã¯ãŠå¥½ã¿ã§

    # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã¯æŠ˜è¿”ã—ï¼‹åºƒã‚
    if "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ" in col_idx:
        j = col_idx["ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ"]
        ws.set_column(j, j, 40, wrap_fmt)

    # ä»–ã®åˆ—ã®å¹…ã‚‚è»½ãèª¿æ•´ï¼ˆä»»æ„ï¼‰
    for name in ["ã‚¿ã‚¤ãƒˆãƒ«", "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸", "åˆ¤å®š"]:
        if name in col_idx:
            width = 28 if name == "ã‚¿ã‚¤ãƒˆãƒ«" else 10 if name == "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸" else 12
            ws.set_column(col_idx[name], col_idx[name], width)

    # ãƒ˜ãƒƒãƒ€è¡Œã‚¹ã‚¿ã‚¤ãƒ«
    for j, name in enumerate(cols):
        ws.write(0, j, name, header_fmt)

    # ä¾¿åˆ©ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    ws.freeze_panes(1, 0)  # 1è¡Œå›ºå®š
    ws.autofilter(0, 0, len(df_result), len(cols) - 1)

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆXLSXï¼‰
st.download_button(
    "ğŸ“¥ ç…§åˆçµæœã‚’Excelã§ä¿å­˜ (.xlsx æ¨å¥¨)",
    data=xlsx_buf.getvalue(),
    file_name="toc_check_local_result.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)

# ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.write({"valid_segments": len(valid_segments), "segments_all": len(segments)})
