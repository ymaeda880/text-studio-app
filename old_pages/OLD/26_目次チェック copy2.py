# -*- coding: utf-8 -*-
# pages/26_ç›®æ¬¡ãƒã‚§ãƒƒã‚¯.py â€” GPT API ä¸ä½¿ç”¨ç‰ˆï¼šæœ¬æ–‡ã®å„è¡Œã§ç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç…§åˆ
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Tuple

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆãƒ»è¡Œå˜ä½ï¼‰", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆãƒ»è¡Œå˜ä½ï¼‰")
st.caption("ç›®æ¬¡å€™è£œã®ã‚¿ã‚¤ãƒˆãƒ«ãŒã€æœ¬æ–‡ã® **å„è¡Œ** ã«ç¾ã‚Œã‚‹ã‹ã‚’ç›´æ¥ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2 = st.columns([1.3, 1])
with c1:
    scheme = st.radio("ãƒšãƒ¼ã‚¸æ–¹å¼", ["(1) 1,2,3,4, â€¦", "(2) 1-1,1-2,2-1,2-2, â€¦"], index=1, horizontal=True)
with c2:
    join_pages = st.checkbox("å…¨ãƒšãƒ¼ã‚¸é€£çµã§æŠ½å‡ºï¼ˆTOCæŠ½å‡ºç”¨ã«å…ˆé ­æ•°ãƒšãƒ¼ã‚¸ã‚’ä½¿ç”¨ï¼‰", value=True, help="TOCå€™è£œã®æŠ½å‡ºã¯å…ˆé ­æ•°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¡Œã„ã¾ã™ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæœ¬æ–‡ã®ä¸Šé™æ–‡å­—æ•°ï¼ˆ0ã§å…¨æ–‡ï¼‰
with st.sidebar:
    excerpt_chars = st.number_input(
        "ãƒšãƒ¼ã‚¸æœ¬æ–‡ã®ä¸Šé™æ–‡å­—æ•°ï¼ˆ0ã§å…¨æ–‡ï¼‰",
        min_value=0, max_value=200000, value=0, step=500,
        help="å„ãƒšãƒ¼ã‚¸ã®å…ˆé ­ã‹ã‚‰ç…§åˆå¯¾è±¡ã«ã™ã‚‹æœ€å¤§æ–‡å­—æ•°ã€‚0 ã¯å…¨æ–‡ã€‚"
    )
    max_toc = st.number_input("TOCå€™è£œã®æœ€å¤§ä»¶æ•°", min_value=10, max_value=500, value=300, step=10)
    max_toc_show = st.number_input("ç”»é¢ã«è¡¨ç¤ºã™ã‚‹TOCä»¶æ•°", min_value=10, max_value=200, value=60, step=10)

run = st.button("â–¶ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯é–‹å§‹", type="primary", use_container_width=True)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()

# =========================
# PDFâ†’ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts

with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text = pdf_to_text_per_page(pdf_path)

st.success(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†ï¼ˆãƒšãƒ¼ã‚¸æ•°: {len(pages_text)}ï¼‰")

# =========================
# ç›®æ¬¡å€™è£œã®æŠ½å‡ºï¼ˆå…ˆé ­æ•°ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"
LEADERS = r"[\.ï¼ãƒ»â€¦]+"

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
    return re.sub(HY, "-", s)

def build_label_tail_regex(scheme: str) -> re.Pattern:
    if scheme.startswith("(1)"):
        tail = r"(?P<label>[0-9ï¼-ï¼™]{1,6})"
    else:
        tail = rf"(?P<label>[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+)"
    pat = rf"""
        ^(?P<head>.*?)                               # å·¦å´æœ¬æ–‡
        (?:\s*{LEADERS}\s*|\s{{2,}})?                # ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ or ç©ºç™½
        {tail}\s*$                                   # è¡Œæœ«ã«ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«
    """
    return re.compile(pat, re.X)

LABEL_TAIL_RE = build_label_tail_regex(scheme)

def extract_toc_lines_from_text(fulltext: str, limit: int) -> List[str]:
    """
    ç›®æ¬¡å€™è£œè¡Œã‚’æŠ½å‡ºï¼ˆè¡Œé ­ãŒã€Œç¬¬ã€or æ•°å­—ã§å§‹ã¾ã‚Šã€è¡Œæœ«ã«ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ï¼‰ã€‚
    è¿”ã‚Šå€¤: "ã‚¿ã‚¤ãƒˆãƒ« ::: ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«" ã®é…åˆ—
    """
    lines = [l.rstrip() for l in fulltext.replace("\r\n","\n").replace("\r","\n").split("\n")]
    head_ok = re.compile(r"^(ç¬¬|[0-9ï¼-ï¼™])")
    text_char = re.compile(r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]")
    out: List[str] = []

    for ln in lines:
        s = ln.strip()
        if not s:
            continue
        if not head_ok.match(s):
            continue
        if not text_char.search(s):
            continue
        m = LABEL_TAIL_RE.match(s)
        if not m:
            continue
        head = re.sub(rf"\s*{LEADERS}\s*$", "", m.group("head")).strip()
        label = z2h_numhy(m.group("label"))
        if len(head) <= 1:
            continue
        out.append(f"{head} ::: {label}")
        if len(out) >= limit:
            break
    return out

# TOCã¯å…ˆé ­ ~10 ãƒšãƒ¼ã‚¸ã‚’å¯¾è±¡ã«æŠ½å‡ºï¼ˆçµæ§‹å¤šã„è³‡æ–™ã§ã‚‚ååˆ†ï¼‰
head_pages = min(10, len(pages_text))
sample_text = "\n".join(pages_text[:head_pages])
toc_lines = extract_toc_lines_from_text(sample_text, limit=int(max_toc))

st.subheader("æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡å€™è£œï¼ˆä¸Šä½ï¼‰")
if len(toc_lines) == 0:
    st.warning("ç›®æ¬¡å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()
st.code("\n".join(toc_lines[: int(max_toc_show) ]))

# =========================
# æœ¬æ–‡ã‚’ã€Œè¡Œã”ã¨ã€ã«ç…§åˆï¼ˆå³å¯†ï¼šè¡Œå†…æ¤œç´¢ï¼‰
# =========================
def normalize_for_match(s: str) -> str:
    """ãƒãƒƒãƒã®é ‘å¥åŒ–ï¼ˆå…¨è§’â†’åŠè§’æ•°å­—, å„ç¨®ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€, é€£ç¶šç©ºç™½ã®1åŒ–, å‰å¾Œç©ºç™½å‰Šé™¤ï¼‰"""
    s = z2h_numhy(s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def find_in_page_lines(title: str, body: str) -> Tuple[str, int, str]:
    """
    ãƒšãƒ¼ã‚¸æœ¬æ–‡ï¼ˆbodyï¼‰ã‚’è¡Œåˆ†å‰²ã—ã€å„è¡Œã¨ç…§åˆã€‚
    - å®Œå…¨ä¸€è‡´ï¼šè¡Œæ–‡å­—åˆ—ã« title ã‚’å«ã‚€
    - éƒ¨åˆ†ä¸€è‡´ï¼štitle å…ˆé ­ 5 or 4 æ–‡å­—ãŒè¡Œã«å«ã¾ã‚Œã‚‹
    æˆ»ã‚Šå€¤: (åˆ¤å®š, è¡Œç•ªå·(1å§‹), ä¸€è‡´ã—ãŸè¡Œå…¨æ–‡) / æœªæ¤œå‡ºãªã‚‰ ("æœªæ¤œå‡º", -1, "")
    """
    # æœ¬æ–‡ã®å¯¾è±¡é•·ã•ã‚’çµã‚‹
    if excerpt_chars and excerpt_chars > 0:
        body = body[:excerpt_chars]

    lines = body.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    n_title = normalize_for_match(title)

    # ---- å®Œå…¨ä¸€è‡´ï¼ˆè¡Œå†…ã«ã‚¿ã‚¤ãƒˆãƒ«å…¨æ–‡ãŒå‡ºç¾ï¼‰
    for idx, raw in enumerate(lines, start=1):
        n_line = normalize_for_match(raw)
        if not n_line:
            continue
        if n_title and n_title in n_line:
            return ("ä¸€è‡´", idx, raw)  # å…ƒã®è¡Œã‚’è¿”ã™

    # ---- éƒ¨åˆ†ä¸€è‡´ï¼ˆå…ˆé ­ 5 â†’ 4 æ–‡å­—ï¼‰
    if len(n_title) >= 4:
        for k in [n_title[:5], n_title[:4]]:
            for idx, raw in enumerate(lines, start=1):
                n_line = normalize_for_match(raw)
                if k and k in n_line:
                    return ("éƒ¨åˆ†ä¸€è‡´", idx, raw)

    return ("æœªæ¤œå‡º", -1, "")

def check_toc_in_pages_linewise(toc_lines: List[str], pages_text: List[str]) -> pd.DataFrame:
    results = []
    for toc in toc_lines:
        if " ::: " not in toc:
            continue
        title, label = toc.split(" ::: ", 1)
        title = title.strip()

        found = False
        hit_page, hit_line_no, hit_line_text, status = None, None, "", "æœªæ¤œå‡º"

        for i, text in enumerate(pages_text):
            judge, line_no, line_text = find_in_page_lines(title, text)
            if judge != "æœªæ¤œå‡º":
                found = True
                hit_page = i + 1
                hit_line_no = line_no
                hit_line_text = line_text.replace("\t", " ")
                status = judge
                break

        results.append({
            "ã‚¿ã‚¤ãƒˆãƒ«": title,
            "ç›®æ¬¡ãƒ©ãƒ™ãƒ«": label,
            "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸": hit_page if found else "-",
            "æœ¬æ–‡å†…è¡Œç•ªå·": hit_line_no if found else "-",
            "åˆ¤å®š": status,
            "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ": hit_line_text if hit_line_text else "-"
        })

    return pd.DataFrame(results)

df_result = check_toc_in_pages_linewise(toc_lines, pages_text)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆè¡Œãƒ™ãƒ¼ã‚¹ï¼‰")
st.dataframe(df_result, use_container_width=True)
st.caption("â€»ã€ä¸€è‡´ã€ã¯è¡Œå†…ã«ã‚¿ã‚¤ãƒˆãƒ«å…¨æ–‡ãŒå‡ºç¾ã€‚ã€éƒ¨åˆ†ä¸€è‡´ã€ã¯ã‚¿ã‚¤ãƒˆãƒ«ã®å…ˆé ­ 4ã€œ5 æ–‡å­—ãŒè¡Œå†…ã«å‡ºç¾ã€‚è©²å½“è¡Œå…¨æ–‡ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

# çµ±è¨ˆ
summary = df_result["åˆ¤å®š"].value_counts().to_dict()
st.markdown(f"**çµæœæ¦‚è¦**: {summary}")

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
buf = io.StringIO()
df_result.to_csv(buf, index=False)
st.download_button(
    "ğŸ“¥ ç…§åˆçµæœã‚’CSVã§ä¿å­˜",
    data=buf.getvalue().encode("utf-8-sig"),
    file_name="toc_check_local_linewise_result.csv",
    mime="text/csv",
)
