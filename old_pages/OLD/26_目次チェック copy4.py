# -*- coding: utf-8 -*-
# pages/26_ç›®æ¬¡ãƒã‚§ãƒƒã‚¯.py â€” GPT API ä¸ä½¿ç”¨ç‰ˆï¼šç›®æ¬¡å€™è£œ â†” æœ¬æ–‡ï¼ˆè¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰ç…§åˆ
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‰", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆ / è¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰")
st.caption("GPT ã‚’ä½¿ã‚ãšã€ç›®æ¬¡å€™è£œï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’æœ¬æ–‡ã«å¯¾ã—ã¦ **è¡Œã”ã¨ã«é †ç•ªã«** ç…§åˆã—ã¾ã™ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2, c3 = st.columns([1.3, 1.1, 1.2])
with c1:
    scheme = st.radio("ãƒšãƒ¼ã‚¸æ–¹å¼", ["(1) 1,2,3,4, â€¦", "(2) 1-1,1-2,2-1,2-2, â€¦"], index=1, horizontal=True)
with c2:
    toc_join_front = st.checkbox("ç›®æ¬¡æŠ½å‡ºã¯å†’é ­10pã‚’é€£çµ", value=True)
with c3:
    run = st.button("â–¶ è§£æãƒ»ç…§åˆã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    max_toc_lines = st.number_input("ç›®æ¬¡å€™è£œã®ä¸Šé™è¡Œæ•°", min_value=10, max_value=500, value=120, step=10)
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", value=False)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts


with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")


# =========================
# æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"   # å„ç¨®ãƒã‚¤ãƒ•ãƒ³
LEADERS = r"[\.ï¼ãƒ»â€¦]+"                                     # ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ç¾¤

def z2h_numhy(s: str) -> str:
    """å…¨è§’æ•°å­—â†’åŠè§’ã€ãƒã‚¤ãƒ•ãƒ³é¡ã‚’ '-' ã«æ­£è¦åŒ–ã€å…¨è§’ç©ºç™½â†’åŠè§’ç©ºç™½"""
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    """è»½ã„æ­£è¦åŒ–ï¼šz2h + é€£ç¶šç©ºç™½ã‚’å˜ä¸€ç©ºç™½ + æœ«å°¾ã®ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼é™¤å»"""
    s = z2h_numhy(s)
    s = re.sub(rf"\s*{LEADERS}\s*$", "", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_loose(s: str) -> str:
    """å¼·ã‚ã®æ­£è¦åŒ–ï¼šç©ºç™½å…¨é™¤å»ï¼ˆè¡Œæœ«ãƒªãƒ¼ãƒ€ãƒ¼ã‚‚é™¤å»ï¼‰"""
    s = z2h_numhy(s)
    s = re.sub(rf"{LEADERS}$", "", s)
    return re.sub(r"\s+", "", s)


# =========================
# ç›®æ¬¡å€™è£œã®æŠ½å‡ºï¼ˆè¡Œæœ«ã«ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒã‚ã‚‹è¡Œï¼‰
# =========================
def build_label_tail_regex(scheme: str) -> re.Pattern:
    if scheme.startswith("(1)"):
        tail = r"(?P<label>[0-9ï¼-ï¼™]{1,6})"          # é€£ç•ª
    else:
        tail = rf"(?P<label>[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+)"  # ç« -ãƒšãƒ¼ã‚¸
    pat = rf"""
        ^(?P<head>.*?)                               # å·¦å´æœ¬æ–‡
        (?:\s*{LEADERS}\s*|\s{{2,}})?                # ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ or é€£ç¶šç©ºç™½
        {tail}\s*$                                   # è¡Œæœ«ãƒ©ãƒ™ãƒ«
    """
    return re.compile(pat, re.X)

LABEL_TAIL_RE = build_label_tail_regex(scheme)

def extract_toc_lines(fulltext: str, limit: int) -> List[str]:
    """
    ç›®æ¬¡å€™è£œã‚’æŠ½å‡ºï¼šè¡Œé ­ãŒã€ç¬¬ã€or æ•°å­—ã§å§‹ã¾ã‚Šã€æ–‡å­—ï¼ˆå’Œ/è‹±ï¼‰ã‚’å«ã‚€è¡Œã®ã¿ã€‚
    å‡ºåŠ›å½¢å¼ï¼š 'ã‚¿ã‚¤ãƒˆãƒ« ::: ãƒ©ãƒ™ãƒ«'
    """
    lines = [l.rstrip() for l in fulltext.replace("\r\n","\n").replace("\r","\n").split("\n")]
    head_ok   = re.compile(r"^(ç¬¬|[0-9ï¼-ï¼™])")
    text_char = re.compile(r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]")
    out: List[str] = []

    for ln in lines:
        s = ln.strip()
        if not s: 
            continue
        if not head_ok.match(s): 
            continue
        if not text_char.search(s): 
            continue

        m = LABEL_TAIL_RE.match(s)
        if not m:
            continue

        head  = re.sub(rf"\s*{LEADERS}\s*$", "", m.group("head")).strip()
        label = z2h_numhy(m.group("label"))

        if len(head) <= 1:
            continue

        out.append(f"{head} ::: {label}")
        if len(out) >= limit:
            break
    return out


# ç›®æ¬¡å€™è£œæŠ½å‡ºï¼ˆå‰åŠãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
front_n = min(10, len(pages_text))
sample_text = "\n".join(pages_text[:front_n]) if toc_join_front else pages_text[0]
toc_lines = extract_toc_lines(sample_text, limit=max_toc_lines)

st.subheader("æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡å€™è£œï¼ˆä¸Šä½ï¼‰")
if not toc_lines:
    st.warning("ç›®æ¬¡å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()
st.code("\n".join(toc_lines[:80]))


# =========================
# å˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§æœ¬æ–‡ã‚’åˆ†å‰²ï¼ˆæŠ½å‡ºãƒšãƒ¼ã‚¸ã‚’ä½œã‚‹ï¼‰
# =========================
def build_label_line_regex(scheme: str) -> re.Pattern:
    if scheme.startswith("(1)"):
        core = r"[0-9ï¼-ï¼™]{1,6}"                 # é€£ç•ª
    else:
        core = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"  # ç« -ãƒšãƒ¼ã‚¸
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

LABEL_LINE_RE = build_label_line_regex(scheme)

def split_segments_by_label(all_text: str) -> List[Dict[str, Any]]:
    """
    å…¨æ–‡ï¼ˆé€£çµï¼‰ã‚’ã€å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§åˆ†å‰²ã—ã€[{'page_label','body'}] ã‚’è¿”ã™ã€‚
    """
    txt = normalize_strict(all_text.replace("\r\n","\n").replace("\r","\n"))
    matches = list(LABEL_LINE_RE.finditer(txt))
    if not matches:
        return []

    def next_nonempty_pos(pos: int) -> int:
        n = pos
        while n < len(txt) and txt[n] == "\n":
            n += 1
        return n

    segs: List[Dict[str, Any]] = []
    for i, m in enumerate(matches):
        label = z2h_numhy(m.group("label"))
        start = next_nonempty_pos(m.end())
        end   = matches[i+1].start() if i+1 < len(matches) else len(txt)
        body  = txt[start:end].lstrip("\n ")
        segs.append({"page_label": label, "body": body})
    return segs


all_text_joined = "\n".join(pages_text)
segments = split_segments_by_label(all_text_joined)

st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆå˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§åŒºåˆ‡ã‚Šï¼‰â€” æ¦‚è¦³")
if not segments:
    st.warning("å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãšã€æŠ½å‡ºãƒšãƒ¼ã‚¸ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    df_segments_overview = pd.DataFrame([{
        "page_label": s["page_label"],
        "char_count": len(s["body"]),
        "preview": s["body"][:120].replace("\n"," ") + ("â€¦" if len(s["body"])>120 else "")
    } for s in segments])
    st.dataframe(df_segments_overview, use_container_width=True)


# =========================
# ãƒ©ãƒ™ãƒ«å¦¥å½“æ€§ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰â†’ valid=True ã®ã¿ã‚’æ¡ç”¨
# =========================
st.subheader("ğŸ“‘ ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«æ¤œè¨¼ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ï¼‰")
def valid_and_reason(label: str, scheme: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    """
    æ–¹å¼(1)ï¼šn == prev+1 ã§ OK
    æ–¹å¼(2)ï¼š(c,p) â†’ (c,p+1) or (c+1,1) ã§ OK
    """
    if scheme.startswith("(1)"):
        try:
            n = int(label)
        except Exception:
            return False, "é€£ç•ªã§æ•°å€¤åŒ–ã§ããªã„"
        if prev_ok is None:
            return True, ""
        try:
            prev = int(prev_ok)
        except Exception:
            return True, ""
        return (True, "") if n == prev + 1 else (False, "éé€£ç•ª")

    # ç« -ãƒšãƒ¼ã‚¸
    parts = label.split("-")
    if not (len(parts) >= 2 and all(p.isdigit() for p in parts)):
        return False, "ç« -ãƒšãƒ¼ã‚¸å½¢å¼ã§ãªã„"
    chap, page_n = int(parts[0]), int(parts[1])
    if prev_ok is None:
        return True, ""
    pparts = prev_ok.split("-")
    if not (len(pparts) >= 2 and all(p.isdigit() for p in pparts)):
        return True, ""
    pchap, ppage = int(pparts[0]), int(pparts[1])
    if (chap == pchap and page_n == ppage + 1) or (chap == pchap + 1 and page_n == 1):
        return True, ""
    return False, "éé€£ç•ª"


rows_check: List[Dict[str, Any]] = []
prev_ok: Optional[str] = None
for s in segments:
    ok, reason = valid_and_reason(s["page_label"], scheme, prev_ok)
    if ok:
        prev_ok = s["page_label"]
    rows_check.append({
        "page_label": s["page_label"],
        "valid": ok,
        "reason": "" if ok else reason,
        "char_count": len(s["body"]),
        "preview": s["body"][:100].replace("\n"," ") + ("â€¦" if len(s["body"])>100 else "")
    })
df_check = pd.DataFrame(rows_check)
st.dataframe(df_check, use_container_width=True)

# valid=True ã®æŠ½å‡ºãƒšãƒ¼ã‚¸ã®ã¿æ¡ç”¨
valid_segments = [s for s in segments if any((r["page_label"]==s["page_label"] and r["valid"]) for r in rows_check)]
seg_index: Dict[str, str] = {s["page_label"]: s["body"] for s in valid_segments}

# æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalidã®ã¿ï¼‰TXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
if valid_segments:
    txt_buf = io.StringIO()
    for s in valid_segments:
        header = f"==== page_label={s['page_label']} (chars={len(s['body'])}) ====\n"
        txt_buf.write(header)
        txt_buf.write(s["body"].rstrip("\n") + "\n\n")
    st.download_button(
        "ğŸ“¥ æŠ½å‡ºãƒšãƒ¼ã‚¸TXTã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆvalid=True ã®ã¿ï¼‰",
        data=txt_buf.getvalue().encode("utf-8"),
        file_name="extracted_pages_valid.txt",
        mime="text/plain"
    )


# =========================
# ç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ« â†” æœ¬æ–‡ï¼ˆè¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰ç…§åˆ
# =========================
def scan_lines_for_match(title_raw: str, body: str) -> Tuple[str, str]:
    """
    æœ¬æ–‡ body ã‚’æ”¹è¡Œã§åˆ†å‰²ã—ã¦ä¸Šã‹ã‚‰é †ã«ãƒã‚§ãƒƒã‚¯ã€‚
    æˆ»ã‚Šå€¤: (åˆ¤å®š, ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ)
      - "ä¸€è‡´"                    : å³ã—ã‚ã®å®Œå…¨ä¸€è‡´ï¼ˆstrictï¼‰
      - "ä¸€è‡´ï¼ˆç©ºç™½å·®å¸åï¼‰"      : ç©ºç™½å…¨é™¤å»ä¸€è‡´ï¼ˆlooseï¼‰
      - "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰"    : è¡Œã«ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹
      - "éƒ¨åˆ†ä¸€è‡´ï¼ˆNæ–‡å­—ï¼‰"        : ã‚¿ã‚¤ãƒˆãƒ«å…ˆé ­ N=5/4/3 ã®éƒ¨åˆ†ä¸€è‡´
      - "æœªæ¤œå‡º"
    """
    title_strict = normalize_strict(title_raw)
    title_loose  = normalize_loose(title_raw)

    lines = [ln for ln in body.split("\n")]  # æ”¹è¡Œã¯ä¿æŒã—ãŸã¾ã¾æ¯”è¼ƒç”¨ã«åˆ¥ã§æ­£è¦åŒ–
    for ln in lines:
        if not ln.strip():
            continue
        ln_strict = normalize_strict(ln)
        if ln_strict == title_strict:
            return "ä¸€è‡´", ln.rstrip("\n")
        ln_loose = normalize_loose(ln)
        if ln_loose == title_loose:
            return "ä¸€è‡´ï¼ˆç©ºç™½å·®å¸åï¼‰", ln.rstrip("\n")
        if title_raw in ln:
            return "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰", ln.rstrip("\n")

        for klen in (5, 4, 3):
            if len(title_raw) >= klen and title_raw[:klen] in ln:
                return f"éƒ¨åˆ†ä¸€è‡´ï¼ˆ{klen}æ–‡å­—ï¼‰", ln.rstrip("\n")

    return "æœªæ¤œå‡º", "-"


def check_toc_by_order(toc_lines: List[str],
                       seg_index: Dict[str, str],
                       pages_text: List[str]) -> pd.DataFrame:
    """
    1) ç›®æ¬¡ãƒ©ãƒ™ãƒ«ã¨ä¸€è‡´ã™ã‚‹æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=Trueï¼‰å†…ã®è¡Œã‚’ä¸Šã‹ã‚‰é †ã«ã‚¹ã‚­ãƒ£ãƒ³
    2) è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸æœ¬æ–‡ã‚’é †ã«è¡Œã‚¹ã‚­ãƒ£ãƒ³
    """
    out_rows: List[Dict[str, Any]] = []

    for toc in toc_lines:
        if " ::: " not in toc:
            continue
        title_raw, label = toc.split(" ::: ", 1)
        title_raw = title_raw.strip()
        label     = label.strip()

        # 1) ãƒ©ãƒ™ãƒ«ä¸€è‡´ãƒšãƒ¼ã‚¸ã§æ¢ç´¢
        body = seg_index.get(label, "")
        status = "æœªæ¤œå‡º"
        matched = "-"
        found_page_num: Optional[int] = None

        if body:
            status, matched = scan_lines_for_match(title_raw, body)

        # 2) ã¾ã æœªæ¤œå‡ºãªã‚‰å…¨ãƒšãƒ¼ã‚¸ã‚’é †ã«æ¢ç´¢
        if status == "æœªæ¤œå‡º":
            for i, ptxt in enumerate(pages_text):
                stt, m = scan_lines_for_match(title_raw, ptxt)
                if stt != "æœªæ¤œå‡º":
                    status, matched = stt, m
                    found_page_num = i + 1
                    break

        out_rows.append({
            "ã‚¿ã‚¤ãƒˆãƒ«": title_raw,
            "ç›®æ¬¡ãƒ©ãƒ™ãƒ«": label,
            "æœ¬æ–‡å†…ãƒ©ãƒ™ãƒ«": label if body else "-",
            "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸": found_page_num if found_page_num is not None else "-",
            "åˆ¤å®š": status,
            "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ": matched,
        })

    return pd.DataFrame(out_rows)


df_result = check_toc_by_order(toc_lines, seg_index, pages_text)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆè¡Œãƒ™ãƒ¼ã‚¹ï¼‰")
st.dataframe(df_result, use_container_width=True)
st.caption("â€» ç›®æ¬¡ã®å„è¡Œã‚’é †ã«è¾¿ã‚Šã€ã¾ãšåŒãƒ©ãƒ™ãƒ«ã®æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=Trueï¼‰ã§è¡Œã‚¹ã‚­ãƒ£ãƒ³ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸ã‚’è¡Œã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã™ã€‚")

# é›†è¨ˆ
summary = df_result["åˆ¤å®š"].value_counts().to_dict()
st.markdown(f"**çµæœæ¦‚è¦**: {summary}")

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSVï¼‰
buf = io.StringIO()
df_result.to_csv(buf, index=False)
st.download_button(
    "ğŸ“¥ ç…§åˆçµæœCSVã‚’ä¿å­˜",
    data=buf.getvalue().encode("utf-8-sig"),
    file_name="toc_check_local_result.csv",
    mime="text/csv"
)

# ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.write({"valid_segments": len(valid_segments), "segments_all": len(segments)})
