# -*- coding: utf-8 -*-
# pages/26_ç›®æ¬¡ãƒã‚§ãƒƒã‚¯.py â€” GPT API ä¸ä½¿ç”¨ç‰ˆï¼šæœ¬æ–‡ç…§åˆã§ç›®æ¬¡ãƒã‚§ãƒƒã‚¯
from __future__ import annotations
import io, os, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆç‰ˆï¼‰", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆç‰ˆï¼‰")
st.caption("GPT APIã‚’ä½¿ã‚ãšã«ã€ç›®æ¬¡å€™è£œã®ã‚¿ã‚¤ãƒˆãƒ«ãŒå®Ÿéš›ã®æœ¬æ–‡ã«å‡ºç¾ã™ã‚‹ã‹ã‚’ç›´æ¥ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2 = st.columns([1.3, 1])
with c1:
    scheme = st.radio("ãƒšãƒ¼ã‚¸æ–¹å¼", ["(1) 1,2,3,4, â€¦", "(2) 1-1,1-2,2-1,2-2, â€¦"], index=1, horizontal=True)
with c2:
    join_pages = st.checkbox("å…¨ãƒšãƒ¼ã‚¸é€£çµã§æŠ½å‡ºï¼ˆæ¨å¥¨ï¼‰", value=True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæœ¬æ–‡ã®ä¸Šé™æ–‡å­—æ•°
with st.sidebar:
    excerpt_chars = st.number_input(
        "ãƒšãƒ¼ã‚¸æœ¬æ–‡ã®ä¸Šé™æ–‡å­—æ•°ï¼ˆ0ã§å…¨æ–‡ï¼‰",
        min_value=0, max_value=20000, value=800, step=100,
        help="å„ãƒšãƒ¼ã‚¸æœ¬æ–‡ã®å…ˆé ­ã‹ã‚‰ä½•æ–‡å­—ã¾ã§ã‚’ç…§åˆå¯¾è±¡ã«ã™ã‚‹ã‹ã€‚"
    )

run = st.button("â–¶ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯é–‹å§‹", type="primary", use_container_width=True)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()

# =========================
# PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts

with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text = pdf_to_text_per_page(pdf_path)

st.success(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†ï¼ˆãƒšãƒ¼ã‚¸æ•°: {len(pages_text)}ï¼‰")

# =========================
# ç›®æ¬¡å€™è£œã®æŠ½å‡º
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"
LEADERS = r"[\.ï¼ãƒ»â€¦]+"

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
    return re.sub(HY, "-", s)

def build_label_tail_regex(scheme: str) -> re.Pattern:
    if scheme.startswith("(1)"):
        tail = r"(?P<label>[0-9ï¼-ï¼™]{1,6})"
    else:
        tail = rf"(?P<label>[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+)"
    pat = rf"""
        ^(?P<head>.*?)                               # å·¦å´æœ¬æ–‡
        (?:\s*{LEADERS}\s*|\s{{2,}})?                # ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ or ç©ºç™½
        {tail}\s*$                                   # è¡Œæœ«ã«ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«
    """
    return re.compile(pat, re.X)

LABEL_TAIL_RE = build_label_tail_regex(scheme)

def extract_toc_lines(fulltext: str, limit: int = 300) -> List[str]:
    """ç›®æ¬¡å€™è£œè¡Œã‚’æŠ½å‡º"""
    lines = [l.rstrip() for l in fulltext.replace("\r\n","\n").replace("\r","\n").split("\n")]
    head_ok = re.compile(r"^(ç¬¬|[0-9ï¼-ï¼™])")
    text_char = re.compile(r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]")
    out: List[str] = []

    for ln in lines:
        s = ln.strip()
        if not s:
            continue
        if not head_ok.match(s):
            continue
        if not text_char.search(s):
            continue
        m = LABEL_TAIL_RE.match(s)
        if not m:
            continue
        head = re.sub(rf"\s*{LEADERS}\s*$", "", m.group("head")).strip()
        label = z2h_numhy(m.group("label"))
        if len(head) <= 1:
            continue
        out.append(f"{head} ::: {label}")
        if len(out) >= limit:
            break
    return out

# å…¨æ–‡ã‹ã‚‰ç›®æ¬¡å€™è£œã‚’æ¢ã™ï¼ˆæœ€åˆã®æ•°ãƒšãƒ¼ã‚¸ã‚’ä½¿ç”¨ï¼‰
sample_text = "\n".join(pages_text[:min(10, len(pages_text))])
toc_lines = extract_toc_lines(sample_text)

st.subheader("æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡å€™è£œï¼ˆä¸Šä½ï¼‰")
if len(toc_lines) == 0:
    st.warning("ç›®æ¬¡å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()
st.code("\n".join(toc_lines[:60]))

# =========================
# ç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ç…§åˆ
# =========================
def check_toc_in_pages(toc_lines: List[str], pages_text: List[str]) -> pd.DataFrame:
    results = []
    for toc in toc_lines:
        if " ::: " not in toc:
            continue
        title, label = toc.split(" ::: ", 1)
        title = title.strip()
        found_page = None
        partial = False
        snippet = ""

        for i, text in enumerate(pages_text):
            body = text[:excerpt_chars] if excerpt_chars > 0 else text

            # å®Œå…¨ä¸€è‡´
            if title in body:
                found_page = i + 1
                status = "ä¸€è‡´"
                pos = body.find(title)
                snippet = body[max(0, pos-20):pos+len(title)+20].replace("\n", " ")
                break

            # éƒ¨åˆ†ä¸€è‡´ï¼ˆå…ˆé ­4ã€œ5æ–‡å­—ãŒæœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ï¼‰
            elif len(title) >= 4:
                for key in [title[:5], title[:4]]:
                    if key in body:
                        found_page = i + 1
                        partial = True
                        pos = body.find(key)
                        snippet = body[max(0, pos-20):pos+len(key)+20].replace("\n", " ")
                        break
                if partial:
                    status = "éƒ¨åˆ†ä¸€è‡´"
                    break

        if not found_page:
            status = "æœªæ¤œå‡º"

        results.append({
            "ã‚¿ã‚¤ãƒˆãƒ«": title,
            "ç›®æ¬¡ãƒ©ãƒ™ãƒ«": label,
            "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸": found_page if found_page else "-",
            "åˆ¤å®š": status,
            "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆæŠœç²‹": snippet if snippet else "-"
        })

    return pd.DataFrame(results)


df_result = check_toc_in_pages(toc_lines, pages_text)
st.subheader("ğŸ” ç…§åˆçµæœ")
st.dataframe(df_result, use_container_width=True)
st.caption("â€»ã€ä¸€è‡´ã€ã¯ã‚¿ã‚¤ãƒˆãƒ«ãŒãƒšãƒ¼ã‚¸æœ¬æ–‡ã«å®Œå…¨ä¸€è‡´ã€ã€éƒ¨åˆ†ä¸€è‡´ã€ã¯ä¸€éƒ¨èªå¥ã®ã¿ä¸€è‡´ã€‚")

# çµ±è¨ˆ
summary = df_result["åˆ¤å®š"].value_counts().to_dict()
st.markdown(f"**çµæœæ¦‚è¦**: {summary}")

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
buf = io.StringIO()
df_result.to_csv(buf, index=False)
st.download_button(
    "ğŸ“¥ ç…§åˆçµæœã‚’CSVã§ä¿å­˜",
    data=buf.getvalue().encode("utf-8-sig"),
    file_name="toc_check_local_result.csv",
    mime="text/csv",
)
