# -*- coding: utf-8 -*-
# pages/24_å›³è¡¨ãƒã‚§ãƒƒã‚¯.py â€” GPTä¸ä½¿ç”¨ï¼šå›³è¡¨è¦‹å‡ºã— â†” æœ¬æ–‡å‚ç…§ã®ç…§åˆï¼ˆDLã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼é›†ç´„ & ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ï¼‰

from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ§¾ å›³è¡¨ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‰", page_icon="ğŸ§¾", layout="wide")
st.title("ğŸ§¾ å›³è¡¨ãƒã‚§ãƒƒã‚¯ï¼ˆè¦‹å‡ºã— â†” å‚ç…§ ã®ç…§åˆï¼‰")
st.caption("ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ã¨åŒä¸€ã®ãƒšãƒ¼ã‚¸æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆé€£ç•ª/ç« -ãƒšãƒ¼ã‚¸/ã‚·ãƒªãƒ¼ã‚º-ç•ªå·ã®æ··åœ¨å¯¾å¿œï¼‰ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«é›†ç´„ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

with st.sidebar:
    st.markdown("### âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ï¼ˆæŠ½å‡ºä¸­é–“ï¼‰ã‚’è¡¨ç¤º", value=False)
run = st.button("â–¶ è§£æãƒ»ç…§åˆã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ­£è¦åŒ–ãƒ»æŠ½å‡ºå…±é€šï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"   # å„ç¨®ãƒã‚¤ãƒ•ãƒ³
LEADER_CHARS_CLASS = r"[\.ï¼ãƒ»ï½¥â€¦â€§ï½¡]"
LEADERS_SPACED = rf"(?:\s*{LEADER_CHARS_CLASS}\s*){{3,}}"
ALPHAJP = r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]+"  # è‹±/ã‹ãª/æ¼¢
DOT = r"[\.ï¼ãƒ»ï½¥]"
NUM = r"[0-9ï¼-ï¼™]+"

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰ï¼»ï¼½ï½›ï½", "0123456789()[]{}"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    s = z2h_numhy(s)
    s = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_loose(s: str) -> str:
    s = z2h_numhy(s)
    s = re.sub(rf"{LEADERS_SPACED}$", "", s)
    return re.sub(r"\s+", "", s)


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts


# =========================
# ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šï¼ˆè¡Œå˜ç‹¬ãƒ©ãƒ™ãƒ«ï¼‰
# =========================
def build_label_line_regex_mixed() -> re.Pattern:
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"                 # 12
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+" # 1-2 / 2-10-3
    core_series = rf"{ALPHAJP}{HY}[0-9ï¼-ï¼™]+"       # åº-1 / è³‡-2 ãªã©
    core = rf"(?:{core_seq}|{core_chap}|{core_series})"
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

LABEL_LINE_RE = build_label_line_regex_mixed()

def split_segments_by_label(all_text: str) -> List[Dict[str, Any]]:
    """
    å…¨æ–‡ï¼ˆé€£çµï¼‰ã‚’ã€å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§åˆ†å‰²ã—ã€[{'page_label','body'}] ã‚’è¿”ã™ã€‚
    """
    txt = normalize_strict(all_text.replace("\r\n", "\n").replace("\r", "\n"))
    matches = list(LABEL_LINE_RE.finditer(txt))
    if not matches:
        return []
    def next_nonempty_pos(pos: int) -> int:
        n = pos
        while n < len(txt) and txt[n] == "\n":
            n += 1
        return n
    segs: List[Dict[str, Any]] = []
    for i, m in enumerate(matches):
        label = z2h_numhy(m.group("label"))
        start = next_nonempty_pos(m.end())
        end   = matches[i+1].start() if i+1 < len(matches) else len(txt)
        body  = txt[start:end].lstrip("\n ")
        segs.append({"page_label": label, "body": body})
    return segs

def _parse_label_kind(label: str) -> Tuple[str, Any]:
    lab = z2h_numhy(label)
    if re.fullmatch(r"[0-9]+", lab):
        return "seq", int(lab)
    parts = lab.split("-")
    if len(parts) >= 2 and all(p.isdigit() for p in parts):
        return "chap", [int(p) for p in parts]
    m = re.fullmatch(rf"({ALPHAJP})-([0-9]+)", lab)
    if m:
        return "series", (m.group(1), int(m.group(2)))
    return "unknown", None

def valid_and_reason_auto(label: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    k, cur = _parse_label_kind(label)
    if k == "unknown":
        return False, "ä¸æ˜ãªãƒ©ãƒ™ãƒ«å½¢å¼"
    if prev_ok is None:
        return True, ""
    pk, prev = _parse_label_kind(prev_ok)
    if pk == "unknown":
        return True, ""
    if k != pk:
        return True, "å½¢å¼åˆ‡æ›¿"
    if k == "seq":
        return (cur == prev + 1, "" if cur == prev + 1 else "éé€£ç•ª")
    if k == "chap":
        c, p = (cur + [1, 1])[:2]; pc, pp = (prev + [1, 1])[:2]
        ok = (c == pc and p == pp + 1) or (c == pc + 1 and p == 1)
        return (ok, "" if ok else "éé€£ç•ª")
    if k == "series":
        s, n = cur; ps, pn = prev
        if s != ps:
            return True, "å½¢å¼åˆ‡æ›¿"
        return (n == pn + 1, "" if n == pn + 1 else "éé€£ç•ª")
    return True, ""


# =========================
# å›³è¡¨ è¦‹å‡ºã— / å‚ç…§ æŠ½å‡º
# =========================
# ä¾‹: 2.2-1 / 2. 2-1 / ï¼’ï¼ï¼‘ï¼ï¼‘ / ï¼ˆï¼‘ï¼‰
NUM_TOKEN = rf"""
(?:                                     # â‘  ãƒ‰ãƒƒãƒˆåŒºåˆ‡ã‚Š + ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Š
    {NUM}
    (?:\s*{DOT}\s*{NUM})*               # 2.2 ã‚„ 2ï¼ 2
    (?:\s*-\s*{NUM})*                   # -1 ã‚„ -ï¼‘
  |
    [ï¼ˆ(]\s*{NUM}\s*[ï¼‰)]               # â‘¡ ï¼ˆï¼‘ï¼‰/ (12)
)
"""
NUM_TOKEN_RE = re.compile(NUM_TOKEN, re.X)

HEADING_RE = re.compile(  # è¡Œé ­ã®è¦‹å‡ºã—1è¡Œ
    rf"^\s*(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})\s*[:ï¼š.\-ï¼ã€]?\s*(?P<title>.+?)\s*$",
    re.X
)
REF_RE = re.compile(      # æœ¬æ–‡å‚ç…§ï¼ˆè¡Œä¸­ï¼‰
    rf"(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})(?![0-9])",
    re.X
)

def canon_num(num: str) -> str:
    s = z2h_numhy(num)                  # å…¨è§’â†’åŠè§’ã€ãƒã‚¤ãƒ•ãƒ³â†’'-'
    s = re.sub(r"[()ï¼ˆï¼‰]", "", s)        # ï¼ˆ1ï¼‰â†’1
    s = re.sub(DOT, ".", s)             # ï¼ãƒ» â†’ .
    s = re.sub(r"\s*\.\s*", ".", s)     # '2 . 2' â†’ '2.2'
    s = re.sub(r"\s*-\s*", "-", s)      # '2 - 1' â†’ '2-1'
    s = re.sub(r"\s+", "", s)           # æ®‹ã‚Šã®ç©ºç™½é™¤å»
    return s

def canon_label(kind: str, num: str) -> str:
    return f"{kind}{canon_num(num)}"


# =========================
# å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# =========================
def get_context_windows(text: str, match_span: Tuple[int, int], win: int = 60) -> Dict[str, str]:
    s, e = match_span
    left  = max(0, s - win)
    right = min(len(text), e + win)
    around = text[left:right].replace("\n", " ")
    # 1æ–‡æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰ï¼šè¡Œã‚’å¥ç‚¹/ãƒ”ãƒªã‚ªãƒ‰/æ”¹è¡Œã§ã‚¹ãƒ—ãƒªãƒƒãƒˆã—ã€è©²å½“ã‚¹ãƒ‘ãƒ³ãŒå…¥ã‚‹æ–‡ã‚’è¿”ã™
    lines = re.split(r"(?<=[ã€‚ï¼\.ï¼ï¼Ÿ!?])\s+|\n", text)
    acc = 0
    hit_sent = ""
    for ln in lines:
        if acc <= s < acc + len(ln) + 1:  # +1 for delimiter char consumed
            hit_sent = ln.strip()
            break
        acc += len(ln) + 1
    return {
        "å‚ç…§_å‰å¾ŒÂ±60": around,
        "å‚ç…§_1æ–‡": hit_sent if hit_sent else around
    }


# =========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆçµæœã‚’å…¨éƒ¨ä½œã£ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼‰
# =========================
def compute_results(pdf_bytes: bytes, show_debug: bool = False) -> Dict[str, Any]:
    # 1) PDFèª­ã¿ â†’ ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆ
    with tempfile.TemporaryDirectory() as td:
        td = Path(td)
        pdf_path = td / "input.pdf"
        pdf_path.write_bytes(pdf_bytes)
        pages_text: List[str] = pdf_to_text_per_page(pdf_path)

    # 2) ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šï¼ˆsegmentsï¼‰ã¨ page_label ç”Ÿæˆï¼ˆvalid ã‚‚åˆ¥é€”è¨ˆç®—ï¼‰
    all_text_joined = "\n".join(pages_text)
    segments = split_segments_by_label(all_text_joined)

    # 2-1) é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆprev_ok ã¯ OK ã®ã¨ãã ã‘æ›´æ–°ï¼‰
    rows_check: List[Dict[str, Any]] = []
    prev_ok: Optional[str] = None
    for s in segments:
        ok, reason = valid_and_reason_auto(s["page_label"], prev_ok)
        if ok:
            prev_ok = s["page_label"]
        rows_check.append({
            "page_label": s["page_label"],
            "valid": ok,
            "reason": "" if ok else reason,
            "char_count": len(s["body"]),
            "preview": s["body"][:120].replace("\n", " ") + ("â€¦" if len(s["body"]) > 120 else "")
        })
    df_check = pd.DataFrame(rows_check)
    df_segments_overview = pd.DataFrame([{
        "page_label": s["page_label"],
        "char_count": len(s["body"]),
        "preview": s["body"][:120].replace("\n", " ") + ("â€¦" if len(s["body"]) > 120 else "")
    } for s in segments]) if segments else pd.DataFrame(columns=["page_label","char_count","preview"])

    # valid_segments ã¯å‚è€ƒç”¨ï¼ˆDLã«ã‚‚ä½¿ã†ï¼‰
    valid_segments = [s for s in segments if any((r["page_label"] == s["page_label"] and r["valid"]) for r in rows_check)]

    # 3) page_label ã®é…åˆ—ï¼ˆæŠ½å‡ºé †ã§å¯¾å¿œï¼‰
    page_labels = [s["page_label"] for s in segments] if segments else [str(i+1) for i in range(len(pages_text))]

    # 4) è¦‹å‡ºã—æŠ½å‡º
    heading_rows: List[Dict[str, Any]] = []
    for pidx, page_text in enumerate(pages_text, start=1):
        page_label = page_labels[pidx-1] if pidx-1 < len(page_labels) else str(pidx)
        for raw_line in page_text.replace("\r\n","\n").replace("\r","\n").split("\n"):
            line = normalize_strict(raw_line)
            if not line:
                continue
            m = HEADING_RE.match(line)
            if not m:
                continue
            kind = m.group("kind")
            num  = m.group("num")
            title= m.group("title").strip()
            heading_rows.append({
                "å›³è¡¨ç¨®é¡": kind,
                "å›³è¡¨ç•ªå·": f"{kind}{z2h_numhy(num)}",  # è¡¨ç¤ºç”¨ï¼ˆå…¨è§’â†’åŠè§’ï¼‰
                "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),     # ç…§åˆã‚­ãƒ¼
                "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«": title,
                "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)": pidx,
                "è¦‹å‡ºã—page_label": page_label,        # â† ã“ã¡ã‚‰ã« page_label
            })
    df_heads = pd.DataFrame(heading_rows) if heading_rows else pd.DataFrame(
        columns=["å›³è¡¨ç¨®é¡","å›³è¡¨ç•ªå·","å›³è¡¨ã‚­ãƒ¼","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«","è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)","è¦‹å‡ºã—page_label"]
    )

    # 5) æœ¬æ–‡å‚ç…§æŠ½å‡ºï¼ˆè¦‹å‡ºã—è¡Œã¯æœ¬æ–‡æ‰±ã„ã—ãªã„ï¼šè¦‹å‡ºã—ã«ä¸€è‡´ã—ãŸè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    ref_rows: List[Dict[str, Any]] = []
    for pidx, page_text in enumerate(pages_text, start=1):
        # è¦‹å‡ºã—è¡Œã‚’é™¤ã„ãŸæœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆï¼‘è¡Œãšã¤è¦‹ã¦ heading ã«ãƒãƒƒãƒãªã‚‰å‚ç…§æ¢ç´¢ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        lines = page_text.replace("\r\n","\n").replace("\r","\n").split("\n")
        acc = 0
        for ln in lines:
            raw_ln = ln
            line = normalize_strict(raw_ln)
            span_base = (acc, acc + len(raw_ln))
            acc += len(raw_ln) + 1  # æ”¹è¡Œåˆ† +1

            if not line:
                continue
            if HEADING_RE.match(line):
                # ã“ã‚Œã¯è¦‹å‡ºã—è¡Œãªã®ã§å‚ç…§æ¢ç´¢ã—ãªã„
                continue
            # å‚ç…§æŠ½å‡ºï¼ˆè¡Œãƒ†ã‚­ã‚¹ãƒˆåŸºæº–ï¼‰
            for m in REF_RE.finditer(raw_ln):
                kind = m.group("kind"); num = m.group("num")
                # åŸæ–‡å…¨ä½“ã§ã®ã‚¹ãƒ‘ãƒ³ï¼ˆã–ã£ãã‚Šè¡Œé ­ã‹ã‚‰ã®ç›¸å¯¾â†’ãƒšãƒ¼ã‚¸å†…å…¨ä½“æ–‡å­—åˆ—ãŒç„¡ã„ã®ã§Â±60ã¯è¡Œãƒ†ã‚­ã‚¹ãƒˆã§ï¼‰
                ctx = get_context_windows(raw_ln, m.span(), win=60)
                ref_rows.append({
                    "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": m.group(0),
                    "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
                    "å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)": pidx,
                    "å‚ç…§_1æ–‡": ctx["å‚ç…§_1æ–‡"],
                    "å‚ç…§_å‰å¾ŒÂ±60": ctx["å‚ç…§_å‰å¾ŒÂ±60"],
                    # page_label ã¯ pages_text ãƒ™ãƒ¼ã‚¹ã«å…¨ä½“å¯¾å¿œï¼ˆsegmentsæ•°ã¨ã‚ºãƒ¬ã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    "å‚ç…§page_label": page_labels[pidx-1] if pidx-1 < len(page_labels) else str(pidx),
                })

    df_refs = pd.DataFrame(ref_rows) if ref_rows else pd.DataFrame(
        columns=["å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å›³è¡¨ã‚­ãƒ¼","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§_1æ–‡","å‚ç…§_å‰å¾ŒÂ±60","å‚ç…§page_label"]
    )

    # 6) ç…§åˆï¼ˆè¦‹å‡ºã— â†” å‚ç…§ï¼‰
    # è¦‹å‡ºã—é‡è¤‡å®šç¾©ãƒã‚§ãƒƒã‚¯
    if df_heads.empty:
        dup_heads = pd.DataFrame(columns=["å›³è¡¨ã‚­ãƒ¼", "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)", "å®šç¾©ä»¶æ•°"])
    else:
        dup_heads = (
            df_heads.groupby("å›³è¡¨ã‚­ãƒ¼", as_index=False)
            .agg(**{"è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)": ("è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)", list)})
        )
        dup_heads["å®šç¾©ä»¶æ•°"] = dup_heads["è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)"].str.len()

    # å‚ç…§é›†ç´„
    if df_refs.empty:
        refs_grouped = pd.DataFrame(columns=["å›³è¡¨ã‚­ãƒ¼", "å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)", "å‚ç…§page_label"])
    else:
        # å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª) ã¯é‡è¤‡æ’é™¤ï¼†æ˜‡é †
        g = df_refs.groupby("å›³è¡¨ã‚­ãƒ¼", as_index=False)
        refs_grouped = g.agg({
            "å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)": lambda x: sorted(set(x)),
            "å‚ç…§page_label":  lambda x: ",".join(sorted(set(map(str, x)))),
        })

    # çµåˆ
    df_merge = pd.merge(dup_heads, refs_grouped, on="å›³è¡¨ã‚­ãƒ¼", how="outer")

    # è¦‹å‡ºã—ãƒ¡ã‚¿ä»˜ä¸ï¼ˆpage_labelã¯æœ€åˆã®è¦‹å‡ºã—ã®ãƒ©ãƒ™ãƒ«ã‚’ä»£è¡¨ã«ï¼‰
    meta = (df_heads.sort_values("è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)")
            .drop_duplicates(subset=["å›³è¡¨ã‚­ãƒ¼"], keep="first"))[
        ["å›³è¡¨ã‚­ãƒ¼","å›³è¡¨ç¨®é¡","å›³è¡¨ç•ªå·","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«","è¦‹å‡ºã—page_label"]
    ]
    df_merge = pd.merge(meta, df_merge, on="å›³è¡¨ã‚­ãƒ¼", how="right")

    # list æ­£è¦åŒ–ï¼ˆæ¬ æâ†’[]ï¼‰
    df_merge["è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)"] = df_merge.get("è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)", pd.Series([], dtype=object)).apply(
        lambda v: v if isinstance(v, list) else ([] if pd.isna(v) else [v])
    )
    df_merge["å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)"] = df_merge.get("å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)", pd.Series([], dtype=object)).apply(
        lambda v: v if isinstance(v, list) else ([] if pd.isna(v) else [v])
    )

    # çŠ¶æ…‹åˆ¤å®š
    def judge(row) -> str:
        defs = row.get("å®šç¾©ä»¶æ•°", 0) or 0
        has_def = defs > 0
        has_ref = isinstance(row.get("å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)"), list) and len(row["å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)"]) > 0
        if has_def and has_ref:
            return "ä¸€è‡´ï¼ˆå‚ç…§ã‚ã‚Šï¼‰" if defs == 1 else "ä¸€è‡´ï¼ˆé‡è¤‡å®šç¾©+å‚ç…§ã‚ã‚Šï¼‰"
        if has_def and not has_ref:
            return "æœªå‚ç…§ï¼ˆè¦‹å‡ºã—ã®ã¿ï¼‰" if defs == 1 else "é‡è¤‡å®šç¾©ï¼ˆå‚ç…§ãªã—ï¼‰"
        if (not has_def) and has_ref:
            return "æœªå®šç¾©å‚ç…§ï¼ˆè¦‹å‡ºã—ãªã—ï¼‰"
        return "ä¸æ˜"

    df_merge["çŠ¶æ…‹"] = df_merge.apply(judge, axis=1)

    def fmt_pages(x):
        if isinstance(x, list):
            return ",".join(str(i) for i in x)
        return x

    df_merge["å®šç¾©ãƒšãƒ¼ã‚¸(é€£ç•ª)"] = df_merge["è¦‹å‡ºã—ãƒšãƒ¼ã‚¸(é€£ç•ª)"].apply(lambda v: fmt_pages(v) if isinstance(v, list) else v)

    df_summary = df_merge[[
        "å›³è¡¨ç¨®é¡","å›³è¡¨ç•ªå·","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«","å›³è¡¨ã‚­ãƒ¼",
        "è¦‹å‡ºã—page_label","å®šç¾©ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§page_label","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","çŠ¶æ…‹"
    ]].sort_values(["å›³è¡¨ç¨®é¡","å›³è¡¨ã‚­ãƒ¼"], ignore_index=True)

    # 7) å‚ç…§è©³ç´°ï¼ˆæ–‡è„ˆï¼‰ãã®ã¾ã¾
    df_ref_details = df_refs[[
        "å›³è¡¨ã‚­ãƒ¼","å‚ç…§page_label","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å‚ç…§_1æ–‡","å‚ç…§_å‰å¾ŒÂ±60"
    ]].sort_values(["å›³è¡¨ã‚­ãƒ¼","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)"], ignore_index=True)

    return {
        "pages_text": pages_text,
        "segments": segments,
        "df_segments_overview": df_segments_overview,
        "df_check": df_check,
        "df_heads": df_heads,
        "df_refs": df_refs,
        "df_summary": df_summary,
        "df_ref_details": df_ref_details,
        "valid_segments": valid_segments,
    }


# =========================
# å®Ÿè¡Œ & ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
# =========================
if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()

st.success("PDF èª­ã¿å–ã‚Šã‚’é–‹å§‹ã—ã¾ã™â€¦")

results = compute_results(uploaded.getvalue(), show_debug=show_debug)
st.session_state.figtbl_results = results
st.session_state.figtbl_filename_stem = Path(getattr(uploaded, "name", "result.pdf")).stem or "result"

# ===== ç”»é¢è¡¨ç¤ºï¼ˆæ¦‚è¦ï¼‰
st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆå˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§åŒºåˆ‡ã‚Šï¼‰â€” æ¦‚è¦³")
if not results["segments"]:
    st.warning("å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãšã€æŠ½å‡ºãƒšãƒ¼ã‚¸ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFã®ç‰ˆé¢ã«ä¾å­˜ã—ã¾ã™ï¼‰ã€‚")
else:
    st.dataframe(results["df_segments_overview"], use_container_width=True)

st.subheader("ğŸ“‘ ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«æ¤œè¨¼ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ï¼‰")
st.dataframe(results["df_check"], use_container_width=True)

col1, col2 = st.columns(2)
with col1:
    st.subheader("æŠ½å‡ºã•ã‚ŒãŸå›³è¡¨è¦‹å‡ºã—")
    st.dataframe(results["df_heads"], use_container_width=True)
with col2:
    st.subheader("æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡å†…å‚ç…§ï¼ˆæ–‡è„ˆã‚ã‚Šï¼‰")
    st.dataframe(results["df_ref_details"], use_container_width=True)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆSummaryï¼‰")
st.dataframe(results["df_summary"], use_container_width=True)
st.caption("â€» page_label ã¯ç‰ˆé¢ãƒ©ãƒ™ãƒ«ï¼ˆä¾‹: 2-1 / åº-1 ãªã©ï¼‰ã€‚å‚ç…§ã¯è¦‹å‡ºã—è¡Œã‚’æœ¬æ–‡ã‹ã‚‰é™¤å¤–ã—ã¦æŠ½å‡ºã€‚")


# =========================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€åº¦ã ã‘ç”Ÿæˆ â†’ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
# =========================
base_stem = st.session_state.figtbl_filename_stem or "result"

# 1) summary.xlsx
xlsx_summary = io.BytesIO()
with pd.ExcelWriter(xlsx_summary, engine="xlsxwriter") as writer:
    sheet = "summary"
    results["df_summary"].to_excel(writer, index=False, sheet_name=sheet)
    wb = writer.book; ws = writer.sheets[sheet]
    text_fmt = wb.add_format({"num_format": "@"})
    header_fmt = wb.add_format({"bold": True})
    wrap_fmt = wb.add_format({"text_wrap": True})
    cols = list(results["df_summary"].columns); col_idx = {n:i for i,n in enumerate(cols)}
    for name in cols:
        width = 36 if name in ["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"] else 22
        fmt = wrap_fmt if name in ["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"] else text_fmt
        ws.set_column(col_idx[name], col_idx[name], width, fmt)
        ws.write(0, col_idx[name], name, header_fmt)
    ws.autofilter(0, 0, len(results["df_summary"]), len(cols)-1)
    ws.freeze_panes(1, 0)
st.session_state.xlsx_summary = xlsx_summary.getvalue()
st.session_state.xlsx_summary_name = f"{base_stem}_summary.xlsx"

# 2) ref_contexts.xlsx
xlsx_refs = io.BytesIO()
with pd.ExcelWriter(xlsx_refs, engine="xlsxwriter") as writer:
    sheet = "ref_contexts"
    df = results["df_ref_details"]
    if df.empty:
        df = pd.DataFrame(columns=["å›³è¡¨ã‚­ãƒ¼","å‚ç…§page_label","å‚ç…§ãƒšãƒ¼ã‚¸(é€£ç•ª)","å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å‚ç…§_1æ–‡","å‚ç…§_å‰å¾ŒÂ±60"])
    df.to_excel(writer, index=False, sheet_name=sheet)
    wb2 = writer.book; ws2 = writer.sheets[sheet]
    text_fmt = wb2.add_format({"num_format": "@"})
    header_fmt = wb2.add_format({"bold": True})
    wrap_fmt = wb2.add_format({"text_wrap": True})
    for j, name in enumerate(df.columns):
        width = 40 if name in ["å‚ç…§_1æ–‡","å‚ç…§_å‰å¾ŒÂ±60"] else 24
        fmt = wrap_fmt if name in ["å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å‚ç…§_1æ–‡","å‚ç…§_å‰å¾ŒÂ±60"] else text_fmt
        ws2.set_column(j, j, width, fmt)
        ws2.write(0, j, name, header_fmt)
    ws2.autofilter(0, 0, len(df), len(df.columns)-1)
    ws2.freeze_panes(1, 0)
st.session_state.xlsx_refs = xlsx_refs.getvalue()
st.session_state.xlsx_refs_name = f"{base_stem}_ref_contexts.xlsx"

# 3) summary.csv / ref_contexts.csv / heads.csv / refs.csv
csv_summary = results["df_summary"].to_csv(index=False).encode("utf-8-sig")
csv_refs    = results["df_ref_details"].to_csv(index=False).encode("utf-8-sig")
csv_heads   = results["df_heads"].to_csv(index=False).encode("utf-8-sig")
csv_refsraw = results["df_refs"].to_csv(index=False).encode("utf-8-sig")

st.session_state.csv_summary = csv_summary
st.session_state.csv_summary_name = f"{base_stem}_summary.csv"
st.session_state.csv_refs = csv_refs
st.session_state.csv_refs_name = f"{base_stem}_ref_contexts.csv"
st.session_state.csv_heads = csv_heads
st.session_state.csv_heads_name = f"{base_stem}_heads.csv"
st.session_state.csv_refsraw = csv_refsraw
st.session_state.csv_refsraw_name = f"{base_stem}_refs_raw.csv"

# 4) æŠ½å‡ºãƒšãƒ¼ã‚¸TXTï¼ˆvalidã®ã¿ï¼‰
if results["valid_segments"]:
    txt_buf = io.StringIO()
    for s in results["valid_segments"]:
        header = f"==== page_label={s['page_label']} (chars={len(s['body'])}) ====\n"
        txt_buf.write(header); txt_buf.write(s["body"].rstrip("\n") + "\n\n")
    st.session_state.txt_valid = txt_buf.getvalue().encode("utf-8")
    st.session_state.txt_valid_name = f"{base_stem}_extracted_pages_valid.txt"


# =========================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é›†ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ç‰©ã‚’ãã®ã¾ã¾é…å¸ƒï¼‰
# =========================
with st.sidebar:
    st.markdown("### ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    if "xlsx_summary" in st.session_state:
        st.download_button(
            "summary ã‚’Excelã§ä¿å­˜ (.xlsx)",
            data=st.session_state.xlsx_summary,
            file_name=st.session_state.xlsx_summary_name,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
    if "xlsx_refs" in st.session_state:
        st.download_button(
            "å‚ç…§ã®æ–‡è„ˆã‚’Excelã§ä¿å­˜ (.xlsx)",
            data=st.session_state.xlsx_refs,
            file_name=st.session_state.xlsx_refs_name,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
    st.download_button(
        "summary ã‚’CSVã§ä¿å­˜",
        data=st.session_state.csv_summary,
        file_name=st.session_state.csv_summary_name,
        mime="text/csv",
        use_container_width=True,
    )
    st.download_button(
        "å‚ç…§ã®æ–‡è„ˆã‚’CSVã§ä¿å­˜",
        data=st.session_state.csv_refs,
        file_name=st.session_state.csv_refs_name,
        mime="text/csv",
        use_container_width=True,
    )
    st.download_button(
        "å›³è¡¨è¦‹å‡ºã— raw ã‚’CSVã§ä¿å­˜",
        data=st.session_state.csv_heads,
        file_name=st.session_state.csv_heads_name,
        mime="text/csv",
        use_container_width=True,
    )
    st.download_button(
        "æœ¬æ–‡å‚ç…§ raw ã‚’CSVã§ä¿å­˜",
        data=st.session_state.csv_refsraw,
        file_name=st.session_state.csv_refsraw_name,
        mime="text/csv",
        use_container_width=True,
    )
    if "txt_valid" in st.session_state:
        st.download_button(
            "æŠ½å‡ºãƒšãƒ¼ã‚¸TXTï¼ˆvalidã®ã¿ï¼‰",
            data=st.session_state.txt_valid,
            file_name=st.session_state.txt_valid_name,
            mime="text/plain",
            use_container_width=True,
        )
