# -*- coding: utf-8 -*-
# pages/43_é æŠ½å‡º.py
# å„PDFãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«èµ°æŸ»ã—ã€ã€Œè¡Œå˜ç‹¬ã®é ãƒ©ãƒ™ãƒ«ã€ã‚’é«˜ã€…1ã¤æŠ½å‡ºã€‚
# æŠ½å‡ºã§ããŸãƒ©ãƒ™ãƒ«åˆ—ã«å¯¾ã—ã¦é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆseq / chap / seriesï¼‰ã‚’å®Ÿæ–½ã€‚

from __future__ import annotations
import re, io, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ é ãƒ©ãƒ™ãƒ«æŠ½å‡ºï¼ˆ1é =é«˜ã€…1ãƒ©ãƒ™ãƒ«ï¼‰+ é€£ç•ªãƒã‚§ãƒƒã‚¯", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ é ãƒ©ãƒ™ãƒ«æŠ½å‡ºï¼ˆ1é =é«˜ã€…1ãƒ©ãƒ™ãƒ«ï¼‰â†’ é€£ç•ªãƒã‚§ãƒƒã‚¯")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
run = st.button("â–¶ æŠ½å‡ºãƒ»é€£ç•ªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰ã‚’è¡¨ç¤º", value=False)

if not uploaded or not run:
    st.stop()

if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts

with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")


# =========================
# æ­£è¦åŒ–ãƒ»ãƒ©ãƒ™ãƒ«åˆ¤å®š
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D\u30FC]"   # å„ç¨®ãƒã‚¤ãƒ•ãƒ³
ALPHAJP = r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]+"
NUM     = r"[0-9ï¼-ï¼™]+"

# ãƒªãƒ¼ãƒ€ãƒ¼ç³»ï¼ˆâ€¦â€¦ã‚„ãƒ»ã‚„ãƒ‰ãƒƒãƒˆã®é€£ãªã‚Šï¼‰
LEADER_CHARS_CLASS = r"[\.ï¼ãƒ»ï½¥â€¦â€§ï½¡]"
LEADERS_SPACED = rf"(?:\s*{LEADER_CHARS_CLASS}\s*){{3,}}"

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰ï¼»ï¼½ï½›ï½", "0123456789()[]{}"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    s = z2h_numhy(s)
    s = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", s)  # è¡Œæœ«ã®ãƒªãƒ¼ãƒ€ãƒ¼åˆ—é™¤å»
    s = re.sub(r"[ \t]+", " ", s)                   # é€£ç¶šç©ºç™½ã‚’1ã¤ã¸
    return s.strip()

# ã€Œè¡Œå˜ç‹¬ã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã€ã‚’æ¤œå‡ºï¼ˆpage/p. ãªã©ä»»æ„ã€seq/chap/serieså¯¾å¿œï¼‰
def build_label_line_regex_mixed() -> re.Pattern:
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"                     # 12
    core_chap   = rf"{NUM}(?:\s*{HY}\s*{NUM})+"          # 2-1, 3-10-2ï¼ˆç©ºç™½OKï¼‰

# åŒºåˆ‡ã‚Š: â‘ ãƒã‚¤ãƒ•ãƒ³/ãƒ‰ãƒƒãƒˆï¼ˆå‰å¾Œã«ä»»æ„ç©ºç™½ï¼‰ â‘¡ã¾ãŸã¯ç©ºç™½ã®ã¿ â‘¢ã©ã¡ã‚‰ã‚‚ç„¡ã—ï¼ˆç›´çµï¼‰
    SEP_OPT = rf"(?:\s*(?:{HY}|[\.ï¼ãƒ»ï½¥])\s*|\s+)?"
    series_word = rf"[ï¼ˆ(ï¼»\[]?{ALPHAJP}[ï¼‰)\]ï¼½]?"
    core_series = rf"{series_word}{SEP_OPT}{NUM}"  # ä¾‹: è³‡æ–™1 / è³‡æ–™ 1 / è³‡æ–™-1 / è³‡æ–™ï¼1 / åºãƒ¼1 ãªã©

    core = rf"(?:{core_seq}|{core_chap}|{core_series})"

    # è¡Œâ€œå˜ç‹¬â€ã§ä¸€è‡´ï¼ˆå‰å¾Œç©ºç™½ã®ã¿OKï¼‰
    return re.compile(
        rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$",
        re.MULTILINE
    )

LABEL_LINE_RE = build_label_line_regex_mixed()


# =========================
# 1ãƒšãƒ¼ã‚¸=é«˜ã€…1ãƒ©ãƒ™ãƒ«æŠ½å‡º
# =========================
def extract_single_page_label(page_text: str) -> Tuple[Optional[str], Optional[str]]:
    """
    1ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€è¡Œå˜ç‹¬ã®ãƒ©ãƒ™ãƒ«ã‚’é«˜ã€…1ã¤æŠ½å‡ºã€‚
    æˆ»ã‚Šå€¤: (label or None, matched_line or None)
    """
    if not page_text:
        return None, None

    # è¡Œã”ã¨ã«ãƒã‚§ãƒƒã‚¯ï¼ˆnormalize_strictã®å‰ã«æŠ˜è¿”ã—ä¿æŒï¼‰
    lines = page_text.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    for raw in lines:
        s = normalize_strict(raw)
        if not s:
            continue
        m = LABEL_LINE_RE.match(s)
        if m:
            label = z2h_numhy(m.group("label"))
            return label, raw  # ç”Ÿã®è¡Œã‚‚è¿”ã—ã¦ãŠãï¼ˆç¢ºèªç”¨ï¼‰
    return None, None


# =========================
# é€£ç•ªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
# =========================
def _parse_label_kind(label: str) -> Tuple[str, Any]:
    # 1) æ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ã€å„ç¨®ãƒã‚¤ãƒ•ãƒ³â†’'-'ï¼‰
    lab = z2h_numhy(label)

    # ç« -ãƒšãƒ¼ã‚¸ç”¨ã«ãƒã‚¤ãƒ•ãƒ³å‘¨ã‚Šã®ç©ºç™½ã‚’è©°ã‚ã‚‹
    lab_chap = re.sub(r"\s*-\s*", "-", lab)

    # 2) é€£ç•ªï¼ˆç´”æ•°å­—ï¼‰
    if re.fullmatch(r"[0-9]+", lab_chap):
        return "seq", int(lab_chap)

    # 3) ç« -ãƒšãƒ¼ã‚¸ï¼ˆ2-1 / 3-10-2 ç­‰ï¼‰
    if re.fullmatch(r"[0-9]+(?:-[0-9]+)+", lab_chap):
        return "chap", [int(p) for p in lab_chap.split("-")]

    # 4) ã‚·ãƒªãƒ¼ã‚º-æ•°å­—ï¼ˆåŒºåˆ‡ã‚Šãªã—/ç©ºç™½/ãƒ‰ãƒƒãƒˆ/å„ç¨®ãƒã‚¤ãƒ•ãƒ³/æ‹¬å¼§ä»˜ãã‚·ãƒªãƒ¼ã‚ºèªã‚‚è¨±å®¹ï¼‰
    #   ä¾‹: è³‡æ–™1, è³‡æ–™ 1, è³‡æ–™-1, è³‡æ–™ï¼1, åºãƒ¼1, (è³‡æ–™) 1, ï¼»åºï¼½-12
    series_pat = re.compile(
        rf"""^
        [ï¼ˆ(ï¼»\[]?          # é–‹ãã‚«ãƒƒã‚³ï¼ˆä»»æ„ï¼‰
        ({ALPHAJP})         # ã‚·ãƒªãƒ¼ã‚ºèª
        [ï¼‰)\]ï¼½]?          # é–‰ã˜ã‚«ãƒƒã‚³ï¼ˆä»»æ„ï¼‰
        (?:\s*(?:{HY}|[\.ï¼ãƒ»ï½¥])\s*|\s+)?  # åŒºåˆ‡ã‚Šï¼šãƒã‚¤ãƒ•ãƒ³/ãƒ‰ãƒƒãƒˆ/ç©ºç™½ ã¾ãŸã¯ç„¡ã—
        ([0-9]+)            # æ•°å­—
        $""", re.X
    )
    m = series_pat.fullmatch(lab)
    if m:
        series_word = m.group(1)
        num = int(m.group(2))
        return "series", (series_word, num)

    return "unknown", None


def valid_and_reason_auto(label: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    k, cur = _parse_label_kind(label)
    if k == "unknown":
        return False, "ä¸æ˜ãªãƒ©ãƒ™ãƒ«å½¢å¼"
    if prev_ok is None:
        return True, ""
    pk, prev = _parse_label_kind(prev_ok)
    if pk == "unknown":
        return True, ""
    if k != pk:
        return True, "å½¢å¼åˆ‡æ›¿"
    if k == "seq":
        return (cur == prev + 1, "" if cur == prev + 1 else "éé€£ç•ª")
    if k == "chap":
        c, p = (cur + [1, 1])[:2]; pc, pp = (prev + [1, 1])[:2]
        ok = (c == pc and p == pp + 1) or (c == pc + 1 and p == 1)
        return (ok, "" if ok else "éé€£ç•ª")
    if k == "series":
        s, n = cur; ps, pn = prev
        if s != ps:
            return True, "å½¢å¼åˆ‡æ›¿"
        return (n == pn + 1, "" if n == pn + 1 else "éé€£ç•ª")
    return True, ""


# =========================
# å®Ÿè¡Œï¼šå„ãƒšãƒ¼ã‚¸ã‹ã‚‰1ãƒ©ãƒ™ãƒ«æŠ½å‡º â†’ è¦‹ã¤ã‹ã£ãŸãƒ©ãƒ™ãƒ«åˆ—ã§é€£ç•ªãƒã‚§ãƒƒã‚¯
# =========================
rows_page: List[Dict[str, Any]] = []
for i, ptxt in enumerate(pages_text, start=1):
    label, matched = extract_single_page_label(ptxt)
    rows_page.append({
        "pdf_page": i,
        "detected_label": label if label is not None else "-",
        "matched_line": matched if matched is not None else "-",
        "has_label": label is not None,
    })

df_per_page = pd.DataFrame(rows_page)
st.subheader("ğŸ” å„ãƒšãƒ¼ã‚¸ã®ãƒ©ãƒ™ãƒ«æŠ½å‡ºçµæœï¼ˆ1é =é«˜ã€…1ãƒ©ãƒ™ãƒ«ï¼‰")
st.dataframe(df_per_page, use_container_width=True)

# è¦‹ã¤ã‹ã£ãŸãƒ©ãƒ™ãƒ«ã ã‘ã§é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆPDFä¸Šã®å‡ºç¾é †ï¼‰
found_labels = [r["detected_label"] for r in rows_page if r["detected_label"] != "-"]

rows_seq: List[Dict[str, Any]] = []
prev_ok: Optional[str] = None
for idx, lab in enumerate(found_labels, start=1):
    ok, reason = valid_and_reason_auto(lab, prev_ok)
    if ok:
        prev_ok = lab
    rows_seq.append({
        "order_in_found": idx,
        "label": lab,
        "valid": ok,
        "reason": "" if ok else reason
    })
df_seq = pd.DataFrame(rows_seq)

st.subheader("âœ… è¦‹ã¤ã‹ã£ãŸãƒ©ãƒ™ãƒ«åˆ—ã®é€£ç•ªãƒã‚§ãƒƒã‚¯")
if df_seq.empty:
    st.info("æŠ½å‡ºã§ããŸãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç‰ˆé¢ã®å‡ºåŠ›å½¢å¼ï¼ˆè¡Œå˜ç‹¬ã®ãƒ©ãƒ™ãƒ«ï¼‰ã‹OCRå“è³ªã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
else:
    st.dataframe(df_seq, use_container_width=True)

# ä¾¿åˆ©: CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
with st.sidebar:
    st.markdown("### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    buf1, buf2 = io.StringIO(), io.StringIO()
    if not df_per_page.empty:
        df_per_page.to_csv(buf1, index=False)
        st.download_button("ğŸ“¥ per_page_labels.csv", data=buf1.getvalue().encode("utf-8-sig"),
                           file_name="per_page_labels.csv", mime="text/csv", use_container_width=True)
    if not df_seq.empty:
        df_seq.to_csv(buf2, index=False)
        st.download_button("ğŸ“¥ label_sequence_check.csv", data=buf2.getvalue().encode("utf-8-sig"),
                           file_name="label_sequence_check.csv", mime="text/csv", use_container_width=True)

# ãƒ‡ãƒãƒƒã‚°
if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.code(f"LABEL_LINE_RE = {LABEL_LINE_RE.pattern}")


# =========================
# è¿½è¨˜ï¼šå˜ç‹¬è¡Œã® å›³/è¡¨/å›³è¡¨ è¦‹å‡ºã—æŠ½å‡ºï¼ˆ1é ã‚ãŸã‚Šè¤‡æ•°å¯ï¼‰
# =========================

# åŒºåˆ‡ã‚Šè¨˜å·ï¼ˆãƒ‰ãƒƒãƒˆé¡ï¼‰
DOT = r"[\.ï¼ãƒ»ï½¥]"
# æ•°å­—ï¼ˆå…¨è§’å«ã‚€ï¼‰
NUM = r"[0-9ï¼-ï¼™]+"

# å›³è¡¨ç•ªå·ãƒˆãƒ¼ã‚¯ãƒ³ï¼š
#   ä¾‹) 2.2-1 / 2. 2-1 / ï¼’ï¼ï¼‘ï¼ï¼‘ / ï¼ˆï¼‘ï¼‰
NUM_TOKEN = rf"""
(?:                                     
    {NUM}                                # å…ˆé ­ã®æ•°
    (?:\s*{DOT}\s*{NUM})*               # 2.2 / 2ï¼ 2 ãªã©
    (?:\s*{HY}\s*{NUM})*                # -1 / ï¼ 1 ãªã©
  |
    [ï¼ˆ(]\s*{NUM}\s*[ï¼‰)]               # ï¼ˆï¼‘ï¼‰ / (12)
)
"""
NUM_TOKEN_RE = re.compile(NUM_TOKEN, re.X)

# è¦‹å‡ºã—ï¼ˆè¡Œâ€œå˜ç‹¬â€ï¼‰ï¼šè¡Œé ­ã« å›³|è¡¨|å›³è¡¨ + ç•ªå· (+ ä»»æ„ã®ã‚¿ã‚¤ãƒˆãƒ«)
HEADING_RE = re.compile(
    rf"^\s*(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})\s*[:ï¼š.\-ï¼ã€]?\s*(?P<title>.+?)?\s*$",
    re.X
)

def canon_num_for_caption(num: str) -> str:
    """ç•ªå·ã®æ­£è¦åŒ–ï¼šå…¨è§’â†’åŠè§’ã€ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€ã€ãƒ‰ãƒƒãƒˆçµ±ä¸€ã€ç©ºç™½é™¤å»ã€æ‹¬å¼§é™¤å»"""
    s = z2h_numhy(num)                 # å…¨è§’â†’åŠè§’ã€å„ç¨®ãƒã‚¤ãƒ•ãƒ³â†’'-'
    s = re.sub(r"[()ï¼ˆï¼‰]", "", s)      # ï¼ˆ1ï¼‰â†’1
    s = re.sub(DOT, ".", s)            # ï¼ãƒ» â†’ .
    s = re.sub(r"\s*\.\s*", ".", s)    # '2 . 2' â†’ '2.2'
    s = re.sub(r"\s*-\s*", "-", s)     # '2 - 1' â†’ '2-1'
    s = re.sub(r"\s+", "", s)          # æ®‹ã‚Šã®ç©ºç™½é™¤å»
    return s

def canon_caption_label(kind: str, num: str) -> str:
    return f"{kind}{canon_num_for_caption(num)}"

def extract_headings_from_page(page_text: str) -> List[Tuple[str, str, str]]:
    """
    1ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€è¡Œå˜ç‹¬ã® å›³/è¡¨/å›³è¡¨ è¦‹å‡ºã—ã‚’ã™ã¹ã¦æŠ½å‡ºã€‚
    æˆ»ã‚Šå€¤: List[(å›³è¡¨ã‚­ãƒ¼, å›³è¡¨ç•ªå·(è¡¨ç¤ºç”¨), è¡Œã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ)]
    """
    out: List[Tuple[str, str, str]] = []
    if not page_text:
        return out
    lines = page_text.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    for raw in lines:
        s = normalize_strict(raw)
        if not s:
            continue
        m = HEADING_RE.match(s)
        if not m:
            continue
        kind  = m.group("kind")
        num   = m.group("num")
        title = (m.group("title") or "").strip()
        key   = canon_caption_label(kind, num)          # ä¾‹: å›³2.2-1 / è¡¨1 / å›³è¡¨1
        disp  = f"{kind}{z2h_numhy(num)}"               # è¡¨ç¤ºç”¨ï¼šå…¨è§’â†’åŠè§’ãƒ»ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€ã®ã¿
        # ï¼ˆå¿…è¦ãªã‚‰ title ã‚‚ä¿æŒã§ãã¾ã™ã€‚ä»Šå›ã¯ raw è¡Œã‚’æ®‹ã—ã¾ã™ï¼‰
        out.append((key, disp, raw))
    return out

# å…¨ãƒšãƒ¼ã‚¸ã‚’èµ°æŸ»
caption_rows: List[Dict[str, Any]] = []
for i, ptxt in enumerate(pages_text, start=1):

    page_label, _ = extract_single_page_label(ptxt)
    hits = extract_headings_from_page(ptxt)
    for key, disp, raw in hits:
        caption_rows.append({
            "pdf_page": i,
            "page_label": page_label if page_label else "-",  # â† ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
            "å›³è¡¨ã‚­ãƒ¼": key,          # ç…§åˆç”¨ã«æ­£è¦åŒ–ã—ãŸã‚­ãƒ¼ï¼ˆä¾‹: å›³2.2-1 / è¡¨1 / å›³è¡¨1ï¼‰
            "å›³è¡¨ç•ªå·": disp,         # è¡¨ç¤ºç”¨ç•ªå·ï¼ˆå…¨è§’â†’åŠè§’ãƒ»ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€ï¼‰
            "matched_line": raw,      # è¡Œã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
        })

df_captions = pd.DataFrame(caption_rows)

st.subheader("ğŸ–¼ï¸ å˜ç‹¬è¡Œã®å›³/è¡¨/å›³è¡¨ è¦‹å‡ºã—ï¼ˆãƒšãƒ¼ã‚¸åˆ¥ï¼‰")
if df_captions.empty:
    st.info("å˜ç‹¬è¡Œã®å›³ãƒ»è¡¨ãƒ»å›³è¡¨ã®è¦‹å‡ºã—ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    st.dataframe(df_captions, use_container_width=True)

# è¿½åŠ ã®CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã«é…ç½®ï¼‰
with st.sidebar:
    if not df_captions.empty:
        buf3 = io.StringIO()
        df_captions.to_csv(buf3, index=False)
        st.download_button(
            "ğŸ“¥ page_headings.csvï¼ˆå›³/è¡¨/å›³è¡¨ï¼‰",
            data=buf3.getvalue().encode("utf-8-sig"),
            file_name="page_headings.csv",
            mime="text/csv",
            use_container_width=True,
        )

# ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆä»»æ„ï¼‰
if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug (captions)")
    st.code(f"HEADING_RE = {HEADING_RE.pattern}")

