# -*- coding: utf-8 -*-
# pages/26_ç›®æ¬¡ãƒã‚§ãƒƒã‚¯.py â€” GPT API ä¸ä½¿ç”¨ç‰ˆï¼šæœ¬æ–‡ç…§åˆã§ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ©ãƒ™ãƒ«é€£ç•ªæ¤œè¨¼ã¤ããƒ»æœ‰åŠ¹ãƒšãƒ¼ã‚¸ã®ã¿TXTå‡ºåŠ›ï¼‰
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‹é€£ç•ªæ¤œè¨¼ï¼‰", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‰")
st.caption("GPT ã‚’ä½¿ã‚ãšã€ç›®æ¬¡å€™è£œã®ã‚¿ã‚¤ãƒˆãƒ«ãŒæœ¬æ–‡ã«å‡ºç¾ã™ã‚‹ã‹ã‚’ç›´æ¥ç…§åˆã—ã¾ã™ã€‚ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã¯é€£ç•ªã®ã¿æ¤œè¨¼ï¼ˆâ€œå¹´ã£ã½ã„é™¤å¤–â€ã¯è¡Œã„ã¾ã›ã‚“ï¼‰ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2 = st.columns([1.3, 1])
with c1:
    scheme = st.radio("ãƒšãƒ¼ã‚¸æ–¹å¼", ["(1) 1,2,3,4, â€¦", "(2) 1-1,1-2,2-1,2-2, â€¦"], index=1, horizontal=True)
with c2:
    join_pages = st.checkbox("ç›®æ¬¡æŠ½å‡ºç”¨ã«å†’é ­æ•°ãƒšãƒ¼ã‚¸ã‚’é€£çµï¼ˆæ¨å¥¨ï¼‰", value=True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæœ¬æ–‡ã®ä¸Šé™æ–‡å­—æ•°ï¼ˆ0ã§å…¨æ–‡ï¼‰
with st.sidebar:
    excerpt_chars = st.number_input(
        "ç…§åˆå¯¾è±¡ã®æœ¬æ–‡ä¸Šé™æ–‡å­—æ•°ï¼ˆ0ã§å…¨æ–‡ï¼‰",
        min_value=0, max_value=50000, value=800, step=100,
        help="å„ãƒšãƒ¼ã‚¸æœ¬æ–‡ã®å…ˆé ­ã‹ã‚‰ä½•æ–‡å­—ã¾ã§ã‚’ç…§åˆå¯¾è±¡ã«ã™ã‚‹ã‹ã€‚0 ãªã‚‰å…¨æ–‡ã€‚"
    )
    show_debug = st.checkbox("ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º", value=False)

run = st.button("â–¶ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯é–‹å§‹", type="primary", use_container_width=True)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()

# =========================
# PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸å˜ä½ï¼‰
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts

with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text = pdf_to_text_per_page(pdf_path)

st.success(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†ï¼ˆãƒšãƒ¼ã‚¸æ•°: {len(pages_text)}ï¼‰")

# =========================
# ç›®æ¬¡å€™è£œã®æŠ½å‡ºï¼ˆè¡Œæœ«ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã®ã‚ã‚‹è¡Œï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D]"  # å„ç¨®ãƒã‚¤ãƒ•ãƒ³
LEADERS = r"[\.ï¼ãƒ»â€¦]+"                                   # ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ç¾¤

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
    return re.sub(HY, "-", s)

def build_label_tail_regex(scheme: str) -> re.Pattern:
    if scheme.startswith("(1)"):
        tail = r"(?P<label>[0-9ï¼-ï¼™]{1,6})"
    else:
        tail = rf"(?P<label>[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+)"
    pat = rf"""
        ^(?P<head>.*?)                               # å·¦å´æœ¬æ–‡
        (?:\s*{LEADERS}\s*|\s{{2,}})?                # ãƒ‰ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ or è¤‡æ•°ç©ºç™½
        {tail}\s*$                                   # è¡Œæœ«ã«ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«
    """
    return re.compile(pat, re.X)

LABEL_TAIL_RE = build_label_tail_regex(scheme)

def extract_toc_lines(fulltext: str, limit: int = 300) -> List[str]:
    """
    ç›®æ¬¡å€™è£œè¡Œã‚’æŠ½å‡ºã€‚
    è¡Œé ­ãŒã€Œç¬¬ã€ã¾ãŸã¯æ•°å­—ã§å§‹ã¾ã‚Šã€æ–‡å­—ï¼ˆæ—¥æœ¬èª/è‹±å­—ï¼‰ã‚’å«ã‚€è¡Œã®ã¿æ¡ç”¨ã€‚
    """
    lines = [l.rstrip() for l in fulltext.replace("\r\n","\n").replace("\r","\n").split("\n")]
    head_ok = re.compile(r"^(ç¬¬|[0-9ï¼-ï¼™])")
    text_char = re.compile(r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]")
    out: List[str] = []

    for ln in lines:
        s = ln.strip()
        if not s:
            continue
        if not head_ok.match(s):
            continue
        if not text_char.search(s):
            continue
        m = LABEL_TAIL_RE.match(s)
        if not m:
            continue
        head = re.sub(rf"\s*{LEADERS}\s*$", "", m.group("head")).strip()
        label = z2h_numhy(m.group("label"))
        if len(head) <= 1:
            continue
        out.append(f"{head} ::: {label}")
        if len(out) >= limit:
            break
    return out

# ç›®æ¬¡å€™è£œã¯å†’é ­ï½æ•°ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºï¼ˆãŸã„ã¦ã„ç›®æ¬¡ã¯å‰åŠï¼‰
sample_pages_n = min(10, len(pages_text))
sample_text = "\n".join(pages_text[:sample_pages_n]) if join_pages else "\n".join(pages_text[:max(1, sample_pages_n//2)])
toc_lines = extract_toc_lines(sample_text)

st.subheader("æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡å€™è£œï¼ˆä¸Šä½ï¼‰")
if len(toc_lines) == 0:
    st.warning("ç›®æ¬¡å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()
st.code("\n".join(toc_lines[:80]))

# =========================
# ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã®å˜ç‹¬è¡ŒæŠ½å‡º â†’ æœ¬æ–‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–
# =========================
def build_label_line_regex(scheme: str) -> re.Pattern:
    if scheme.startswith("(1)"):
        core = r"[0-9ï¼-ï¼™]{1,6}"               # é€£ç•ª
    else:
        core = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"  # ç« -ãƒšãƒ¼ã‚¸ï¼ˆè¤‡åˆå¯ï¼‰
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

LABEL_LINE_RE = build_label_line_regex(scheme)

def split_segments_by_label(all_text: str, scheme: str) -> List[Dict[str, Any]]:
    """
    PDF å…¨æ–‡ï¼ˆãƒšãƒ¼ã‚¸é€£çµï¼‰ã‹ã‚‰ã€å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§æœ¬æ–‡ã‚’åˆ†å‰²ã€‚
    """
    txt = z2h_numhy(all_text.replace("\r\n", "\n").replace("\r", "\n"))
    matches = list(LABEL_LINE_RE.finditer(txt))
    if not matches:
        return []

    segs: List[Dict[str, Any]] = []

    def next_nonempty_pos(pos: int) -> int:
        n = pos
        while n < len(txt) and txt[n] == "\n":
            n += 1
        return n

    for i, m in enumerate(matches):
        label = z2h_numhy(m.group("label"))
        start = next_nonempty_pos(m.end())
        end = matches[i+1].start() if i+1 < len(matches) else len(txt)
        body = txt[start:end].lstrip("\n ")
        segs.append({
            "page_label": label,
            "body": body,
        })
    return segs

# å…¨æ–‡ã‚’é€£çµã—ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ï¼ˆæ¤œå‡ºã—ã‚„ã™ã•é‡è¦–ï¼‰
all_text_joined = "\n".join(pages_text)
segments = split_segments_by_label(all_text_joined, scheme)

st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆå˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ã§åŒºåˆ‡ã‚Šï¼‰â€” æ¦‚è¦³")
if len(segments) == 0:
    st.warning("å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãšã€æœ¬æ–‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½œã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    df_segments_overview = pd.DataFrame([{
        "page_label": s["page_label"],
        "char_count": len(s["body"]),
        "preview": s["body"][:120].replace("\n"," ") + ("â€¦" if len(s["body"])>120 else "")
    } for s in segments])
    st.dataframe(df_segments_overview, use_container_width=True)

# =========================
# ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆç…§åˆå‰ã«è¡¨ç¤ºï¼‰
# =========================
st.subheader("ğŸ“‘ ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«æ¤œè¨¼ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ï¼‰")

def valid_and_reason(label: str, scheme: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    """
    ãƒ©ãƒ™ãƒ«å¦¥å½“æ€§ï¼š
    - æ–¹å¼(1)ï¼šé€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆn == prev+1 ãŒOKï¼‰
    - æ–¹å¼(2)ï¼šç« -ãƒšãƒ¼ã‚¸å½¢å¼ã®é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆ(c,p) â†’ (c, p+1) or (c+1, 1) ãŒOKï¼‰
    """
    if scheme.startswith("(1)"):
        try:
            n = int(label)
        except Exception:
            return False, "é€£ç•ªå½¢å¼ã§æ•°å€¤åŒ–ã§ããªã„"
        if prev_ok is None:
            return True, ""
        try:
            prev = int(prev_ok)
        except Exception:
            return True, ""
        return (True, "") if n == prev + 1 else (False, "éé€£ç•ª")
    else:
        parts = label.split("-")
        if not (len(parts) >= 2 and all(p.isdigit() for p in parts)):
            return False, "ç« -ãƒšãƒ¼ã‚¸å½¢å¼ã§ãªã„"
        chap, page_n = int(parts[0]), int(parts[1])
        if prev_ok is None:
            return True, ""
        pparts = prev_ok.split("-")
        if not (len(pparts) >= 2 and all(p.isdigit() for p in pparts)):
            return True, ""
        pchap, ppage = int(pparts[0]), int(pparts[1])
        if (chap == pchap and page_n == ppage + 1) or (chap == pchap + 1 and page_n == 1):
            return True, ""
        return False, "éé€£ç•ª"

rows_check = []
prev_ok = None
for s in segments:
    ok, reason = valid_and_reason(s["page_label"], scheme, prev_ok)
    if ok:
        prev_ok = s["page_label"]
    rows_check.append({
        "page_label": s["page_label"],
        "valid": ok,
        "reason": "" if ok else reason,
        "char_count": len(s["body"]),
        "preview": s["body"][:100].replace("\n", " ") + ("â€¦" if len(s["body"]) > 100 else ""),
    })
df_check = pd.DataFrame(rows_check)
st.dataframe(df_check, use_container_width=True)
warn_df = df_check[~df_check["valid"]]
if len(warn_df):
    st.warning(f"âš ï¸ éé€£ç•ªãªã©ã®ä¸æ­£ãƒ©ãƒ™ãƒ«: {len(warn_df)} ä»¶")

# ==== æœ‰åŠ¹ï¼ˆvalid=Trueï¼‰ã®ãƒšãƒ¼ã‚¸ã ã‘ã‚’TXTã§ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ====
st.subheader("ğŸ“„ æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=True ã®ã¿ï¼‰ã‚’TXTã§ä¿å­˜")
if len(segments):
    # valid=True ã® label ã‚»ãƒƒãƒˆ
    valid_labels = {row["page_label"] for row in rows_check if row["valid"]}
    valid_segments = [s for s in segments if s["page_label"] in valid_labels]

    if valid_segments:
        sep = "ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼"
        buf_txt = io.StringIO()

        for s in valid_segments:
            page_label = str(s.get("page_label", ""))
            body = str(s.get("body", ""))
            char_count = len(body)

            buf_txt.write(f"{sep}\n")
            buf_txt.write(f"page_label: {page_label}    char_count: {char_count}\n")
            buf_txt.write(f"{sep}\n")
            buf_txt.write(body.rstrip() + "\n\n")

        st.download_button(
            "ğŸ“¥ æŠ½å‡ºãƒšãƒ¼ã‚¸TXTï¼ˆvalid=True ã®ã¿ï¼‰ã‚’ä¿å­˜",
            data=buf_txt.getvalue().encode("utf-8"),
            file_name="extracted_pages_valid_only.txt",
            mime="text/plain",
            use_container_width=True,
        )
    else:
        st.info("valid=True ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# =========================
# ç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç…§åˆï¼ˆé †é€ã‚Šï¼‰
# =========================
def slice_body(text: str) -> str:
    if excerpt_chars and excerpt_chars > 0:
        return text[:excerpt_chars]
    return text

# seg_index: page_label â†’ segment body
seg_index = {s["page_label"]: s["body"] for s in segments}

def find_partial(title: str, body: str) -> Tuple[bool, str]:
    """
    éƒ¨åˆ†ä¸€è‡´ï¼šã‚¿ã‚¤ãƒˆãƒ«ã®å…ˆé ­ 5 / 4 / 3 æ–‡å­—ã®ã„ãšã‚Œã‹ãŒæœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ã‹ã€‚
    è¦‹ã¤ã‹ã£ãŸã‚‰æŠœç²‹ã‚’è¿”ã™ã€‚
    """
    t = title.strip()
    for klen in (5, 4, 3):
        if len(t) >= klen:
            key = t[:klen]
            pos = body.find(key)
            if pos >= 0:
                snippet = body[max(0, pos-20):pos+klen+20].replace("\n", " ")
                return True, snippet
    return False, ""

def check_toc_by_order(toc_lines: List[str], seg_index: Dict[str,str], pages_text: List[str]) -> pd.DataFrame:
    """
    1) ç›®æ¬¡ãƒ©ãƒ™ãƒ«ã«åˆè‡´ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã§æ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
    2) è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸ã‚’é ­ã‹ã‚‰æ¤œç´¢
    """
    rows = []
    for toc in toc_lines:
        if " ::: " not in toc:
            continue
        title, label = toc.split(" ::: ", 1)
        title = title.strip()
        label = label.strip()

        found_page = "-"
        status = "æœªæ¤œå‡º"
        matched_line = "-"

        # 1) ãƒ©ãƒ™ãƒ«ä¸€è‡´ãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’å„ªå…ˆï¼ˆvalid ã‹ã©ã†ã‹ã¯å•ã‚ãšæœ¬æ–‡ã‚’å‚ç…§ï¼‰
        body = seg_index.get(label, "")
        if body:
            body_s = slice_body(body)
            lines = [ln.rstrip() for ln in body_s.split("\n") if ln.strip()]

            # å®Œå…¨ä¸€è‡´ï¼ˆè¡Œå…¨ä½“ï¼‰
            hit = next((ln for ln in lines if ln.strip() == title), None)
            if hit:
                status = "ä¸€è‡´"
                matched_line = hit
            else:
                # è¡Œå†…ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã‚€
                hit = next((ln for ln in lines if title in ln), None)
                if hit:
                    status = "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰"
                    matched_line = hit
                else:
                    ok, snippet = find_partial(title, body_s)
                    if ok:
                        status = "éƒ¨åˆ†ä¸€è‡´"
                        matched_line = snippet

        # 2) ã¾ã æœªæ¤œå‡ºãªã‚‰ã€å…¨ãƒšãƒ¼ã‚¸ã‚’é †ã«èµ°æŸ»ï¼ˆè¡Œå˜ä½ï¼‰
        if status.startswith("æœªæ¤œå‡º"):
            for i, ptxt in enumerate(pages_text):
                body_s = slice_body(ptxt)
                lines = [ln.rstrip() for ln in body_s.split("\n") if ln.strip()]

                hit = next((ln for ln in lines if ln.strip() == title), None)
                if hit:
                    status = "ä¸€è‡´"
                    matched_line = hit
                    found_page = i + 1
                    break

                hit = next((ln for ln in lines if title in ln), None)
                if hit:
                    status = "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰"
                    matched_line = hit
                    found_page = i + 1
                    break

                ok, snippet = find_partial(title, body_s)
                if ok:
                    status = "éƒ¨åˆ†ä¸€è‡´"
                    matched_line = snippet
                    found_page = i + 1
                    break

        rows.append({
            "ã‚¿ã‚¤ãƒˆãƒ«": title,
            "ç›®æ¬¡ãƒ©ãƒ™ãƒ«": label,
            "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸": found_page,
            "åˆ¤å®š": status,
            "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ": matched_line,
        })

    return pd.DataFrame(rows)

df_result = check_toc_by_order(toc_lines, seg_index, pages_text)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆè¡Œãƒ™ãƒ¼ã‚¹ï¼‰")
st.dataframe(df_result, use_container_width=True)
st.caption("â€» ç›®æ¬¡ã®å„è¡Œï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’é †ã«è¦‹ã¦ã€ã¾ãšåŒãƒ©ãƒ™ãƒ«ã®æœ¬æ–‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§ç…§åˆã—ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ãƒšãƒ¼ã‚¸ã‚’èµ°æŸ»ã€‚å®Œå…¨ä¸€è‡´ï¼éƒ¨åˆ†ä¸€è‡´ã®è¡Œã‚’æŠœç²‹è¡¨ç¤ºã—ã¾ã™ã€‚")

# çµ±è¨ˆ
summary = df_result["åˆ¤å®š"].value_counts().to_dict()
st.markdown(f"**çµæœæ¦‚è¦**: {summary}")

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSVï¼‰
buf = io.StringIO()
df_result.to_csv(buf, index=False)
st.download_button(
    "ğŸ“¥ ç…§åˆçµæœã‚’CSVã§ä¿å­˜",
    data=buf.getvalue().encode("utf-8-sig"),
    file_name="toc_check_local_result.csv",
    mime="text/csv",
    use_container_width=True,
)
