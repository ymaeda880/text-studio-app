# -*- coding: utf-8 -*-
# pages/24_å›³è¡¨ã‚¯ãƒ­ã‚¹ãƒã‚§ãƒƒã‚¯.py
# å›³/è¡¨/å›³è¡¨ã®è¦‹å‡ºã— â†” æœ¬æ–‡å‚ç…§ã‚’ã€PDFå†…ã®ã€Œãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã€ï¼ˆ1-2, 3-10, åº-1, è³‡-2 ç­‰ï¼‰å˜ä½ã§ã‚¯ãƒ­ã‚¹ãƒã‚§ãƒƒã‚¯
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ–¼ï¸ å›³è¡¨ã‚¯ãƒ­ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«å¯¾å¿œï¼‰", page_icon="ğŸ–¼ï¸", layout="wide")
st.title("ğŸ–¼ï¸ å›³è¡¨ã‚¯ãƒ­ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆè¦‹å‡ºã— â†” æœ¬æ–‡å‚ç…§ï¼ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«å¯¾å¿œï¼‰")
st.caption("PDFã‹ã‚‰ã€ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã€ï¼ˆ1-2 / åº-1 ç­‰ï¼‰ã‚’æŠ½å‡ºã—ã¦æœ¬æ–‡ã‚’åˆ†å‰²ã—ã€å›³è¡¨è¦‹å‡ºã—ã¨å‚ç…§ã®çªåˆã‚’è¡Œã„ã¾ã™ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2, c3 = st.columns([1.2, 1.1, 1.2])
with c1:
    join_front = st.checkbox("å…ˆé ­10ãƒšãƒ¼ã‚¸ã‚’çµåˆã—ã¦ãƒ˜ãƒƒãƒ€ä¾‹ã‚’è¡¨ç¤º", value=True)
with c2:
    min_caption = st.number_input("è¦‹å‡ºã—ã®æœ€çŸ­æ–‡å­—æ•°ï¼ˆè¦‹å‡ºã—æŠ½å‡ºã®ä¸‹é™ï¼‰", 0, 200, 4, 1)
with c3:
    run = st.button("â–¶ è§£æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    strict_kind_match = st.checkbox(
        "ã€å›³è¡¨nã€è¦‹å‡ºã—ã¯å›³/è¡¨å‚ç…§ã¨åˆ¥æ‰±ã„ï¼ˆå³æ ¼ï¼‰",
        value=True,
        help="OFFã«ã™ã‚‹ã¨ã€å›³è¡¨1ã€è¦‹å‡ºã—ã«å¯¾ã—ã¦ã€å›³1ã€ã€è¡¨1ã€å‚ç…§ã‚‚ä¸€è‡´æ‰±ã„ï¼ˆå¯›å®¹ï¼‰ã€‚"
    )
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰ã‚’è¡¨ç¤º", value=False)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts


with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")


# =========================
# æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆç›®æ¬¡ãƒã‚§ãƒƒã‚¯æº–æ‹ ï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0Dãƒ¼â€“â€”âˆ’]"  # ãƒã‚¤ãƒ•ãƒ³é¡
LEADER_CHARS_CLASS = r"[\.ï¼ãƒ»ï½¥â€¦â€§ï½¡]"                               # ãƒªãƒ¼ãƒ€å€™è£œ
LEADERS_SPACED = rf"(?:\s*{LEADER_CHARS_CLASS}\s*){{3,}}"          # ç©ºç™½ã‚’æŒŸã‚€ãƒªãƒ¼ãƒ€ãƒ¼åˆ—
ALPHAJP = r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]+"                   # è‹±å­—/ã‹ãª/æ¼¢å­—

def z2h_numhy(s: str) -> str:
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰ï¼»ï¼½ï½›ï½", "0123456789()[]{}"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    s = z2h_numhy(s)
    s = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", s)   # è¡Œæœ«ãƒªãƒ¼ãƒ€ãƒ¼åˆ—é™¤å»
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def norm_label_core(s: str) -> str:
    """(1)/(ï¼‘ï¼’)ç­‰â†’æ•°å­—ã€å…¨è§’â†’åŠè§’ã€ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€"""
    s = z2h_numhy(s)
    s = re.sub(r"^[\(\[]\s*([0-9]+)\s*[\)\]]$", r"\1", s)  # (1)/[1] â†’ 1
    return s.strip().strip("ï¼.ï¼š:ã€ï¼Œ")


# =========================
# ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ï¼ˆè¡Œå˜ç‹¬ï¼‰ã‚’æ¤œå‡ºã—ã¦æœ¬æ–‡ã‚’åˆ†å‰²
#   è¨±å®¹ãƒ©ãƒ™ãƒ«:
#     1) é€£ç•ª           12
#     2) ç« -ãƒšãƒ¼ã‚¸      3-2, 10-4-2
#     3) ã‚·ãƒªãƒ¼ã‚º-æ•°å­—  åº-1, è³‡-2, ä»˜-3, A-10 ãªã©ï¼ˆå…ˆé ­ã«å’Œæ–‡/è‹±å­—ã®å˜èªï¼‰
# =========================
def build_label_line_regex_mixed() -> re.Pattern:
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:{HY}[0-9ï¼-ï¼™]+)+"
    core_series = rf"{ALPHAJP}{HY}[0-9ï¼-ï¼™]+"
    core = rf"(?:{core_seq}|{core_chap}|{core_series})"
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

LABEL_LINE_RE = build_label_line_regex_mixed()

def split_segments_by_label(all_text: str) -> List[Dict[str, Any]]:
    """
    å…¨æ–‡ã‚’å˜ç‹¬è¡Œã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§åˆ†å‰²ã—ã€[{'page_label','body'}] ã‚’è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºé…åˆ—ï¼ˆâ†’å¾Œæ®µã§æ•°å€¤ãƒšãƒ¼ã‚¸ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ã€‚
    """
    txt = normalize_strict(all_text.replace("\r\n","\n").replace("\r","\n"))
    matches = list(LABEL_LINE_RE.finditer(txt))
    if not matches:
        return []

    def next_nonempty_pos(pos: int) -> int:
        n = pos
        while n < len(txt) and txt[n] == "\n":
            n += 1
        return n

    segs: List[Dict[str, Any]] = []
    for i, m in enumerate(matches):
        label = norm_label_core(m.group("label"))
        start = next_nonempty_pos(m.end())
        end   = matches[i+1].start() if i+1 < len(matches) else len(txt)
        body  = txt[start:end].lstrip("\n ")
        segs.append({"page_label": label, "body": body})
    return segs


# === ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«å˜ä½ï¼‰ ===
all_text_joined = "\n".join(pages_text)
segments = split_segments_by_label(all_text_joined)

if not segments:
    st.warning("å˜ç‹¬è¡Œã®ã€ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ•°å€¤ãƒšãƒ¼ã‚¸ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ•°å€¤ãƒšãƒ¼ã‚¸ã”ã¨ã«æ“¬ä¼¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
    segments = [{"page_label": str(i+1), "body": normalize_strict(p)} for i, p in enumerate(pages_text)]

st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã§åŒºåˆ‡ã‚Šï¼‰â€” æ¦‚è¦³")
df_segments_overview = pd.DataFrame([{
    "ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«": s["page_label"],
    "char_count": len(s["body"]),
    "preview": s["body"][:120].replace("\n"," ") + ("â€¦" if len(s["body"])>120 else "")
} for s in segments])
st.dataframe(df_segments_overview, use_container_width=True)


# =========================
# å›³è¡¨è¦‹å‡ºã—ï¼æœ¬æ–‡å‚ç…§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
# =========================
# è¦‹å‡ºã—ï¼šè¡Œå…¨ä½“ãŒã€Œ(å›³|è¡¨|å›³è¡¨|Fig|Table) + ç•ªå· + ã‚¿ã‚¤ãƒˆãƒ«ã€ã§æ§‹æˆ
RE_HEAD_LINE = re.compile(
    rf"""(?ix) ^
    \s*
    (?P<kind>å›³è¡¨|å›³|è¡¨|Fig(?:\.|ure)?|Tab(?:\.|le)?)
    \s*[:ï¼šï¼.]?\s*
    (?P<label>
        [0-9]+(?:-{1}[0-9]+)* |      # 12 / 1-2 / 2-10-3
        \([0-9]+\) | \[[0-9]+\]      # (1) / [1]
    )
    (?P<after>
        (?:\s*(?:{LEADER_CHARS_CLASS}|\:|ï¼š))*    # è¨˜å·
        .*                                        # ã‚¿ã‚¤ãƒˆãƒ«
    )? $
""")

# æœ¬æ–‡å‚ç…§ï¼šè¡Œä¸­ã«å‡ºç¾ã™ã‚‹ã‚‚ã®ã‚’å…¨ã¦æ‹¾ã†
RE_REF_INLINE = re.compile(
    rf"""(?ix)
    (?P<kind>å›³è¡¨|å›³|è¡¨|Fig(?:\.|ure)?|Tab(?:\.|le)?)
    \s*[:ï¼šï¼.]?\s*
    (?P<label>
        [0-9]+(?:-{1}[0-9]+)* |
        \([0-9]+\) | \[[0-9]+\]
    )
    """
)

def norm_kind(k: str) -> str:
    k2 = k.lower()
    if k2.startswith("fig"): return "å›³"
    if k2.startswith("tab"): return "è¡¨"
    return "å›³è¡¨" if "å›³è¡¨" in k else ("å›³" if "å›³" in k else "è¡¨")


# =========================
# æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«å˜ä½ï¼‰
# =========================
def extract_headings_from_segments(segs: List[Dict[str, str]], min_caption: int) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for seg in segs:
        label_page = seg["page_label"]
        for raw in seg["body"].split("\n"):
            line = normalize_strict(raw)
            if not line: 
                continue
            m = RE_HEAD_LINE.match(line)
            if not m:
                continue
            kind = norm_kind(m.group("kind"))
            num  = norm_label_core(m.group("label"))
            title = (m.group("after") or "").strip()
            if len(title) < min_caption:
                title = line[m.end():].strip() or line
            title = title[:200] + ("â€¦" if len(title) > 200 else "")
            out.append({
                "ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«": label_page,
                "ç¨®åˆ¥": kind,
                "ç•ªå·": num,
                "è¦‹å‡ºã—": title,
                "è¡Œå…¨æ–‡": line,
            })
    return out

def extract_refs_from_segments(segs: List[Dict[str, str]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for seg in segs:
        label_page = seg["page_label"]
        for raw in seg["body"].split("\n"):
            line = normalize_strict(raw)
            if not line:
                continue
            for m in RE_REF_INLINE.finditer(line):
                kind = norm_kind(m.group("kind"))
                num  = norm_label_core(m.group("label"))
                out.append({
                    "ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«": label_page,
                    "ç¨®åˆ¥": kind,
                    "ç•ªå·": num,
                    "å‘¨è¾ºæŠœç²‹": line[:200] + ("â€¦" if len(line) > 200 else ""),
                })
    return out


heads = extract_headings_from_segments(segments, min_caption)
refs  = extract_refs_from_segments(segments)

df_heads = pd.DataFrame(heads)
df_refs  = pd.DataFrame(refs)

if df_heads.empty and df_refs.empty:
    st.warning("å›³è¡¨ã®è¦‹å‡ºã—ãƒ»å‚ç…§ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ›¸å¼ã‚„é–¾å€¤ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

st.subheader("ğŸ“Œ è¦‹å‡ºã—ï¼ˆå›³/è¡¨/å›³è¡¨ï¼‰æ¤œå‡ºçµæœ")
st.dataframe(df_heads.sort_values(["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«","ç¨®åˆ¥","ç•ªå·"]) if not df_heads.empty else pd.DataFrame(), use_container_width=True)

st.subheader("ğŸ” æœ¬æ–‡å‚ç…§ã®æ¤œå‡ºçµæœ")
st.dataframe(df_refs.sort_values(["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«","ç¨®åˆ¥","ç•ªå·"]) if not df_refs.empty else pd.DataFrame(), use_container_width=True)


# =========================
# ç…§åˆï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã‚’ä¸»ã‚­ãƒ¼ã«ï¼‰
# =========================
def kind_keys(kind: str, strict: bool) -> List[str]:
    if strict:
        return [kind]
    if kind == "å›³è¡¨":
        return ["å›³è¡¨", "å›³", "è¡¨"]
    return [kind, "å›³è¡¨"]

def cross_check_by_label(head_df: pd.DataFrame, ref_df: pd.DataFrame, strict: bool) -> Tuple[pd.DataFrame, pd.DataFrame]:
    # å‚ç…§ã‚’ { (ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«, ç¨®åˆ¥, ç•ªå·) : [å‚ç…§â€¦] } ã«
    refs_map: Dict[Tuple[str,str,str], List[Dict[str,Any]]] = {}
    for _, r in ref_df.iterrows():
        refs_map.setdefault((r["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"], r["ç¨®åˆ¥"], r["ç•ªå·"]), []).append(r)

    # è¦‹å‡ºã—â†’å‚ç…§
    rows_h2r: List[Dict[str, Any]] = []
    for _, h in head_df.iterrows():
        hits: List[Dict[str, Any]] = []
        for k in kind_keys(h["ç¨®åˆ¥"], strict):
            key = (h["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"], k, h["ç•ªå·"])
            if key in refs_map:
                for rr in refs_map[key]:
                    hits.append({"ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«": rr["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"], "æŠœç²‹": rr["å‘¨è¾ºæŠœç²‹"]})
        rows_h2r.append({
            "ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«": h["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"],
            "è¦‹å‡ºã—_ç¨®åˆ¥": h["ç¨®åˆ¥"],
            "è¦‹å‡ºã—_ç•ªå·": h["ç•ªå·"],
            "è¦‹å‡ºã—_ã‚¿ã‚¤ãƒˆãƒ«": h.get("è¦‹å‡ºã—",""),
            "å‚ç…§ãƒ’ãƒƒãƒˆæ•°": len(hits),
            "å‚ç…§ã‚ã‚Š": "Yes" if hits else "No",
        })
    df_h2r = pd.DataFrame(rows_h2r)

    # å‚ç…§â†’è¦‹å‡ºã—å­˜åœ¨
    head_keys = set((row["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"], row["ç¨®åˆ¥"], row["ç•ªå·"]) for _, row in head_df.iterrows())
    if not strict:
        # å¯›å®¹ï¼šå›³è¡¨<->å›³/è¡¨ã‚’ç›¸äº’ã«å±•é–‹
        expanded = set()
        for pl, kind, num in head_keys:
            for kk in kind_keys(kind, strict=False):
                expanded.add((pl, kk, num))
        head_keys = expanded

    rows_r2h: List[Dict[str, Any]] = []
    for _, r in ref_df.iterrows():
        exists = (r["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"], r["ç¨®åˆ¥"], r["ç•ªå·"]) in head_keys
        rows_r2h.append({
            "ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«": r["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«"],
            "å‚ç…§_ç¨®åˆ¥": r["ç¨®åˆ¥"],
            "å‚ç…§_ç•ªå·": r["ç•ªå·"],
            "å¯¾å¿œè¦‹å‡ºã—ã‚ã‚Š": "Yes" if exists else "No",
            "å‘¨è¾ºæŠœç²‹": r["å‘¨è¾ºæŠœç²‹"],
        })
    df_r2h = pd.DataFrame(rows_r2h)
    return df_h2r, df_r2h


df_h2r, df_r2h = cross_check_by_label(df_heads, df_refs, strict=strict_kind_match)

st.subheader("âœ… è¦‹å‡ºã— â†’ å‚ç…§ ç…§åˆï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«å˜ä½ï¼‰")
st.dataframe(df_h2r.sort_values(["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«","è¦‹å‡ºã—_ç¨®åˆ¥","è¦‹å‡ºã—_ç•ªå·"]), use_container_width=True)

st.subheader("âœ… å‚ç…§ â†’ è¦‹å‡ºã— å­˜åœ¨ç¢ºèªï¼ˆãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«å˜ä½ï¼‰")
st.dataframe(df_r2h.sort_values(["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«","å‚ç…§_ç¨®åˆ¥","å‚ç…§_ç•ªå·"]), use_container_width=True)

summary = {
    "è¦‹å‡ºã—æ•°": int(len(df_heads)),
    "å‚ç…§æ•°": int(len(df_refs)),
    "è¦‹å‡ºã—ã®å‚ç…§ãªã—ä»¶æ•°": int((df_h2r["å‚ç…§ã‚ã‚Š"]=="No").sum()),
    "å‚ç…§ã®è¦‹å‡ºã—æ¬ è½ä»¶æ•°": int((df_r2h["å¯¾å¿œè¦‹å‡ºã—ã‚ã‚Š"]=="No").sum()),
}
st.markdown(f"**é›†è¨ˆ**: {summary}")


# =========================
# Excelå‡ºåŠ›ï¼ˆç•ªå·/ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã¯æ–‡å­—åˆ—å›ºå®šï¼‰
# =========================
xlsx_buf = io.BytesIO()
with pd.ExcelWriter(xlsx_buf, engine="xlsxwriter") as writer:
    if not df_heads.empty:
        df_heads.sort_values(["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«","ç¨®åˆ¥","ç•ªå·"]).to_excel(writer, index=False, sheet_name="headings")
    if not df_refs.empty:
        df_refs.sort_values(["ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«","ç¨®åˆ¥","ç•ªå·"]).to_excel(writer, index=False, sheet_name="references")
    df_h2r.to_excel(writer, index=False, sheet_name="H_to_R")
    df_r2h.to_excel(writer, index=False, sheet_name="R_to_H")

    wb = writer.book
    text_fmt = wb.add_format({"num_format": "@"})
    wrap = wb.add_format({"text_wrap": True})
    bold = wb.add_format({"bold": True})

    def format_sheet(sheet: str, cols: List[str]):
        ws = writer.sheets[sheet]
        for j, name in enumerate(cols):
            # æ–‡å­—åˆ—å›ºå®šã«ã—ã¦ Excel ã®æ—¥ä»˜åŒ–ã‚’é˜²æ­¢
            if name in ("ç•ªå·", "ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«") or name.endswith("ç•ªå·"):
                ws.set_column(j, j, 16, text_fmt)
            elif "è¦‹å‡ºã—" in name or "æŠœç²‹" in name or "ã‚¿ã‚¤ãƒˆãƒ«" in name:
                ws.set_column(j, j, 48, wrap)
            else:
                ws.set_column(j, j, 12)
            ws.write(0, j, name, bold)
        ws.freeze_panes(1, 0)
        ws.autofilter(0, 0, 100000, len(cols)-1)

    for sheet, df_ in [
        ("headings", df_heads) if not df_heads.empty else None,
        ("references", df_refs) if not df_refs.empty else None,
        ("H_to_R", df_h2r),
        ("R_to_H", df_r2h),
    ]:
        if sheet:
            format_sheet(sheet, df_.columns.tolist())

st.download_button(
    "ğŸ“¥ Excelã§ä¿å­˜ï¼ˆheadings / references / H_to_R / R_to_Hï¼‰",
    data=xlsx_buf.getvalue(),
    file_name="figure_table_crosscheck_by_page_label.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)

if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.write({
        "segments": len(segments),
        "heads_detected": 0 if df_heads is None else len(df_heads),
        "refs_detected": 0 if df_refs is None else len(df_refs),
        "strict_kind_match": strict_kind_match
    })
