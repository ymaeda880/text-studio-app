# -*- coding: utf-8 -*-
# pages/72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢.py
from __future__ import annotations

import io, re
from pathlib import Path
from typing import List, Iterable

import pandas as pd
import streamlit as st

# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(page_title="ğŸ” ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” 72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢")
st.caption("ãƒ•ã‚©ãƒ«ãƒ€: data/redlist/{fukushima, MOE, chiba} ã‚’èª­ã¿è¾¼ã¿ã€å’Œå/ç¨®åã‚’è¤‡æ•°èªã¾ã¨ã‚ã¦æ¨ªæ–­æ¤œç´¢ã—ã¾ã™ã€‚")

# =========================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆlib/redlist/loaders ã‹ã‚‰ï¼‰
# =========================================================
from lib.redlist.loaders import load_all

DATA_ROOT = Path("data/redlist").resolve()
moe_df, fuku_df, chiba_df = load_all(DATA_ROOT)

with st.sidebar:
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    st.code(str(DATA_ROOT))
    st.markdown("**èª­ã¿è¾¼ã¿ä»¶æ•°ï¼ˆå‚è€ƒï¼‰**")
    st.write({
        "ç’°å¢ƒçœ": 0 if moe_df is None or moe_df.empty else int(moe_df.shape[0]),
        "ç¦å³¶çœŒ": 0 if fuku_df is None or fuku_df.empty else int(fuku_df.shape[0]),
        "åƒè‘‰çœŒ": 0 if chiba_df is None or chiba_df.empty else int(chiba_df.shape[0]),
    })

# =========================================================
# å…¥åŠ›ï¼ˆè¤‡æ•°èªï¼‰ï¼šã€Œå’Œåã€ï¼ˆåƒè‘‰ã¯ã€Œç¨®åã€ï¼‰
# =========================================================
st.subheader("ğŸ§­ å’Œå/ç¨®å æ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰")

def normalize_name(s: str) -> str:
    if pd.isna(s):
        return ""
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’ã€å‰å¾Œç©ºç™½é™¤å»
    return str(s).replace("\u3000", " ").strip()

def parse_queries(raw: str) -> List[str]:
    """
    ã‚«ãƒ³ãƒãƒ»ç©ºç™½ï¼ˆåŠ/å…¨ï¼‰ãƒ»ã‚¿ãƒ–ãƒ»æ”¹è¡Œãƒ»ç©ºç™½è¡Œã§åˆ†å‰²ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆé †åºä¿æŒï¼‰ã€‚
    """
    if not raw:
        return []
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’
    raw = raw.replace("\u3000", " ")
    # ã‚«ãƒ³ãƒ or ä»»æ„ã®ãƒ›ãƒ¯ã‚¤ãƒˆã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
    parts = re.split(r"[,\s]+", raw)
    # ç©ºè¦ç´ ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–
    parts = [normalize_name(p) for p in parts if p and normalize_name(p)]
    # é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
    seen = set()
    uniq: List[str] = []
    for p in parts:
        if p not in seen:
            seen.add(p)
            uniq.append(p)
    return uniq

col_q1, col_q2 = st.columns([3, 1])
with col_q1:
    multi_query_raw = st.text_area(
        "å’Œåï¼ˆç’°å¢ƒçœ/ç¦å³¶ï¼‰ãƒ»ç¨®åï¼ˆåƒè‘‰ï¼‰ã‚’æ”¹è¡Œ/ã‚«ãƒ³ãƒ/ã‚¹ãƒšãƒ¼ã‚¹/ã‚¿ãƒ–ã§è¤‡æ•°å…¥åŠ›ã§ãã¾ã™",
        value="",
        height=120,
        placeholder="ä¾‹ï¼‰\nãƒ‹ãƒ›ãƒ³ã‚¶ãƒ«\nãƒ¤ãƒãƒ, ãƒˆã‚­\n  ãƒ¤ãƒ³ãƒãƒ«ã‚¯ã‚¤ãƒŠ"
    )
with col_q2:
    mode_exact = st.toggle("å®Œå…¨ä¸€è‡´", value=True, help="OFFã«ã™ã‚‹ã¨éƒ¨åˆ†ä¸€è‡´ï¼ˆåŒ…å«ï¼‰ã§æ¤œç´¢ã—ã¾ã™ã€‚")

queries = parse_queries(multi_query_raw)
if queries:
    st.caption(f"ğŸ” æ¤œç´¢èªï¼š{', '.join(queries)}ï¼ˆ{len(queries)} èªï¼‰")

def mask_for_queries(series: pd.Series, qs: Iterable[str], exact: bool) -> pd.Series:
    """
    series ã‚’æ­£è¦åŒ–ã—ãŸä¸Šã§ã€è¤‡æ•°ã‚¯ã‚¨ãƒªã® OR ãƒãƒƒãƒã‚’è¿”ã™ã€‚
    exact=True: å®Œå…¨ä¸€è‡´ï¼ˆ==ã®ORï¼‰
    exact=False: éƒ¨åˆ†ä¸€è‡´ï¼ˆcontainsã®ORï¼‰
    """
    s_norm = series.fillna("").map(normalize_name)
    if exact:
        return s_norm.isin(list(qs))
    # éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚±ãƒ¼ã‚¹ç„¡è¦–ï¼‰
    s_low = s_norm.str.lower()
    mask = pd.Series(False, index=series.index)
    for q in qs:
        if not q:
            continue
        ql = q.lower()
        mask = mask | s_low.str.contains(re.escape(ql), na=False)
    return mask

results: List[pd.DataFrame] = []

if queries:
    # ç’°å¢ƒçœï¼šå’Œå
    if moe_df is not None and not moe_df.empty and "å’Œå" in moe_df.columns:
        mask = mask_for_queries(moe_df["å’Œå"], queries, mode_exact)
        hit = moe_df.loc[mask].copy()
        if not hit.empty:
            hit.insert(0, "ã‚½ãƒ¼ã‚¹", "ç’°å¢ƒçœ")
            cols = ["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","å’Œå","å­¦å","åˆ†é¡ç¾¤","ã‚«ãƒ†ã‚´ãƒªãƒ¼","ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
            for c in cols:
                if c not in hit.columns:
                    hit[c] = ""
            results.append(hit[cols])

    # ç¦å³¶çœŒï¼šå’Œå
    if fuku_df is not None and not fuku_df.empty and "å’Œå" in fuku_df.columns:
        mask = mask_for_queries(fuku_df["å’Œå"], queries, mode_exact)
        hit = fuku_df.loc[mask].copy()
        if not hit.empty:
            hit.insert(0, "ã‚½ãƒ¼ã‚¹", "ç¦å³¶çœŒ")
            cols = ["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","å’Œå","å­¦å","ç”Ÿç‰©ç¾¤","åˆ†é¡","ç§‘å",
                    "ç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼","ãµãã—ã¾RL2022ã‚«ãƒ†ã‚´ãƒªãƒ¼","ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
            for c in cols:
                if c not in hit.columns:
                    hit[c] = ""
            results.append(hit[cols])

    # åƒè‘‰çœŒï¼šç¨®å
    if chiba_df is not None and not chiba_df.empty and "ç¨®å" in chiba_df.columns:
        mask = mask_for_queries(chiba_df["ç¨®å"], queries, mode_exact)
        hit = chiba_df.loc[mask].copy()
        if not hit.empty:
            hit.insert(0, "ã‚½ãƒ¼ã‚¹", "åƒè‘‰çœŒ")
            cols = ["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","ç¨®å","å­¦å","åˆ†é¡ç¾¤","ç›®ãƒ»ç§‘å","ã‚«ãƒ†ã‚´ãƒªãƒ¼","è¨˜å·","åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
            for c in cols:
                if c not in hit.columns:
                    hit[c] = ""
            results.append(hit[cols])

# =========================================================
# çµæœè¡¨ç¤º & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå†…éƒ¨ä¿å­˜ï¼‹æ¤œç´¢èªã®å…¥åŠ›é †ã§æ•´åˆ—ï¼‰
# =========================================================
if queries:
    def _norm(s: pd.Series) -> pd.Series:
        return s.fillna("").astype(str).str.replace("\u3000", " ").str.strip()

    # å…¥åŠ›é †ã‚’ä¿æŒã—ã¤ã¤é‡è¤‡ã¯æœ€åˆã ã‘æ¡ç”¨
    queries_ordered: list[str] = []
    seen = set()
    for q in queries:
        qn = q.replace("\u3000", " ").strip()
        if qn and qn not in seen:
            queries_ordered.append(qn)
            seen.add(qn)

    def _hits_for_query(q: str) -> pd.DataFrame:
        parts = []

        # --- ç’°å¢ƒçœï¼ˆå’Œåï¼‰
        if not moe_df.empty and "å’Œå" in moe_df.columns:
            m = _norm(moe_df["å’Œå"]) == q
            h = moe_df.loc[m].copy()
            if not h.empty:
                h.insert(0, "ã‚½ãƒ¼ã‚¹", "ç’°å¢ƒçœ")
                h.insert(0, "æ¤œç´¢èª", q)
                cols = ["æ¤œç´¢èª","ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","å’Œå","å­¦å","åˆ†é¡ç¾¤","ã‚«ãƒ†ã‚´ãƒªãƒ¼","ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
                for c in cols:
                    if c not in h.columns: h[c] = ""
                parts.append(h[cols])

        # --- ç¦å³¶çœŒï¼ˆå’Œåï¼‰
        if not fuku_df.empty and "å’Œå" in fuku_df.columns:
            m = _norm(fuku_df["å’Œå"]) == q
            h = fuku_df.loc[m].copy()
            if not h.empty:
                h.insert(0, "ã‚½ãƒ¼ã‚¹", "ç¦å³¶çœŒ")
                h.insert(0, "æ¤œç´¢èª", q)
                cols = ["æ¤œç´¢èª","ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","å’Œå","å­¦å","ç”Ÿç‰©ç¾¤","åˆ†é¡","ç§‘å",
                        "ç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼","ãµãã—ã¾RL2022ã‚«ãƒ†ã‚´ãƒªãƒ¼","ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
                for c in cols:
                    if c not in h.columns: h[c] = ""
                parts.append(h[cols])

        # --- åƒè‘‰çœŒï¼ˆç¨®åï¼‰
        if not chiba_df.empty and "ç¨®å" in chiba_df.columns:
            m = _norm(chiba_df["ç¨®å"]) == q
            h = chiba_df.loc[m].copy()
            if not h.empty:
                h.insert(0, "ã‚½ãƒ¼ã‚¹", "åƒè‘‰çœŒ")
                h.insert(0, "æ¤œç´¢èª", q)
                cols = ["æ¤œç´¢èª","ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","ç¨®å","å­¦å",
                        "åˆ†é¡ç¾¤","ç›®ãƒ»ç§‘å","ã‚«ãƒ†ã‚´ãƒªãƒ¼","è¨˜å·","åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
                for c in cols:
                    if c not in h.columns: h[c] = ""
                parts.append(h[cols])

        return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()

    # æ¤œç´¢èªã®å…¥åŠ›é †ã§ãƒ–ãƒ­ãƒƒã‚¯é€£çµï¼ˆã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼‰
    blocks = [ _hits_for_query(q) for q in queries_ordered ]
    results_df = pd.concat([b for b in blocks if not b.empty], ignore_index=True) if any(not b.empty for b in blocks) else pd.DataFrame()

    # ---- å†…éƒ¨ä¿å­˜ï¼ˆæœ€æ–°ã®æ¤œç´¢çµæœï¼‰ ----
    st.session_state["redlist_last_queries"] = queries_ordered
    st.session_state["redlist_last_results"] = results_df

    if results_df.empty:
        st.warning("ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.success(f"æ¤œç´¢ãƒ’ãƒƒãƒˆä»¶æ•°: {len(results_df):,} ä»¶ï¼ˆ{len(queries_ordered)}èªï¼‰")
        # å…¥åŠ›é †ã§ã‚½ãƒ¼ãƒˆï¼ˆåŒã˜æ¤œç´¢èªå†…ã¯å…ƒé †ã®ã¾ã¾ï¼‰
        if "æ¤œç´¢èª" in results_df.columns:
            results_df["__ord"] = results_df["æ¤œç´¢èª"].map({q:i for i,q in enumerate(queries_ordered)})
            results_df = results_df.sort_values(["__ord"]).drop(columns="__ord")
        
        # ---- åŒã˜æ¤œç´¢èªãŒé€£ç¶šã™ã‚‹å ´åˆã€2è¡Œç›®ä»¥é™ã¯ç©ºç™½åŒ–ã—ã¦è¦‹ã‚„ã™ã ----
        if not results_df.empty and "æ¤œç´¢èª" in results_df.columns:
            mask = results_df["æ¤œç´¢èª"].shift() == results_df["æ¤œç´¢èª"]
            results_df.loc[mask, "æ¤œç´¢èª"] = ""

        st.dataframe(results_df, use_container_width=True)

        # Excelï¼ˆ1ã‚·ãƒ¼ãƒˆã€å…¥åŠ›é †ã®ã¾ã¾ï¼‰
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
            results_df.to_excel(writer, sheet_name="Search_Results", index=False)

            # ã¤ã„ã§ã«ç°¡å˜ãªé›†è¨ˆï¼ˆæ¤œç´¢èªÃ—ã‚½ãƒ¼ã‚¹ä»¶æ•°ï¼‰ã‚’2æšç›®ã«
            if not results_df.empty and "ã‚½ãƒ¼ã‚¹" in results_df.columns and "æ¤œç´¢èª" in results_df.columns:
                cnt = results_df.groupby(["æ¤œç´¢èª","ã‚½ãƒ¼ã‚¹"]).size().reset_index(name="ä»¶æ•°")
                cnt["__ord"] = cnt["æ¤œç´¢èª"].map({q:i for i,q in enumerate(queries_ordered)})
                cnt = cnt.sort_values(["__ord","ã‚½ãƒ¼ã‚¹"]).drop(columns="__ord")
                cnt.to_excel(writer, sheet_name="Counts", index=False)

        st.download_button(
            "ğŸ“¥ æ¤œç´¢çµæœã‚’Excelã§ä¿å­˜",
            data=buf.getvalue(),
            file_name="redlist_search_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
else:
    st.info("æ¤œç´¢èªã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")



# =========================================================
# å‚™è€ƒ
# =========================================================
with st.expander("â„¹ï¸ å‚™è€ƒ", expanded=False):
    st.markdown("""
- **è¤‡æ•°èªå…¥åŠ›**ï¼šã‚«ãƒ³ãƒãƒ»ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆåŠ/å…¨ï¼‰ãƒ»ã‚¿ãƒ–ãƒ»æ”¹è¡Œãƒ»ç©ºç™½è¡Œã§åŒºåˆ‡ã‚Œã¾ã™ã€‚  
- **å®Œå…¨ä¸€è‡´**ï¼ˆæ—¢å®šï¼‰/ **éƒ¨åˆ†ä¸€è‡´** ã‚’ãƒˆã‚°ãƒ«ã§åˆ‡æ›¿ã§ãã¾ã™ã€‚  
- ç’°å¢ƒçœãƒ»ç¦å³¶ã¯ **ã€Œå’Œåã€**ã€åƒè‘‰ã¯ **ã€Œç¨®åã€** ã‚’æ¤œç´¢å¯¾è±¡ã«ã—ã¦ã„ã¾ã™ã€‚  
- è¡¨ç¤ºãƒ»ä¿å­˜ã®åˆ—åã¯ **å‚ç…§ãƒ©ãƒ™ãƒ«** ã§çµ±ä¸€ã—ã¦ã„ã¾ã™ã€‚  
- å–è¾¼ãƒ»æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã¯ `lib/redlist/loaders.py` ã«ä¾å­˜ã—ã¾ã™ã€‚  
    """)
