# -*- coding: utf-8 -*-
# pages/22_ç›®æ¬¡ãƒã‚§ãƒƒã‚¯.py â€” GPT API ä¸ä½¿ç”¨ç‰ˆï¼šç›®æ¬¡å€™è£œ â†” æœ¬æ–‡ï¼ˆè¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰ç…§åˆ
from __future__ import annotations
import io, re, tempfile
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import pandas as pd

# ==== PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆ ====
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
try:
    import pdfplumber
except Exception:
    pdfplumber = None


# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆï¼‰", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ ç›®æ¬¡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç…§åˆ / è¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰")
st.caption("GPT ã‚’ä½¿ã‚ãšã€ç›®æ¬¡å€™è£œï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’æœ¬æ–‡ã«å¯¾ã—ã¦ **è¡Œã”ã¨ã«é †ç•ªã«** ç…§åˆã—ã¾ã™ã€‚")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])

c1, c2, c3 = st.columns([1.3, 1.1, 1.2])
with c1:
    scheme = "auto"
with c2:
    toc_join_front = st.checkbox("ç›®æ¬¡æŠ½å‡ºã¯å†’é ­10pã‚’é€£çµ", value=True)
with c3:
    run = st.button("â–¶ è§£æãƒ»ç…§åˆã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    max_toc_lines = st.number_input("ç›®æ¬¡å€™è£œã®ä¸Šé™è¡Œæ•°", min_value=10, max_value=500, value=120, step=10)
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", value=False)

if not uploaded or not run:
    st.stop()
if fitz is None and pdfplumber is None:
    st.error("PyMuPDF ã‹ pdfplumber ã®ã©ã¡ã‚‰ã‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚`pip install pymupdf pdfplumber`")
    st.stop()


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
def pdf_to_text_per_page(pdf_path: Path) -> List[str]:
    texts: List[str] = []
    if fitz is not None:
        doc = fitz.open(str(pdf_path))
        for p in doc:
            texts.append(p.get_text("text") or "")
    else:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                texts.append(p.extract_text() or "")
    return texts


with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    pdf_path = td / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")


# =========================
# æ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ24_å›³è¡¨ãƒã‚§ãƒƒã‚¯.py ã¨åŒä¸€ã®æ–¹é‡ï¼‰
# =========================
HY = r"[\-\u2010\u2011\u2012\u2013\u2014\u2015\u2212\uFF0D\u30FC]"   # å„ç¨®ãƒã‚¤ãƒ•ãƒ³/é•·éŸ³
LEADER_CHARS_CLASS = r"[\.ï¼ãƒ»ï½¥â€¦â€§ï½¡]"
LEADERS_SPACED = rf"(?:\s*{LEADER_CHARS_CLASS}\s*){{3,}}"

def z2h_numhy(s: str) -> str:
    """å…¨è§’æ•°å­—/è¨˜å·â†’åŠè§’ã€å„ç¨®ãƒã‚¤ãƒ•ãƒ³é¡â†’ '-' ã«çµ±ä¸€ã€‚"""
    s = (s or "").replace("\u3000", " ")
    s = s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰ï¼»ï¼½ï½›ï½", "0123456789()[]{}"))
    return re.sub(HY, "-", s)

def normalize_strict(s: str) -> str:
    """è¡Œå˜ä½ã®æ­£è¦åŒ–ï¼ˆãƒªãƒ¼ãƒ€ãƒ¼å‰Šé™¤ãƒ»ç©ºç™½åœ§ç¸®ï¼‰ã€‚"""
    s = z2h_numhy(s)
    s = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_loose(s: str) -> str:
    """ã‚†ã‚‹ã‚ã®æ­£è¦åŒ–ï¼ˆå…¨æ–‡æ¤œç´¢ã‚„éƒ¨åˆ†ä¸€è‡´ç”¨ï¼‰ã€‚"""
    s = z2h_numhy(s)
    s = re.sub(rf"{LEADERS_SPACED}$", "", s)
    return re.sub(r"\s+", "", s)


# =========================
# ç›®æ¬¡å€™è£œã®æŠ½å‡ºï¼ˆæœ«å°¾ãƒ©ãƒ™ãƒ«æ¤œå‡ºï¼‰
# =========================
ALPHAJP = r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]+"  # è‹±å­—/ã‹ãª/æ¼¢å­—ã®é€£ç¶š

def build_label_tail_regex_mixed() -> re.Pattern:
    """
    è¡Œæœ«ã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«ã‚’æ¤œå‡ºã—ã¦ã€å·¦å´æœ¬æ–‡ï¼ˆç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã¨åˆ†é›¢ã™ã‚‹ã€‚
    è¨±å®¹:
      1) é€£ç•ª:           12
      2) ç« -ãƒšãƒ¼ã‚¸:      1-2, 3-10-2ï¼ˆãƒã‚¤ãƒ•ãƒ³å‰å¾Œã®ç©ºç™½ã‚‚è¨±å®¹ï¼‰
      3) ã‚·ãƒªãƒ¼ã‚º-æ•°å­—:  åº-1, è³‡-2, ä»˜-3, A-10 ç­‰
    """
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:\s*{HY}\s*[0-9ï¼-ï¼™]+)+"
    core_series = rf"{ALPHAJP}\s*{HY}\s*[0-9ï¼-ï¼™]+"
    tail = rf"(?P<label>(?:{core_seq}|{core_chap}|{core_series}))"

    pat = rf"""
        ^(?P<head>.*?)                            # å·¦å´æœ¬æ–‡
        (?:{LEADERS_SPACED}|\s{{2,}})?            # ãƒ‰ãƒƒãƒˆ/ä¸­é»’/â€¦ + ç©ºç™½ã®ãƒªãƒ¼ãƒ€ãƒ¼åˆ— or 2é€£ä»¥ä¸Šã®ç©ºç™½
        {tail}\s*$                                # è¡Œæœ«ãƒ©ãƒ™ãƒ«
    """
    return re.compile(pat, re.X)

# â€”â€”â€” ã“ã“ã‹ã‚‰ãŒ â€œ24_å›³è¡¨ãƒã‚§ãƒƒã‚¯.py ã¨åŒã˜â€ å˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«æ¤œå‡º â€”â€”â€”
def build_label_line_regex_mixed() -> re.Pattern:
    """
    è¡Œå˜ç‹¬ã§ç¾ã‚Œã‚‹é ãƒ©ãƒ™ãƒ«ï¼ˆæ•°å­— / ç« -é  / ã‚·ãƒªãƒ¼ã‚ºï¼‹ç•ªå·ï¼‰ã‚’æ¤œå‡ºã€‚
    ä¾‹: "12", "2-1", "3-10-2", "è³‡æ–™1", "(è³‡æ–™)12", "ï¼»åºï¼½-3", "p.12"
    """
    core_seq    = r"[0-9ï¼-ï¼™]{1,6}"                          # 12
    core_chap   = rf"[0-9ï¼-ï¼™]+(?:\s*{HY}\s*[0-9ï¼-ï¼™]+)+"     # 2-1, 3-10-2ï¼ˆç©ºç™½è¨±å®¹ï¼‰
    series_word = rf"[ï¼ˆ(ï¼»\[]?{ALPHAJP}[ï¼‰)\]ï¼½]?"             # (è³‡æ–™) / ï¼»åºï¼½ / è³‡æ–™ / åº
    SEP_OPT     = rf"(?:\s*(?:{HY}|[\.ï¼ãƒ»ï½¥])\s*|\s+)?"        # -, . , ã¾ãŸã¯ç©ºç™½ï¼ˆä»»æ„ï¼‰
    core_series = rf"{series_word}{SEP_OPT}[0-9ï¼-ï¼™]+"        # è³‡æ–™1 / (è³‡æ–™)12 / ï¼»åºï¼½-3 ç­‰
    core = rf"(?:{core_seq}|{core_chap}|{core_series})"
    return re.compile(rf"^\s*(?:p(?:age)?\.?\s*)?(?P<label>{core})\s*$", re.MULTILINE)

LABEL_TAIL_RE = build_label_tail_regex_mixed()
LABEL_LINE_RE = build_label_line_regex_mixed()

def extract_single_page_label(page_text: str) -> Tuple[Optional[str], Optional[str]]:
    """1ãƒšãƒ¼ã‚¸ã‹ã‚‰è¡Œå˜ç‹¬ã®é ãƒ©ãƒ™ãƒ«ã‚’é«˜ã€…1ã¤æŠ½å‡ºã€‚æˆ»ã‚Šå€¤: (label or None, matched_line or None)"""
    if not page_text:
        return None, None
    for raw in page_text.replace("\r\n","\n").replace("\r","\n").split("\n"):
        s = normalize_strict(raw)
        if not s:
            continue
        m = LABEL_LINE_RE.match(s)
        if m:
            return z2h_numhy(m.group("label")), raw
    return None, None
# â€”â€”â€” ã“ã“ã¾ã§ â€”â€”â€”


# =========================
# ç›®æ¬¡å€™è£œæŠ½å‡ºï¼ˆå‰åŠãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
# =========================
def extract_toc_lines(fulltext: str, limit: int) -> List[str]:
    lines = [l.rstrip() for l in fulltext.replace("\r\n","\n").replace("\r","\n").split("\n")]

    head_ok = re.compile(
        r"^\s*(?:"
        r"åº|è³‡æ–™|ä»˜éŒ²|ç¬¬|"          # å…¸å‹ã®å…ˆé ­èª
        r"[0-9ï¼-ï¼™]|"                # ç´ ã®æ•°å­—
        r"\[|ï¼»|"                     # è§’æ‹¬å¼§å§‹ã¾ã‚Š
        r"[ï¼ˆ(][0-9ï¼-ï¼™]{1,3}[ï¼‰)]"  # (1) / ï¼ˆï¼‘ï¼‰ / (12) ãªã©
        r")"
    )
    text_char = re.compile(r"[A-Za-z\u3040-\u30FF\u4E00-\u9FFF]")
    out: List[str] = []

    for ln in lines:
        s = ln.strip()
        if not s or not head_ok.match(s) or not text_char.search(s):
            continue

        m = LABEL_TAIL_RE.match(s)
        if not m:
            continue

        head  = re.sub(rf"\s*{LEADERS_SPACED}\s*$", "", m.group("head")).strip()
        label = z2h_numhy(m.group("label"))

        if len(head) <= 1:
            continue

        out.append(f"{head} ::: {label}")
        if len(out) >= limit:
            break
    return out

front_n = min(10, len(pages_text))
sample_text = "\n".join(pages_text[:front_n]) if toc_join_front else pages_text[0]
toc_lines = extract_toc_lines(sample_text, limit=max_toc_lines)

st.subheader("æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡å€™è£œï¼ˆä¸Šä½ï¼‰")
if not toc_lines:
    st.warning("ç›®æ¬¡å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()
st.code("\n".join(toc_lines[:80]))


# =========================
# ğŸ“Œ 24æ–¹å¼ã«çµ±ä¸€ï¼šå„ãƒšãƒ¼ã‚¸ã‹ã‚‰ â€œå˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«â€ ã‚’1ã¤ã¾ã§æŠ½å‡º â†’ segments ã‚’æ§‹ç¯‰
# =========================
page_labels: List[Optional[str]] = []
segments: List[Dict[str, Any]] = []
for i, ptxt in enumerate(pages_text, start=1):
    label, matched = extract_single_page_label(ptxt)
    page_labels.append(label)
    segments.append({
        "page_label": label if label else "-",
        "body": normalize_strict(ptxt),
        "pdf_page": i,
        "matched_line": matched if matched else "-"
    })

st.subheader("æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆå„ãƒšãƒ¼ã‚¸ã®å˜ç‹¬è¡Œãƒ©ãƒ™ãƒ«ï¼‰â€” æ¦‚è¦³")
df_segments_overview = pd.DataFrame([{
    "pdf_page": s["pdf_page"],
    "page_label": s["page_label"],
    "char_count": len(s["body"]),
    "matched_line": s["matched_line"][:120].replace("\n"," ") if isinstance(s["matched_line"], str) else "-"
} for s in segments])
st.dataframe(df_segments_overview, use_container_width=True)


# =========================
# ãƒ©ãƒ™ãƒ«å¦¥å½“æ€§ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰â†’ valid=True ã®ã¿ã‚’æ¡ç”¨
# =========================
st.subheader("ğŸ“‘ ãƒšãƒ¼ã‚¸ãƒ©ãƒ™ãƒ«æ¤œè¨¼ï¼ˆé€£ç•ªãƒã‚§ãƒƒã‚¯ï¼‰")
def _parse_label_kind(label: str) -> Tuple[str, Any]:
    lab = z2h_numhy(label)
    if re.fullmatch(r"[0-9]+", lab):
        return "seq", int(lab)
    parts = lab.split("-")
    if len(parts) >= 2 and all(p.isdigit() for p in parts):
        return "chap", [int(p) for p in parts]
    m = re.fullmatch(rf"({ALPHAJP})-([0-9]+)", lab)
    if m:
        return "series", (m.group(1), int(m.group(2)))
    return "unknown", None

def valid_and_reason_auto(label: str, prev_ok: Optional[str]) -> Tuple[bool, str]:
    k, cur = _parse_label_kind(label)
    if k == "unknown":
        return False, "ä¸æ˜ãªãƒ©ãƒ™ãƒ«å½¢å¼"
    if prev_ok is None:
        return True, ""
    pk, prev = _parse_label_kind(prev_ok)
    if pk == "unknown":
        return True, ""
    if k != pk:
        return True, "å½¢å¼åˆ‡æ›¿"
    if k == "seq":
        return (cur == prev + 1, "" if cur == prev + 1 else "éé€£ç•ª")
    if k == "chap":
        c, p = (cur + [1, 1])[:2]; pc, pp = (prev + [1, 1])[:2]
        ok = (c == pc and p == pp + 1) or (c == pc + 1 and p == 1)
        return (ok, "" if ok else "éé€£ç•ª")
    if k == "series":
        s, n = cur; ps, pn = prev
        if s != ps:
            return True, "å½¢å¼åˆ‡æ›¿"
        return (n == pn + 1, "" if n == pn + 1 else "éé€£ç•ª")
    return True, ""


rows_check: List[Dict[str, Any]] = []
prev_ok: Optional[str] = None
for s in segments:
    lab = s["page_label"]
    if lab == "-":
        rows_check.append({
            "pdf_page": s["pdf_page"],
            "page_label": lab,
            "valid": False,
            "reason": "ãƒ©ãƒ™ãƒ«ãªã—",
            "char_count": len(s["body"]),
            "preview": s["body"][:100].replace("\n"," ") + ("â€¦" if len(s["body"])>100 else "")
        })
        continue
    ok, reason = valid_and_reason_auto(lab, prev_ok)
    if ok:
        prev_ok = lab
    rows_check.append({
        "pdf_page": s["pdf_page"],
        "page_label": lab,
        "valid": ok,
        "reason": "" if ok else reason,
        "char_count": len(s["body"]),
        "preview": s["body"][:100].replace("\n"," ") + ("â€¦" if len(s["body"])>100 else "")
    })
df_check = pd.DataFrame(rows_check)
st.dataframe(df_check, use_container_width=True)

# valid=True ã®æŠ½å‡ºãƒšãƒ¼ã‚¸ã®ã¿æ¡ç”¨ï¼ˆpage_label ãŒæœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ï¼‰
valid_segments = [s for s in segments if any(
    (r["pdf_page"] == s["pdf_page"] and r["valid"]) for _, r in df_check.iterrows()
)]
seg_index: Dict[str, str] = {s["page_label"]: s["body"] for s in valid_segments if s["page_label"] != "-"}

# æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalidã®ã¿ï¼‰TXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
if valid_segments:
    txt_buf = io.StringIO()
    for s in valid_segments:
        header = f"==== pdf_page={s['pdf_page']} page_label={s['page_label']} (chars={len(s['body'])}) ====\n"
        txt_buf.write(header)
        txt_buf.write(s["body"].rstrip("\n") + "\n\n")
    st.download_button(
        "ğŸ“¥ æŠ½å‡ºãƒšãƒ¼ã‚¸TXTã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆvalid=True ã®ã¿ï¼‰",
        data=txt_buf.getvalue().encode("utf-8"),
        file_name="extracted_pages_valid.txt",
        mime="text/plain"
    )


# =========================
# ç›®æ¬¡ã‚¿ã‚¤ãƒˆãƒ« â†” æœ¬æ–‡ï¼ˆè¡Œã‚¹ã‚­ãƒ£ãƒ³ï¼‰ç…§åˆ
# =========================
def scan_lines_for_match(title_raw: str, body: str) -> Tuple[str, str]:
    """
    æœ¬æ–‡ body ã‚’æ”¹è¡Œã§åˆ†å‰²ã—ã¦ä¸Šã‹ã‚‰é †ã«ãƒã‚§ãƒƒã‚¯ã€‚
    æˆ»ã‚Šå€¤: (åˆ¤å®š, ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ)
      - "ä¸€è‡´"                    : å³ã—ã‚ã®å®Œå…¨ä¸€è‡´ï¼ˆstrictï¼‰
      - "ä¸€è‡´ï¼ˆç©ºç™½å·®å¸åï¼‰"      : ç©ºç™½å…¨é™¤å»ä¸€è‡´ï¼ˆlooseï¼‰
      - "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰"    : è¡Œã«ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹
      - "éƒ¨åˆ†ä¸€è‡´ï¼ˆNæ–‡å­—ï¼‰"        : ã‚¿ã‚¤ãƒˆãƒ«å…ˆé ­ N=5/4/3 ã®éƒ¨åˆ†ä¸€è‡´
      - "æœªæ¤œå‡º"
    """
    title_strict = normalize_strict(title_raw)
    title_loose  = normalize_loose(title_raw)

    lines = [ln for ln in body.split("\n")]
    for ln in lines:
        if not ln.strip():
            continue
        ln_strict = normalize_strict(ln)
        if ln_strict == title_strict:
            return "ä¸€è‡´", ln.rstrip("\n")
        ln_loose = normalize_loose(ln)
        if ln_loose == title_loose:
            return "ä¸€è‡´ï¼ˆç©ºç™½å·®å¸åï¼‰", ln.rstrip("\n")
        if title_raw in ln:
            return "ä¸€è‡´ï¼ˆè¡Œå†…éƒ¨åˆ†ä¸€è‡´ï¼‰", ln.rstrip("\n")
        for klen in (5, 4, 3):
            if len(title_raw) >= klen and title_raw[:klen] in ln:
                return f"éƒ¨åˆ†ä¸€è‡´ï¼ˆ{klen}æ–‡å­—ï¼‰", ln.rstrip("\n")
    return "æœªæ¤œå‡º", "-"


def check_toc_by_order(toc_lines: List[str],
                       seg_index: Dict[str, str],
                       pages_text: List[str]) -> pd.DataFrame:
    """
    1) ç›®æ¬¡ãƒ©ãƒ™ãƒ«ã¨ä¸€è‡´ã™ã‚‹æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=Trueï¼‰å†…ã®è¡Œã‚’ä¸Šã‹ã‚‰é †ã«ã‚¹ã‚­ãƒ£ãƒ³
    2) è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸æœ¬æ–‡ã‚’é †ã«è¡Œã‚¹ã‚­ãƒ£ãƒ³
    """
    out_rows: List[Dict[str, Any]] = []

    for toc in toc_lines:
        if " ::: " not in toc:
            continue
        title_raw, label = toc.split(" ::: ", 1)
        title_raw = title_raw.strip()
        label     = label.strip()

        # 1) ãƒ©ãƒ™ãƒ«ä¸€è‡´ãƒšãƒ¼ã‚¸ã§æ¢ç´¢
        body = seg_index.get(label, "")
        status = "æœªæ¤œå‡º"
        matched = "-"
        found_page_num: Optional[int] = None

        if body:
            status, matched = scan_lines_for_match(title_raw, body)

        # 2) ã¾ã æœªæ¤œå‡ºãªã‚‰å…¨ãƒšãƒ¼ã‚¸ã‚’é †ã«æ¢ç´¢
        if status == "æœªæ¤œå‡º":
            for i, ptxt in enumerate(pages_text):
                stt, m = scan_lines_for_match(title_raw, ptxt)
                if stt != "æœªæ¤œå‡º":
                    status, matched = stt, m
                    found_page_num = i + 1
                    break

        out_rows.append({
            "ã‚¿ã‚¤ãƒˆãƒ«": title_raw,
            "ç›®æ¬¡ãƒ©ãƒ™ãƒ«": label,
            "æœ¬æ–‡å†…ãƒ©ãƒ™ãƒ«": label if body else "-",
            "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸": found_page_num if found_page_num is not None else "-",
            "åˆ¤å®š": status,
            "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ": matched,
        })

    return pd.DataFrame(out_rows)


df_result = check_toc_by_order(toc_lines, seg_index, pages_text)

st.subheader("ğŸ” ç…§åˆçµæœï¼ˆè¡Œãƒ™ãƒ¼ã‚¹ï¼‰")
st.dataframe(df_result, use_container_width=True)
st.caption("â€» ç›®æ¬¡ã®å„è¡Œã‚’é †ã«è¾¿ã‚Šã€ã¾ãšåŒãƒ©ãƒ™ãƒ«ã®æŠ½å‡ºãƒšãƒ¼ã‚¸ï¼ˆvalid=Trueï¼‰ã§è¡Œã‚¹ã‚­ãƒ£ãƒ³ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…¨ãƒšãƒ¼ã‚¸ã‚’è¡Œã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã™ã€‚")

# é›†è¨ˆ
summary = df_result["åˆ¤å®š"].value_counts().to_dict()
st.markdown(f"**çµæœæ¦‚è¦**: {summary}")


# =========================
# Excel(.xlsx) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šæ—¥ä»˜åŒ–ã‚’å®Œå…¨é˜²æ­¢ï¼ˆåˆ—ã‚’ã€Œæ–‡å­—åˆ—(@)ã€ã«æŒ‡å®šï¼‰
# =========================
xlsx_buf = io.BytesIO()

with pd.ExcelWriter(xlsx_buf, engine="xlsxwriter") as writer:
    sheet_name = "result"
    df_result.to_excel(writer, index=False, sheet_name=sheet_name)

    wb = writer.book
    ws = writer.sheets[sheet_name]

    text_fmt = wb.add_format({"num_format": "@"})
    header_fmt = wb.add_format({"bold": True})
    wrap_fmt = wb.add_format({"text_wrap": True})

    cols = list(df_result.columns)
    col_idx = {name: i for i, name in enumerate(cols)}

    for name in ["ç›®æ¬¡ãƒ©ãƒ™ãƒ«", "æœ¬æ–‡å†…ãƒ©ãƒ™ãƒ«"]:
        if name in col_idx:
            j = col_idx[name]
            ws.set_column(j, j, 16, text_fmt)

    if "ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ" in col_idx:
        j = col_idx["ä¸€è‡´ãƒ†ã‚­ã‚¹ãƒˆè¡Œ"]
        ws.set_column(j, j, 40, wrap_fmt)

    for name in ["ã‚¿ã‚¤ãƒˆãƒ«", "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸", "åˆ¤å®š"]:
        if name in col_idx:
            width = 28 if name == "ã‚¿ã‚¤ãƒˆãƒ«" else 10 if name == "æœ¬æ–‡å†…ãƒšãƒ¼ã‚¸" else 12
            ws.set_column(col_idx[name], col_idx[name], width)

    for j, name in enumerate(cols):
        ws.write(0, j, name, header_fmt)

    ws.freeze_panes(1, 0)
    # ws.autofilter(0, 0, len(df_result), len(cols) - 1)

st.download_button(
    "ğŸ“¥ ç…§åˆçµæœã‚’Excelã§ä¿å­˜ (.xlsx æ¨å¥¨)",
    data=xlsx_buf.getvalue(),
    file_name="toc_check_local_result.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)

if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.code(f"LABEL_TAIL_RE = {LABEL_TAIL_RE.pattern}")
    st.code(f"LABEL_LINE_RE = {LABEL_LINE_RE.pattern}")
    st.write({"segments": len(segments), "valid_segments": len(valid_segments)})
