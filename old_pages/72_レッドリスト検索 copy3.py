# -*- coding: utf-8 -*-
# pages/72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢.py
from __future__ import annotations

import io, re
from pathlib import Path
from typing import List, Iterable
import pandas as pd
import streamlit as st

# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(page_title="ğŸ” ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” 72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢")
st.caption("ãƒ•ã‚©ãƒ«ãƒ€: data/redlist/{fukushima, MOE, chiba} ã‚’èª­ã¿è¾¼ã¿ã€å’Œå/ç¨®åã‚’è¤‡æ•°èªã¾ã¨ã‚ã¦æ¨ªæ–­æ¤œç´¢ã—ã¾ã™ã€‚")

# =========================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =========================================================
from lib.redlist.loaders import load_all
DATA_ROOT = Path("data/redlist").resolve()
moe_df, fuku_df, chiba_df = load_all(DATA_ROOT)

with st.sidebar:
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    st.code(str(DATA_ROOT))
    st.markdown("**èª­ã¿è¾¼ã¿ä»¶æ•°ï¼ˆå‚è€ƒï¼‰**")
    st.write({
        "ç’°å¢ƒçœ": 0 if moe_df.empty else int(moe_df.shape[0]),
        "ç¦å³¶çœŒ": 0 if fuku_df.empty else int(fuku_df.shape[0]),
        "åƒè‘‰çœŒ": 0 if chiba_df.empty else int(chiba_df.shape[0]),
    })

# =========================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼
# =========================================================
def normalize_name(s: str) -> str:
    return "" if pd.isna(s) else str(s).replace("\u3000", " ").strip()

def parse_queries(raw: str) -> List[str]:
    """æ”¹è¡Œãƒ»ã‚«ãƒ³ãƒãƒ»ã‚¹ãƒšãƒ¼ã‚¹ãªã©ã§åŒºåˆ‡ã£ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆé †åºä¿æŒï¼‰"""
    if not raw:
        return []
    raw = raw.replace("\u3000", " ")
    parts = re.split(r"[,\s]+", raw)
    seen, uniq = set(), []
    for p in parts:
        p = normalize_name(p)
        if p and p not in seen:
            seen.add(p)
            uniq.append(p)
    return uniq

def mask_for_queries(series: pd.Series, qs: Iterable[str], exact: bool) -> pd.Series:
    s_norm = series.fillna("").map(normalize_name)
    if exact:
        return s_norm.isin(list(qs))
    mask = pd.Series(False, index=series.index)
    s_low = s_norm.str.lower()
    for q in qs:
        ql = q.lower()
        mask = mask | s_low.str.contains(re.escape(ql), na=False)
    return mask

def ensure_line_numbers(hit: pd.DataFrame) -> pd.Series:
    """ãƒ­ãƒ¼ãƒ€ãƒ¼ã§è¡Œç•ªå·ãŒã‚ã‚Œã°ä½¿ç”¨ã€‚ç„¡ã‘ã‚Œã°cumcountã§è£œå®Œ (+2: è¦‹å‡ºã—1è¡Œæƒ³å®š)"""
    if "è¡Œç•ªå·" in hit.columns:
        try:
            return pd.to_numeric(hit["è¡Œç•ªå·"], errors="coerce").fillna("").astype("Int64")
        except Exception:
            pass
    if "ãƒ•ã‚¡ã‚¤ãƒ«å" in hit.columns and "ã‚·ãƒ¼ãƒˆå" in hit.columns:
        seq = hit.groupby(["ãƒ•ã‚¡ã‚¤ãƒ«å", "ã‚·ãƒ¼ãƒˆå"], sort=False).cumcount()
    elif "ãƒ•ã‚¡ã‚¤ãƒ«å" in hit.columns:
        seq = hit.groupby(["ãƒ•ã‚¡ã‚¤ãƒ«å"], sort=False).cumcount()
    else:
        seq = pd.Series(range(len(hit)), index=hit.index)
    return (seq + 2).astype(int)

# å‡ºåŠ›æ§‹æˆï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ï¼‹å…ƒã‚«ãƒ†ã‚´ãƒªã‚‚å‡ºã™ï¼‰
def build_output(hit: pd.DataFrame, source: str, name_col: str) -> pd.DataFrame:
    hit = hit.copy()
    out = pd.DataFrame(index=hit.index)
    out["å’Œå"] = hit[name_col].astype(str).map(normalize_name)
    out["ã‚½ãƒ¼ã‚¹"] = source

    # ãƒ¡ã‚¿
    if "ãƒ•ã‚¡ã‚¤ãƒ«å" in hit.columns:
        out["ãƒ•ã‚¡ã‚¤ãƒ«å"] = hit["ãƒ•ã‚¡ã‚¤ãƒ«å"].astype(str).map(normalize_name)
    if "ã‚·ãƒ¼ãƒˆå" in hit.columns:
        out["ã‚·ãƒ¼ãƒˆå"] = hit["ã‚·ãƒ¼ãƒˆå"].astype(str).map(normalize_name)
    out["è¡Œç•ªå·"] = ensure_line_numbers(hit)
    if "å­¦å" in hit.columns:
        out["å­¦å"] = hit["å­¦å"].astype(str).map(normalize_name)

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å· & å…ƒã‚«ãƒ†ã‚´ãƒª
    if source == "ç’°å¢ƒçœ":
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = hit.get("ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "")
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = hit.get("ã‚«ãƒ†ã‚´ãƒªãƒ¼", "")
    elif source == "ç¦å³¶çœŒ":
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = hit.get("ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "")
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = hit.get("ç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼", "")
    elif source == "åƒè‘‰çœŒ":
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = hit.get("åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "")
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = hit.get("ã‚«ãƒ†ã‚´ãƒªãƒ¼", "")
    else:
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = ""
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = ""

    # åˆ—é †
    first = ["å’Œå", "ã‚½ãƒ¼ã‚¹", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "å…ƒã‚«ãƒ†ã‚´ãƒª", "ãƒ•ã‚¡ã‚¤ãƒ«å", "è¡Œç•ªå·", "ã‚·ãƒ¼ãƒˆå", "å­¦å"]
    rest = [c for c in out.columns if c not in first]
    out = out[first + rest]
    return out.reset_index(drop=True)

# =========================================================
# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
# =========================================================
st.subheader("ğŸ§­ å’Œå/ç¨®å æ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰")
with st.form("search_form"):
    col_q1, col_q2 = st.columns([3, 1])
    with col_q1:
        multi_query_raw = st.text_area(
            "å’Œåï¼ˆç’°å¢ƒçœ/ç¦å³¶ï¼‰ãƒ»ç¨®åï¼ˆåƒè‘‰ï¼‰ã‚’è¤‡æ•°å…¥åŠ›å¯èƒ½",
            height=120,
            placeholder="ä¾‹ï¼‰\nãƒ‹ãƒ›ãƒ³ã‚¶ãƒ«\nãƒ¤ãƒãƒ, ãƒˆã‚­\nãƒ¤ãƒ³ãƒãƒ«ã‚¯ã‚¤ãƒŠ",
        )
    with col_q2:
        mode_exact = st.toggle("å®Œå…¨ä¸€è‡´", True, help="OFFã§éƒ¨åˆ†ä¸€è‡´")
    do_search = st.form_submit_button("ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œ")

queries = parse_queries(multi_query_raw) if do_search else []
if do_search and queries:
    st.caption(f"ğŸ” æ¤œç´¢èªï¼š{', '.join(queries)}ï¼ˆ{len(queries)}èªï¼‰")

# =========================================================
# æ¤œç´¢ â†’ ä¸¦ã¹æ›¿ãˆï¼ˆå…¥åŠ›é †ï¼‰ â†’ é€£ç¶šé‡è¤‡ã®ç©ºç™½åŒ– â†’ è¡¨ç¤º & Excel
# =========================================================
if do_search and queries:
    parts: List[pd.DataFrame] = []

    with st.spinner("æ¤œç´¢ä¸­â€¦"):
        # ç’°å¢ƒçœï¼ˆå’Œåï¼‰
        if not moe_df.empty and "å’Œå" in moe_df.columns:
            m = mask_for_queries(moe_df["å’Œå"], queries, mode_exact)
            hit = moe_df.loc[m]
            if not hit.empty:
                parts.append(build_output(hit, "ç’°å¢ƒçœ", "å’Œå"))

        # ç¦å³¶çœŒï¼ˆå’Œåï¼‰
        if not fuku_df.empty and "å’Œå" in fuku_df.columns:
            m = mask_for_queries(fuku_df["å’Œå"], queries, mode_exact)
            hit = fuku_df.loc[m]
            if not hit.empty:
                parts.append(build_output(hit, "ç¦å³¶çœŒ", "å’Œå"))

        # åƒè‘‰çœŒï¼ˆç¨®åï¼‰
        if not chiba_df.empty and "ç¨®å" in chiba_df.columns:
            m = mask_for_queries(chiba_df["ç¨®å"], queries, mode_exact)
            hit = chiba_df.loc[m]
            if not hit.empty:
                parts.append(build_output(hit, "åƒè‘‰çœŒ", "ç¨®å"))

    results_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()

    if results_df.empty:
        st.warning("ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        # --- å…¥åŠ›é †ï¼ˆqueriesï¼‰ã§æ•´åˆ— ---
        order_map = {q: i for i, q in enumerate(queries)}
        results_df["__name_norm"] = results_df["å’Œå"].astype(str).map(normalize_name)
        results_df["__ord"] = results_df["__name_norm"].map(order_map).fillna(len(queries) + 1)
        results_df = results_df.sort_values(["__ord"], kind="stable").drop(columns=["__ord", "__name_norm"])

        # --- åŒã˜å’ŒåãŒé€£ç¶šã—ãŸã‚‰2è¡Œç›®ä»¥é™ã‚’ç©ºç™½åŒ– ---
        dup_mask = results_df["å’Œå"].eq(results_df["å’Œå"].shift())
        results_df.loc[dup_mask, "å’Œå"] = ""

        # è¡¨ç¤ºï¼ˆåˆ—é †ã®ä¿è¨¼ï¼‰
        base = ["å’Œå", "ã‚½ãƒ¼ã‚¹", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "å…ƒã‚«ãƒ†ã‚´ãƒª", "ãƒ•ã‚¡ã‚¤ãƒ«å", "è¡Œç•ªå·", "ã‚·ãƒ¼ãƒˆå", "å­¦å"]
        results_df = results_df[[c for c in base if c in results_df.columns]]
        st.dataframe(results_df, use_container_width=True)

        # Excelå‡ºåŠ›ï¼ˆç©ºç™½åŒ–å¾Œã®å†…å®¹ã‚’å‡ºåŠ›ï¼‰
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
            results_df.to_excel(w, index=False, sheet_name="Search_Results")

        st.download_button(
            "ğŸ“¥ æ¤œç´¢çµæœã‚’Excelã§ä¿å­˜",
            buf.getvalue(),
            file_name="redlist_search_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
else:
    st.info("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ã€ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

# =========================================================
# å‚™è€ƒ
# =========================================================
with st.expander("â„¹ï¸ å‚™è€ƒ", expanded=False):
    st.markdown("""
- **ä¸¦ã¹æ›¿ãˆ**ï¼šå…¥åŠ›ã—ãŸæ¤œç´¢èªã®é †ç•ªã«çµæœã‚’ä¸¦ã¹ã¦ã„ã¾ã™ã€‚  
- **é€£ç¶šé‡è¤‡ã®ç©ºç™½åŒ–**ï¼šåŒã˜â€œå’Œåâ€ãŒé€£ç¶šã™ã‚‹å ´åˆã€2è¡Œç›®ä»¥é™ã®â€œå’Œåâ€ã¯ç©ºç™½è¡¨ç¤ºã—ã¾ã™ï¼ˆExcelã«ã‚‚åæ˜ ï¼‰ã€‚  
- **å…ƒã‚«ãƒ†ã‚´ãƒª**ã¯å„ã‚½ãƒ¼ã‚¹ã®åŸæ–‡ï¼šç’°å¢ƒçœ=ã€Œã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ï¼ç¦å³¶çœŒ=ã€Œç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ï¼åƒè‘‰çœŒ=ã€Œã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ã€‚  
- **è¡Œç•ªå·**ã¯åŸå‰‡ãƒ­ãƒ¼ãƒ€ãƒ¼ä»˜ä¸å€¤ã€ç„¡ã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆï¼‹ã‚·ãƒ¼ãƒˆåï¼‰å˜ä½ã®ç™»å ´é †+2ã§è£œå®Œã€‚  
    """)
