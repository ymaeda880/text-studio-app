# -*- coding: utf-8 -*-
# pages/21_æ–‡ç« ä½œæˆ.py
from __future__ import annotations

import os
import json
from typing import Optional, Dict

import streamlit as st
import docx
from openai import OpenAI

from config.config import has_gemini_api_key, DEFAULT_USDJPY, estimate_tokens_from_text
from lib.gemini_responder import GeminiResponder
from lib.costs_new import estimate_chat_cost, ChatUsage


# ============================================================
# Page
# ============================================================
st.set_page_config(
    page_title="ğŸ“ æ–‡ç« ä½œæˆï¼ˆGPT / Geminiï¼‰",
    page_icon="ğŸ“",
    layout="wide",
)
st.title("ğŸ“ æ–‡ç« ä½œæˆï¼ˆGPT / Geminiï¼‰")

st.caption(
    "å…ƒã¨ãªã‚‹æ–‡ç« ï¼ˆWord / txt / json / md / è²¼ã‚Šä»˜ã‘ï¼‰ï¼‹ ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆãƒ¡ãƒ¼ãƒ«/å ±å‘Šæ›¸ãªã©ï¼‰ï¼‹ è¿½åŠ æŒ‡ç¤ºã‚’ã¤ãªã„ã§ã€æ–‡ç« ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
)


# ============================================================
# Sidebar: model / cost settings
# ============================================================
with st.sidebar:
    st.header("è¨­å®š")

    # --- ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆGPT / Geminiï¼‰ ---
    OPENAI_MODELS = ["gpt-5-mini", "gpt-5-nano"]
    GEMINI_MODELS = ["gemini-2.0-flash"]

    model_options = list(OPENAI_MODELS)
    if has_gemini_api_key():
        model_options += GEMINI_MODELS

    chat_model = st.radio(
        "ãƒ¢ãƒ‡ãƒ«",
        model_options,
        index=0,
        help="Gemini ã¯ API ã‚­ãƒ¼è¨­å®šæ™‚ã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚",
    )

    max_output_tokens = st.number_input(
        "æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¸Šé™ï¼‰",
        min_value=256,
        max_value=20000,
        value=4000,
        step=256,
        help="å‡ºåŠ›ã®â€œä¸Šé™â€ã§ã™ã€‚å¤§ããã™ã‚‹ã¨é•·æ–‡ãŒå‡ºã›ã¾ã™ãŒã€æ–™é‡‘ã¨æ™‚é–“ã‚‚å¢—ãˆã‚„ã™ããªã‚Šã¾ã™ã€‚",
    )

    st.caption(f"ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼ˆæ¦‚ç®—ï¼‰: {DEFAULT_USDJPY:.2f} JPY/USD")


# ============================================================
# API clients (OpenAI / Gemini)
# ============================================================
def get_openai_client() -> OpenAI:
    api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
    if not api_key:
        raise RuntimeError(
            "OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`.streamlit/secrets.toml` ã« OPENAI_API_KEY ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
        )
    return OpenAI(api_key=api_key)


def is_gemini_model(m: str) -> bool:
    return m.startswith("gemini-")


# ============================================================
# Templates
# ============================================================
TEMPLATES: Dict[str, str] = {
    "ãƒ¡ãƒ¼ãƒ«æ–‡ï¼ˆãƒ“ã‚¸ãƒã‚¹ï¼‰": """ã‚ãªãŸã¯ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ã®ä½œæˆã«é•·ã‘ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä¸å¯§ã§ç°¡æ½”ã€èª¤è§£ãŒç”Ÿã˜ã«ãã„ãƒ¡ãƒ¼ãƒ«æ–‡ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ»ä»¶åã‚‚ä½œã‚‹
ãƒ»æ•¬èªã¯éä¸è¶³ãªã
ãƒ»ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã†
ãƒ»å¿…è¦ãªã‚‰ã€Œå¿µã®ãŸã‚ã€ã€Œå·®ã—æ”¯ãˆãªã‘ã‚Œã°ã€ç­‰ã®ã‚¯ãƒƒã‚·ãƒ§ãƒ³è¨€è‘‰ã‚’å…¥ã‚Œã‚‹
""",
    "ãƒ¡ãƒ¼ãƒ«æ–‡ï¼ˆåŒåƒšï¼‰": """ã‚ãªãŸã¯ç¤¾å†…å‘ã‘ã®é€£çµ¡æ–‡ï¼ˆåŒåƒšå®›ã¦ï¼‰ã®ä½œæˆã«é•·ã‘ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ç •ã‘ã™ããšã€ã—ã‹ã—å …ã™ããªã„ãƒˆãƒ¼ãƒ³ã§ã€æ—¥æœ¬èªã®ãƒ¡ãƒ¼ãƒ«/ãƒãƒ£ãƒƒãƒˆæ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ»è¦ç‚¹â†’è©³ç´°â†’ä¾é ¼ï¼ˆã‚ã‚Œã°ï¼‰ã®é †
ãƒ»èª­ã¿ã‚„ã™ã•é‡è¦–ï¼ˆçŸ­ã„æ®µè½ã€ç®‡æ¡æ›¸ãï¼‰
""",
    "å ±å‘Šæ›¸(ä¸€èˆ¬ï¼‰": """ã‚ãªãŸã¯å ±å‘Šæ›¸ã®ä½œæˆã«é•·ã‘ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®å…ƒã¨ãªã‚‹æ–‡ç« ã‚„ç´ æã‚’ã¾ã¨ã‚ã‚‹å½¢ã§ï¼Œå ±å‘Šæ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ»ç®‡æ¡æ›¸ãã§ã¯ãªãï¼Œæ›¸ãä¸‹ã—æ–‡ã§æ›¸ã„ã¦ãã ã•ã„ï¼
ãƒ»è‡ªç„¶ãªæµã‚Œã®ä¸€é€£ã®æ›¸ãä¸‹ã—æ–‡ã§æ›¸ã„ã¦ãã ã•ã„ï¼
""",
    "å ±å‘Šæ›¸": """ã‚ãªãŸã¯å ±å‘Šæ›¸ã®ä½œæˆã«é•·ã‘ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®å…ƒã¨ãªã‚‹æ–‡ç« ã‚„ç´ æã‚’è¸ã¾ãˆã¦ã€æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãã€æ§‹é€ åŒ–ã•ã‚ŒãŸå ±å‘Šæ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ»çµè«–ï¼ˆè¦ç´„ï¼‰â†’èƒŒæ™¯â†’è¦³å¯Ÿ/äº‹å®Ÿâ†’åˆ†æâ†’ææ¡ˆ/æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
ãƒ»å¿…è¦ãªã‚‰è¦‹å‡ºã—ã‚’ä»˜ã‘ã‚‹
ãƒ»ä¸»è¦³ã¨äº‹å®Ÿã‚’åˆ†ã‘ã‚‹
""",
    "è­°äº‹ãƒ¡ãƒ¢": """ã‚ãªãŸã¯è­°äº‹ãƒ¡ãƒ¢ã®ä½œæˆã«é•·ã‘ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
å…ƒæ–‡ç« ã‚’è¸ã¾ãˆã€æ„æ€æ±ºå®šãƒ»å®¿é¡Œãƒ»è«–ç‚¹ãŒåˆ†ã‹ã‚‹å½¢ã§ã€æ—¥æœ¬èªã§è­°äº‹ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ»æ±ºå®šäº‹é …
ãƒ»æœªæ±ºäº‹é …/è«–ç‚¹
ãƒ»ToDoï¼ˆæ‹…å½“/æœŸé™ãŒåˆ†ã‹ã‚‹ãªã‚‰æ˜è¨˜ï¼‰
""",
    "ãƒ—ãƒ¬ã‚¼ãƒ³ç”¨ã‚¹ãƒ©ã‚¤ãƒ‰": """ã‚ãªãŸã¯ã‚¹ãƒ©ã‚¤ãƒ‰ã®ä½œæˆã«é•·ã‘ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®å…ƒã¨ãªã‚‹æ–‡ç« ã‚’è¸ã¾ãˆã€æ—¥æœ¬èªã§ãƒ—ãƒ¬ã‚¼ãƒ³ç”¨ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ»ãƒ‘ãƒ¯ãƒ¼ãƒã‚¤ãƒ³ãƒˆã«ã‚³ãƒ”ãƒšã§ãã‚‹å½¢å¼ã§å‡ºã—ã¦ãã ã•ã„ï¼
""",
}


def build_final_prompt(
    *,
    template_name: str,
    base_text_kind: str,
    base_text: str,
    additional_instruction: str,
    max_chars: int,
) -> str:
    tmpl = TEMPLATES.get(template_name, "")
    used_text = (base_text or "")[:max_chars]

    add = (additional_instruction or "").strip()
    add_block = f"\n\nã€è¿½åŠ ã®æŒ‡ç¤ºï¼ˆä»»æ„ï¼‰ã€‘\n{add}\n" if add else ""

    return f"""{tmpl.strip()}

ã‚ãªãŸã¯æ—¥æœ¬èªã§æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚æ¬¡ã®ç´ æã‚’è¸ã¾ãˆã¦ã€æŒ‡å®šã®ç›®çš„ã«åˆã†æœ€çµ‚æ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
é‡è¦: ç´ æã«ãªã„äº‹å®Ÿã¯æé€ ã—ãªã„ã§ãã ã•ã„ã€‚ä¸æ˜ãªç‚¹ã¯ã€æ–­å®šã›ãšã«ã€Œä¸æ˜ã€ã¨ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã¨ãªã‚‹æ–‡ç« ï¼ˆ{base_text_kind} / å…ˆé ­ã€œæœ€å¤§{max_chars}æ–‡å­—ï¼‰ã€‘
{used_text}{add_block}

ã€å‡ºåŠ›è¦ä»¶ã€‘
ãƒ»èª­ã¿ã‚„ã™ã„æ®µè½æ§‹æˆ
ãƒ»å†—é•·ã•ã‚’é¿ã‘ã‚‹
ãƒ»å¿…è¦ãªã‚‰ç®‡æ¡æ›¸ã
""".strip()


# ============================================================
# Input source (TEXT FIRST)
# ============================================================
st.subheader("1ï¸âƒ£ å…ƒã¨ãªã‚‹æ–‡ç« ã®å…¥åŠ›")

# å…ˆã«ã€Œãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘ã€ã‚’å‡ºã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹ï¼ˆtabs ã®å…ˆé ­ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
tab_text, tab_file = st.tabs(["ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥è²¼ã‚Šä»˜ã‘ï¼ˆæ¨å¥¨ï¼‰", "ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"])

source_text: str = ""
source_kind: str = ""

with tab_text:
    pasted = st.text_area(
        "å…ƒã¨ãªã‚‹æ–‡ç« ã‚’è²¼ã‚Šä»˜ã‘",
        height=260,
        placeholder="ã“ã“ã«å…ƒã¨ãªã‚‹æ–‡ç« ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚ï¼ˆãƒ¡ãƒ¼ãƒ«ä¸‹æ›¸ãã€ãƒ¡ãƒ¢ã€Wordã‹ã‚‰ã‚³ãƒ”ãƒšç­‰ï¼‰",
    )
    if pasted.strip():
        source_text = pasted
        source_kind = "è²¼ã‚Šä»˜ã‘ãƒ†ã‚­ã‚¹ãƒˆ"
        st.info(f"ğŸ“Œ å…¥åŠ›æ¸ˆã¿: {source_kind}ï¼ˆç´„ {len(source_text)} æ–‡å­—ï¼‰")

with tab_file:
    uploaded = st.file_uploader(
        "Word / ãƒ†ã‚­ã‚¹ãƒˆ / JSON / Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["docx", "txt", "json", "md"],
    )

    if uploaded is not None:
        file_name = uploaded.name
        ext = file_name.lower().rsplit(".", 1)[-1]

        try:
            if ext == "docx":
                doc = docx.Document(uploaded)
                file_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                file_kind = "Word(.docx)"
            elif ext in ("txt", "md"):
                raw = uploaded.read()
                file_text = raw.decode("utf-8", errors="ignore")
                file_kind = f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.{ext})"
            elif ext == "json":
                raw = uploaded.read()
                obj = json.loads(raw.decode("utf-8", errors="ignore"))
                file_text = json.dumps(obj, ensure_ascii=False, indent=2)
                file_kind = "JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.jsonï¼‰"
            else:
                file_text = ""
                file_kind = ""
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            file_text = ""
            file_kind = ""

        # ã€Œè²¼ã‚Šä»˜ã‘ã€ãŒç©ºã®ã¨ãã ã‘ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¡ç”¨ï¼ˆè²¼ã‚Šä»˜ã‘å„ªå…ˆï¼‰
        if file_text and not source_text.strip():
            source_text = file_text
            source_kind = file_kind
            st.success(f"âœ… {source_kind} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆç´„ {len(source_text)} æ–‡å­—ï¼‰")

        if file_text:
            with st.expander("èª­ã¿è¾¼ã‚“ã æœ¬æ–‡ï¼ˆå…ˆé ­éƒ¨åˆ†ã‚’ç¢ºèªï¼‰", expanded=False):
                preview = file_text[:1000]
                if len(file_text) > 1000:
                    preview += "\nâ€¦ï¼ˆçœç•¥ï¼‰"
                st.code(preview, language="text")

if not source_text.strip():
    st.warning("ã¾ã å…¥åŠ›ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è²¼ã‚Šä»˜ã‘ã‚‹ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")


# ============================================================
# Generation controls
# ============================================================
st.subheader("2ï¸âƒ£ æ–‡ç« ç”Ÿæˆ")

col_a, col_b = st.columns([1, 2], vertical_alignment="top")
with col_a:
    template_name = st.radio(
        "ãƒ†ãƒ³ãƒ—ãƒ¬",
        list(TEMPLATES.keys()),
        index=0,
        help="ç”¨é€”ã«åˆã‚ã›ãŸâ€œå›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆâ€ã‚’é¸ã³ã¾ã™ã€‚",
    )
with col_b:
    additional_instruction = st.text_area(
        "è¿½åŠ ã®æŒ‡ç¤ºï¼ˆä»»æ„ï¼‰",
        height=140,
        placeholder="ä¾‹ï¼šã€ã‚‚ã†å°‘ã—çŸ­ãã€ã€ç®‡æ¡æ›¸ãã‚’å¤šã‚ã«ã€ã€çµã³ã«æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å…¥ã‚Œã¦ã€ãªã©",
    )

run = st.button("ğŸ“ æ–‡ç« ç”Ÿæˆ", type="primary")


# ============================================================
# Execute
# ============================================================
if run:
    if not source_text.strip():
        st.error("å…ˆã«å…ƒã¨ãªã‚‹æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    max_chars = 15000
    final_prompt = build_final_prompt(
        template_name=template_name,
        base_text_kind=source_kind or "ä¸æ˜",
        base_text=source_text,
        additional_instruction=additional_instruction,
        max_chars=max_chars,
    )

    answer = ""
    used_gemini = is_gemini_model(chat_model)

    # usageï¼ˆå–ã‚Œãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§æ¨å®š fallbackï¼‰
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    note = ""

    with st.spinner("AIãŒæ–‡ç« ã‚’ç”Ÿæˆä¸­..."):
        if used_gemini:
            if not has_gemini_api_key():
                st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Gemini ã‚’ä½¿ã†ã«ã¯ã‚­ãƒ¼è¨­å®šãŒå¿…è¦ã§ã™ã€‚")
                st.stop()

            responder = GeminiResponder()
            result = responder.complete(
                model=chat_model,
                system_instruction="ã‚ãªãŸã¯ä¸å¯§ãªæ—¥æœ¬èªã§æ–‡ç« ã‚’ä½œæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
                user_content=final_prompt,
                max_output_tokens=int(max_output_tokens),
            )
            answer = (result.text or "").strip()

            # Gemini ã¯ usage ãŒå–ã‚Œãªã„ã“ã¨ãŒå¤šã„ã®ã§æ¨å®š
            input_tokens = int(estimate_tokens_from_text(final_prompt))
            output_tokens = int(estimate_tokens_from_text(answer))
            note = "ï¼ˆGemini: ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼‰"

        else:
            # --- OpenAI (Responses API) ---
            try:
                client = get_openai_client()
            except Exception as e:
                st.error(str(e))
                st.stop()

            # Responses API ã«çµ±ä¸€
            resp = client.responses.create(
                model=chat_model,
                input=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ä¸å¯§ãªæ—¥æœ¬èªã§æ–‡ç« ã‚’ä½œæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": final_prompt},
                ],
                max_output_tokens=int(max_output_tokens),
            )

            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆSDKãƒãƒ¼ã‚¸ãƒ§ãƒ³å·®ã«å¼·ã‚ï¼‰
            answer = ""
            try:
                # ã¾ãšã¯ä¾¿åˆ©ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
                if hasattr(resp, "output_text") and resp.output_text:
                    answer = str(resp.output_text).strip()
            except Exception:
                pass

            if not answer:
                # æœ€å¾Œã®æ‰‹æ®µ: output ã‚’èµ°æŸ»
                try:
                    # resp.output ã¯ list ã®ã¯ãš
                    for item in getattr(resp, "output", []) or []:
                        for c in getattr(item, "content", []) or []:
                            if getattr(c, "type", None) in ("output_text", "text"):
                                answer += getattr(c, "text", "") or ""
                    answer = answer.strip()
                except Exception:
                    answer = ""

            # usageï¼ˆå–ã‚Œã‚‹ãªã‚‰æ¡ç”¨ï¼‰
            try:
                u = getattr(resp, "usage", None)
                if u is not None:
                    input_tokens = int(getattr(u, "input_tokens", 0) or 0)
                    output_tokens = int(getattr(u, "output_tokens", 0) or 0)
            except Exception:
                input_tokens = None
                output_tokens = None

            # å–ã‚Œãªã‘ã‚Œã°æ¨å®š
            if not input_tokens:
                input_tokens = int(estimate_tokens_from_text(final_prompt))
                note = "ï¼ˆOpenAI: ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼‰"
            if not output_tokens:
                output_tokens = int(estimate_tokens_from_text(answer))
                note = "ï¼ˆOpenAI: ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼‰"

    # --- Answer ---
    st.markdown("### âœ… ç”Ÿæˆçµæœ")
    st.write(answer)

    # --- Cost ---
    try:
        cost = estimate_chat_cost(
            chat_model,
            ChatUsage(input_tokens=int(input_tokens or 0), output_tokens=int(output_tokens or 0)),
            rate=DEFAULT_USDJPY,
        )
        st.markdown("### ğŸ’° æ–™é‡‘ï¼ˆæ¦‚ç®—ï¼‰")
        st.write(f"- ãƒ¢ãƒ‡ãƒ«: **{chat_model}**")
        st.write(f"- ãƒ†ãƒ³ãƒ—ãƒ¬: **{template_name}**")
        st.write(f"- Input tokens: {int(input_tokens or 0):,}")
        st.write(f"- Output tokens: {int(output_tokens or 0):,}")
        st.info(f"ğŸ“Š æ¦‚ç®—: **Â¥{cost['jpy']:,.2f}**ï¼ˆ${cost['usd']:.6f}ï¼‰{note}")
    except Exception as e:
        st.warning(f"æ–™é‡‘è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    with st.expander("ğŸ”§ å®Ÿéš›ã«é€ã£ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç¢ºèªç”¨ï¼‰", expanded=False):
        st.code(final_prompt, language="text")
