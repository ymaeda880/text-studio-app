# -*- coding: utf-8 -*-
# pages/75_åè©ãƒªã‚¹ãƒˆ.py
#
# PDF ã¾ãŸã¯ Word(.docx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦
# ãƒ»åè©æŠ½å‡ºï¼ˆJanome / MeCab ã‚’ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§åˆ‡æ›¿ï¼‰
# ãƒ»é€£ç¶šåè©ã®è¤‡åˆèªåŒ–ï¼ˆç”Ÿç‰© + å¤šæ§˜æ€§ â†’ ç”Ÿç‰©å¤šæ§˜æ€§ï¼‰
# ãƒ»æ•°å­—ã ã‘ã®èªã‚’æ’é™¤
# ãƒ»ã‚«ã‚¿ã‚«ãƒŠã®ã¿èªã‚’è¤‡åˆåŒ–å‰ã«åˆ¥æŠ½å‡º
# ãƒ»åè© / ã‚«ã‚¿ã‚«ãƒŠèª ã®é »åº¦é›†è¨ˆ
# ãƒ»CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆPDF / Word ã§ prefix åŒºåˆ¥ï¼‰
# ãƒ»æ—¥æœ¬èªå¯¾å¿œã®ç°¡æ˜“ã‚°ãƒ©ãƒ•è¡¨ç¤º

from __future__ import annotations

import io
import os
import re
from collections import Counter
from typing import List, Tuple

import pandas as pd
import streamlit as st

# ============== ãƒšãƒ¼ã‚¸è¨­å®š ==============
st.set_page_config(
    page_title="ğŸ“„ PDF/Word åè©ãƒªã‚¹ãƒˆï¼ˆå½¢æ…‹ç´ è§£æï¼‰",
    page_icon="ğŸ“„",
    layout="wide",
)
st.title("ğŸ“„ 53_ PDF/Word åè©ãƒªã‚¹ãƒˆï¼ˆå½¢æ…‹ç´ è§£æï¼‰")
st.caption("PDF ã¾ãŸã¯ Word(.docx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€åè©ã‚’æŠ½å‡ºãƒ»é›†è¨ˆã—ã¾ã™ã€‚")

# ============== å½¢æ…‹ç´ ã‚¨ãƒ³ã‚¸ãƒ³ã®èª­ã¿è¾¼ã¿ ==============

# --- Janome ---
try:
    from janome.tokenizer import Tokenizer

    tokenizer = Tokenizer()
    HAS_JANOME = True
except Exception:
    HAS_JANOME = False

# --- MeCab ---
try:
    import MeCab  # pip install mecab-python3

    HAS_MECAB = True
except Exception:
    HAS_MECAB = False

if not HAS_JANOME and not HAS_MECAB:
    st.error("Janome / MeCab ã®ã©ã¡ã‚‰ã‚‚åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚janome ã¾ãŸã¯ mecab-python3 ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ============== PDF èª­ã¿è¾¼ã¿ ==============
try:
    import fitz  # PyMuPDF

    _HAS_FITZ = True
except Exception:
    _HAS_FITZ = False

try:
    import pdfplumber

    _HAS_PDFPLUMBER = True
except Exception:
    _HAS_PDFPLUMBER = False

# ============== Word èª­ã¿è¾¼ã¿ ==============
try:
    from docx import Document

    _HAS_DOCX = True
except Exception:
    _HAS_DOCX = False

# ============== ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆè¨­å®šï¼‰ ==============
with st.sidebar:
    st.subheader("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    # åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ³ã‚¸ãƒ³ã ã‘ã‚’å€™è£œã«ã™ã‚‹
    analyzer_options = []
    if HAS_MECAB:
        analyzer_options.append("MeCab")
    if HAS_JANOME:
        analyzer_options.append("Janome")
  

    analyzer_choice = st.radio(
        "å½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³",
        analyzer_options,
        index=0,
        help="Janome ã¨ MeCab ã®ã©ã¡ã‚‰ã§åè©æŠ½å‡ºã‚’è¡Œã†ã‹é¸æŠã—ã¾ã™ã€‚",
    )

    mecab_dic_path = ""
    if "MeCab" in analyzer_options:
        mecab_dic_path = st.text_input(
            "MeCab è¾æ›¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆä»»æ„ï¼‰",
            value="",
            help="NEologd ãªã©ã‚’ä½¿ã†å ´åˆã¯ dic ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆä¾‹: /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologdï¼‰ã€‚ç©ºæ¬„ãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¾æ›¸ã€‚",
        )

    min_len = st.number_input("æœ€å°æ–‡å­—æ•°ï¼ˆåè©ï¼‰", 1, 20, 2)
    use_base = st.checkbox("åŸå½¢ã§é›†è¨ˆï¼ˆæºã‚Œã‚’æŠ‘ãˆã‚‹ï¼‰", True)
    show_pos_detail = st.checkbox("å“è©ç´°åˆ†é¡ã‚’è¡¨ç¤º", False)

    st.markdown("---")
    stopwords_raw = st.text_area(
        "ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šï¼‰",
        "ã“ã¨\nã‚‚ã®\nãã‚Œ\nã“ã‚Œ\nãŸã‚\nã‚ˆã†\nã¨ã“ã‚",
        height=120,
    )
    stopwords = {w.strip() for w in stopwords_raw.splitlines() if w.strip()}
    st.caption("PDF / Word ã«å¯¾å¿œã€‚åè©ãƒ»ã‚«ã‚¿ã‚«ãƒŠèªã‚’æŠ½å‡ºã—ã¾ã™ã€‚")

# ============== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ==============
uploaded = st.file_uploader(
    "ğŸ“¥ PDF ã¾ãŸã¯ Word(.docx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
    type=["pdf", "docx"],
)

run = st.button("ğŸš€ è§£æã™ã‚‹")

# ============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==============


def extract_text_from_pdf(file) -> str:
    """PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆPyMuPDF â†’ pdfplumber ã®é †ã§è©¦ã™ï¼‰"""
    data = file.read()

    if _HAS_FITZ:
        try:
            doc = fitz.open(stream=data, filetype="pdf")
            return "\n".join(page.get_text("text") for page in doc)
        except Exception:
            pass

    if _HAS_PDFPLUMBER:
        try:
            import io as _io

            with pdfplumber.open(_io.BytesIO(data)) as pdf:
                return "\n".join(page.extract_text() or "" for page in pdf.pages)
        except Exception:
            pass

    return ""


def extract_text_from_docx(file) -> str:
    """Word(.docx) ã®æ®µè½ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    if not _HAS_DOCX:
        st.error("python-docx ãŒå¿…è¦ã§ã™ã€‚`pip install python-docx` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return ""

    data = file.read()
    import io as _io

    doc = Document(_io.BytesIO(data))
    return "\n".join(p.text for p in doc.paragraphs)


def extract_nouns_janome(s: str) -> List[Tuple[str, str, str, int]]:
    """Janome ã§åè©ã¨é–‹å§‹ä½ç½®ã‚’å–å¾—"""
    res: List[Tuple[str, str, str, int]] = []
    ofs = 0
    for t in tokenizer.tokenize(s):
        surf = t.surface
        base = t.base_form if t.base_form != "*" else surf
        pos = t.part_of_speech
        start = ofs
        ofs += len(surf)
        if pos.startswith("åè©"):
            res.append((surf, base, pos, start))
    return res


def extract_nouns_mecab(s: str, tagger: "MeCab.Tagger") -> List[Tuple[str, str, str, int]]:
    """MeCab ã§åè©ã¨é–‹å§‹ä½ç½®ã‚’å–å¾—"""
    res: List[Tuple[str, str, str, int]] = []
    node = tagger.parseToNode(s)
    ofs = 0
    while node:
        surf = node.surface
        feat = node.feature.split(",") if node.feature else []
        pos = feat[0] if feat else ""
        base = feat[6] if len(feat) > 6 and feat[6] != "*" else surf

        if surf:
            start = ofs
            ofs += len(surf)
            if pos == "åè©":
                res.append((surf, base, pos, start))

        node = node.next
    return res


def merge_contiguous(tokens: List[Tuple[str, str, str, int]]) -> List[Tuple[str, str, str, int]]:
    """é€£ç¶šåè©ã‚’çµåˆï¼ˆä¾‹: ç”Ÿç‰© + å¤šæ§˜æ€§ â†’ ç”Ÿç‰©å¤šæ§˜æ€§ï¼‰"""
    if not tokens:
        return []

    merged: List[Tuple[str, str, str, int]] = []
    cur_surf, cur_base, cur_pos, cur_idx = tokens[0]
    prev_end = cur_idx + len(cur_surf)

    for surf, base, pos, idx in tokens[1:]:
        if idx == prev_end:
            cur_surf += surf
            cur_base += base
            prev_end = idx + len(surf)
        else:
            merged.append((cur_surf, cur_base, cur_pos, cur_idx))
            cur_surf, cur_base, cur_pos, cur_idx = surf, base, pos, idx
            prev_end = idx + len(surf)

    merged.append((cur_surf, cur_base, cur_pos, cur_idx))
    return merged


def normalize(s: str) -> str:
    return s.replace("\u3000", " ").strip()


def is_katakana_only(s: str) -> bool:
    """ã‚«ã‚¿ã‚«ãƒŠã®ã¿ï¼ˆé•·éŸ³ç¬¦å«ã‚€ï¼‰ã‹ã©ã†ã‹"""
    return bool(re.fullmatch(r"[ã‚¡-ãƒ¶ãƒ¼]+", s))


# ============== ãƒ¡ã‚¤ãƒ³å‡¦ç† ==============
if run:
    if uploaded is None:
        st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    name = uploaded.name.lower()
    base_name = os.path.splitext(uploaded.name)[0]

    # PDF / Word ã§ prefix ã‚’åˆ‡ã‚Šæ›¿ãˆ
    if name.endswith(".pdf"):
        prefix = "pdf"
    elif name.endswith(".docx"):
        prefix = "docx"
    else:
        prefix = "unknown"

    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    if prefix == "pdf":
        with st.spinner("PDF ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦"):
            text = extract_text_from_pdf(uploaded)
    else:
        with st.spinner("Word ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦"):
            text = extract_text_from_docx(uploaded)

    if not text.strip():
        st.error("ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆç”»åƒPDF ãªã©ã®å¯èƒ½æ€§ï¼‰ã€‚")
        st.stop()

    # -------- å½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³ã®é¸æŠ --------
    if analyzer_choice == "MeCab":
        if not HAS_MECAB:
            st.error("MeCab ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚mecab-python3 ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # --- MeCabï¼ˆNEologd å¼·åˆ¶ä½¿ç”¨ï¼‰ ---
        NEOLOGD_PATH = "/opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd"

        try:
            if not os.path.exists(NEOLOGD_PATH):
                st.error(f"NEologd è¾æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {NEOLOGD_PATH}\nã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.stop()

            mecab_args = f'-d {NEOLOGD_PATH}'
            tagger = MeCab.Tagger(mecab_args)

        except Exception as e:
            st.error(f"MeCab(NEologd) ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.stop()

        # try:
        #     mecab_args = ""
        #     if mecab_dic_path.strip():
        #         mecab_args = f'-d {mecab_dic_path.strip()}'
        #     tagger = MeCab.Tagger(mecab_args)
        # except Exception as e:
        #     st.error(f"MeCab ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        #     st.stop()

        tokens_raw = extract_nouns_mecab(text, tagger)
    else:  # Janome
        if not HAS_JANOME:
            st.error("Janome ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚janome ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        tokens_raw = extract_nouns_janome(text)

    # --- ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆè¤‡åˆå‰ã«æŠ½å‡ºï¼‰ ---
    kata_raw: list[Tuple[str, str, str, int, str]] = []
    for surf, base, pos, idx in tokens_raw:
        key = base if use_base else surf
        key = normalize(key)
        if len(key) < min_len:
            continue
        if key in stopwords:
            continue
        if key.isdigit():
            continue
        if is_katakana_only(key):
            kata_raw.append((surf, base, pos, idx, key))

    # --- åè©ã®è¤‡åˆåè©åŒ– ---
    tokens = merge_contiguous(tokens_raw)

    # --- åè©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆè¤‡åˆåè©è¾¼ã¿ï¼‰ ---
    filtered: list[Tuple[str, str, str, int, str]] = []
    for surf, base, pos, idx in tokens:
        key = base if use_base else surf
        key = normalize(key)
        if len(key) < min_len:
            continue
        if key in stopwords:
            continue
        if key.isdigit():
            continue
        filtered.append((surf, base, pos, idx, key))

    # =============================
    # åè©ä¸€è¦§ï¼ˆè¤‡åˆåè©è¾¼ã¿ï¼‰
    # =============================
    st.subheader("ğŸ“ åè©ä¸€è¦§ï¼ˆè¤‡åˆåè©è¾¼ã¿ï¼‰")

    rows_tokens = []
    for surf, base, pos, idx, key in filtered:
        pos_disp = pos if show_pos_detail else (pos.split(",")[0] if pos else "")
        rows_tokens.append((surf, base, pos_disp, idx))

    df_tokens = pd.DataFrame(rows_tokens, columns=["è¡¨å±¤å½¢", "åŸå½¢", "å“è©", "é–‹å§‹ä½ç½®"])
    st.dataframe(df_tokens, use_container_width=True, height=280)

    buf = io.StringIO()
    df_tokens.to_csv(buf, index=False, encoding="utf-8-sig")
    st.download_button(
        "ğŸ“¥ åè©ä¸€è¦§ CSV",
        buf.getvalue().encode("utf-8-sig"),
        file_name=f"{prefix}_{base_name}__åè©ä¸€è¦§.csv",
        mime="text/csv",
        use_container_width=True,
    )

    # =============================
    # ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆè¤‡åˆå‰ï¼‰
    # =============================
    st.subheader("ğŸ”¤ ã‚«ã‚¿ã‚«ãƒŠèªä¸€è¦§ï¼ˆè¤‡åˆåŒ–å‰ï¼‰")

    if kata_raw:
        rows_k = []
        for surf, base, pos, idx, key in kata_raw:
            pos_disp = pos if show_pos_detail else (pos.split(",")[0] if pos else "")
            rows_k.append((surf, base, pos_disp, idx))

        df_k = pd.DataFrame(rows_k, columns=["è¡¨å±¤å½¢", "åŸå½¢", "å“è©", "é–‹å§‹ä½ç½®"])
        st.dataframe(df_k, use_container_width=True, height=240)

        buf_k = io.StringIO()
        df_k.to_csv(buf_k, index=False, encoding="utf-8-sig")
        st.download_button(
            "ğŸ“¥ ã‚«ã‚¿ã‚«ãƒŠèªä¸€è¦§ CSV",
            buf_k.getvalue().encode("utf-8-sig"),
            file_name=f"{prefix}_{base_name}__ã‚«ã‚¿ã‚«ãƒŠä¸€è¦§.csv",
            mime="text/csv",
            use_container_width=True,
        )

        # ã‚«ã‚¿ã‚«ãƒŠé »åº¦
        counter_k = Counter(key for *_, key in kata_raw)
        df_kfreq = pd.DataFrame(counter_k.most_common(), columns=["ã‚«ã‚¿ã‚«ãƒŠèª", "é »åº¦"])
        st.dataframe(df_kfreq, use_container_width=True, height=240)

        buf_kf = io.StringIO()
        df_kfreq.to_csv(buf_kf, index=False, encoding="utf-8-sig")
        st.download_button(
            "ğŸ“¥ ã‚«ã‚¿ã‚«ãƒŠé »åº¦ CSV",
            buf_kf.getvalue().encode("utf-8-sig"),
            file_name=f"{prefix}_{base_name}__ã‚«ã‚¿ã‚«ãƒŠé »åº¦.csv",
            mime="text/csv",
            use_container_width=True,
        )
    else:
        st.info("ã‚«ã‚¿ã‚«ãƒŠèªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # =============================
    # åè©é »åº¦
    # =============================
    st.subheader("ğŸ“Š åè©é »åº¦ï¼ˆè¤‡åˆåè©è¾¼ã¿ï¼‰")

    counter = Counter(key for *_, key in filtered)
    df_freq = pd.DataFrame(counter.most_common(), columns=["åè©", "é »åº¦"])
    st.dataframe(df_freq, use_container_width=True, height=300)

    buf_f = io.StringIO()
    df_freq.to_csv(buf_f, index=False, encoding="utf-8-sig")
    st.download_button(
        "ğŸ“¥ åè©é »åº¦ CSV",
        buf_f.getvalue().encode("utf-8-sig"),
        file_name=f"{prefix}_{base_name}__åè©é »åº¦.csv",
        mime="text/csv",
        use_container_width=True,
    )

    # =============================
    # ã‚°ãƒ©ãƒ•ï¼ˆä¸Šä½20ï¼‰
    # =============================
    try:
        import matplotlib.pyplot as plt

        plt.rcParams["font.family"] = "Hiragino Sans"  # macOS ã®å ´åˆ
        plt.rcParams["axes.unicode_minus"] = False

        if len(df_freq) > 0:
            st.subheader("ğŸ“ˆ åè©é »åº¦ï¼ˆä¸Šä½20ï¼‰")

            topn = min(20, len(df_freq))
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.barh(
                df_freq["åè©"].iloc[:topn][::-1],
                df_freq["é »åº¦"].iloc[:topn][::-1],
            )
            ax.set_xlabel("é »åº¦")
            ax.set_ylabel("åè©")
            ax.set_title("åè©å‡ºç¾é »åº¦ï¼ˆä¸Šä½20ï¼‰")
            st.pyplot(fig)
    except Exception as e:
        st.info(f"ã‚°ãƒ©ãƒ•æç”»ã«å¤±æ•—: {e}")

else:
    st.info("PDF ã¾ãŸã¯ Word ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ŒğŸš€ è§£æã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
