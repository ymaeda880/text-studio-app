# -*- coding: utf-8 -*-
# pages/22_Wordæ¤œæŸ».py
from __future__ import annotations
import io, os, re, json, tempfile, subprocess, sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import pandas as pd
import streamlit as st

# ========== ä¾å­˜ï¼ˆä»»æ„ã®ã‚‚ã®ã¯å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰ ==========
try:
    import docx  # python-docx
except Exception:
    docx = None

try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

try:
    import pdfplumber
except Exception:
    pdfplumber = None

try:
    from docx2pdf import convert as docx2pdf_convert  # macOS/Windowsã®ã¿å®‰å®š
except Exception:
    docx2pdf_convert = None

try:
    import pypandoc  # pandoc ãŒå…¥ã£ã¦ã„ã‚Œã°DOCXâ†’PDFå¯èƒ½
except Exception:
    pypandoc = None

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ========== ãƒšãƒ¼ã‚¸è¨­å®š ==========
st.set_page_config(page_title="ğŸ“ Wordãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œæŸ»ï¼ˆgpt-5-miniï¼‰", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ Wordãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œæŸ»ï¼ˆgpt-5-miniï¼‰")
st.caption("DOCXã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€è¦‹å‡ºã—ãƒã‚§ãƒƒã‚¯ãƒ»è¡¨è¨˜ã‚†ã‚Œãƒ»å›³è¡¨å¼•ç”¨ã®ã‚ºãƒ¬ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã™ã€‚")

# ========== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==========
FIG_PATTERNS = [
    r"(?:å›³|Fig(?:\.|ure)?)\s*(\d+)(?:\s*[-â€“]\s*|[:ï¼š]\s*|\s+)?(.+)?",
]
TAB_PATTERNS = [
    r"(?:è¡¨|Tab(?:\.|le)?)\s*(\d+)(?:\s*[-â€“]\s*|[:ï¼š]\s*|\s+)?(.+)?",
]
REF_PATTERNS = [
    r"(?:å›³|Fig(?:\.|ure)?)\s*(\d+)",
    r"(?:è¡¨|Tab(?:\.|le)?)\s*(\d+)",
]

HEADING_STYLES_JA = {"è¦‹å‡ºã— 1","è¦‹å‡ºã— 2","è¦‹å‡ºã— 3","Heading 1","Heading 2","Heading 3"}

def safe_openai_client() -> Optional["OpenAI"]:
    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ï¼‰"""
    if OpenAI is None:
        return None
    # å…ˆã« Streamlit ã® secrets â†’ ç’°å¢ƒå¤‰æ•° ã®é †ã§
    api_key = None
    try:
        api_key = st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        return None
    return OpenAI(api_key=api_key)

def try_docx_to_pdf(input_docx: Path, out_pdf: Path) -> bool:
    """DOCXâ†’PDFå¤‰æ›ã‚’è¤‡æ•°æ‰‹æ®µã§è©¦ã¿ã‚‹ã€‚æˆåŠŸã—ãŸã‚‰True"""
    # 1) docx2pdfï¼ˆmacOS/Windowsï¼‰
    if docx2pdf_convert is not None:
        try:
            docx2pdf_convert(str(input_docx), str(out_pdf))
            return out_pdf.exists() and out_pdf.stat().st_size > 0
        except Exception:
            pass
    # 2) pandoc
    if pypandoc is not None:
        try:
            pypandoc.convert_file(str(input_docx), 'pdf', outputfile=str(out_pdf))
            return out_pdf.exists() and out_pdf.stat().st_size > 0
        except Exception:
            pass
    # 3) LibreOfficeï¼ˆsofficeï¼‰ã‚³ãƒãƒ³ãƒ‰
    try:
        cmd = ["soffice","--headless","--convert-to","pdf","--outdir",str(out_pdf.parent), str(input_docx)]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        guess = input_docx.with_suffix(".pdf")
        if guess.exists():
            guess.replace(out_pdf)
        return out_pdf.exists() and out_pdf.stat().st_size > 0
    except Exception:
        pass
    return False

def extract_paragraphs(doc: "docx.document.Document") -> List[Tuple[str, str]]:
    """(style_name, text) ã‚’æ®µè½é †ã§è¿”ã™"""
    rows = []
    for p in doc.paragraphs:
        style = getattr(getattr(p, "style", None), "name", "") or ""
        text = p.text.strip()
        if text:
            rows.append((style, text))
    return rows

def list_headings(paras: List[Tuple[str,str]]) -> List[Dict]:
    """è¦‹å‡ºã—å€™è£œã‚’æŠ½å‡º"""
    out = []
    for i, (style, txt) in enumerate(paras, start=1):
        if style in HEADING_STYLES_JA:
            out.append({"index": i, "style": style, "heading": txt})
        else:
            # ã‚¹ã‚¿ã‚¤ãƒ«ä¸æ˜ã§ã‚‚ç•ªå·ãƒ»ç« ç¯€å½¢ãªã‚‰è¦‹å‡ºã—å€™è£œ
            if re.match(r"^\d+(?:\.\d+)*\s+.+", txt):
                out.append({"index": i, "style": style or "ï¼ˆä¸æ˜ï¼‰", "heading": txt})
    return out

def regex_search_all(patterns: List[str], text: str) -> List[re.Match]:
    matches = []
    for pat in patterns:
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            matches.append(m)
    return matches

def extract_fig_tab_captions(paras: List[Tuple[str,str]]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """æ®µè½ã‹ã‚‰å›³è¡¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    fig_rows, tab_rows = [], []
    for idx, (style, txt) in enumerate(paras, start=1):
        for m in regex_search_all(FIG_PATTERNS, txt):
            num = m.group(1)
            title = (m.group(2) or "").strip()
            fig_rows.append({"para_index": idx, "number": int(num), "title": title, "raw": txt})
        for m in regex_search_all(TAB_PATTERNS, txt):
            num = m.group(1)
            title = (m.group(2) or "").strip()
            tab_rows.append({"para_index": idx, "number": int(num), "title": title, "raw": txt})
    return pd.DataFrame(fig_rows), pd.DataFrame(tab_rows)

def extract_references(paras: List[Tuple[str,str]]) -> pd.DataFrame:
    """æœ¬æ–‡ä¸­ã®å›³è¡¨å‚ç…§ï¼ˆå›³1ã€è¡¨3ã€Figure 2ã€Table 4 ãªã©ï¼‰ã‚’æŠ½å‡º"""
    rows = []
    for idx, (_, txt) in enumerate(paras, start=1):
        for pat in REF_PATTERNS:
            for m in re.finditer(pat, txt, flags=re.IGNORECASE):
                label = "å›³" if re.match(r"^(?:å›³|Fig)", m.group(0), re.IGNORECASE) else "è¡¨"
                num = int(m.group(1))
                rows.append({"para_index": idx, "label": label, "number": num, "context": txt})
    return pd.DataFrame(rows)

def detect_spelling_variants(paras: List[Tuple[str,str]]) -> Dict[str, List[str]]:
    """ç°¡æ˜“ï¼šåŒç¶´ã®å¤§å°ãƒ»å…¨åŠè§’ãƒ»ã‚«ãƒŠ/è‹±ãªã©ã®å€™è£œã‚’æ‹¾ã†ï¼ˆå¾Œæ®µã§GPTã«æ¸¡ã™ï¼‰"""
    # å˜èªã‚¹ã‚­ãƒ£ãƒ³ï¼ˆæ—¥æœ¬èªã¯é›£ã—ã„ã®ã§ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼‰
    text_all = "\n".join(t for _, t in paras)
    # åŒä¸€èªã®å¤§å°/å…¨åŠè§’/é•·éŸ³ãªã©ã‚’ç·©ãæ‹¾ã†ãŸã‚ã®ç°¡æ˜“è¦å‰‡
    tokens = re.findall(r"[A-Za-z][A-Za-z0-9\-_/]*|[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]{2,}", text_all)
    norm_map: Dict[str, set] = {}
    def norm(s: str) -> str:
        s2 = s.lower()
        s2 = s2.replace("â€","-").replace("â€”","-").replace("â€“","-").replace("ï¼","/").replace("ãƒ»","Â·")
        return s2
    for w in tokens:
        key = norm(w)
        norm_map.setdefault(key, set()).add(w)
    # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒè¤‡æ•°ã‚ã‚‹ã‚‚ã®ã ã‘è¿”ã™
    variants = {k: sorted(list(v)) for k,v in norm_map.items() if len(v) >= 2}
    return variants

def guess_pages_from_pdf(pdf_path: Path, headings: List[Dict]) -> List[Dict]:
    """PDFåŒ–ã§ããŸå ´åˆã€è¦‹å‡ºã—ãŒã©ã®ãƒšãƒ¼ã‚¸ã«ç¾ã‚Œã‚‹ã‹ã‚’æ¤œç´¢ï¼ˆç°¡æ˜“ä¸€è‡´ï¼‰"""
    if not pdfplumber:
        return headings
    try:
        with pdfplumber.open(str(pdf_path)) as pdf:
            pages_text = [p.extract_text() or "" for p in pdf.pages]
    except Exception:
        return headings
    for h in headings:
        title = re.escape(h["heading"][:40])  # å…ˆé ­40æ–‡å­—ã§ç°¡æ˜“æ¤œç´¢
        found_page = None
        for i, t in enumerate(pages_text, start=1):
            if re.search(title, t):
                found_page = i
                break
        h["page"] = found_page
    return headings

def run_gpt_check(client: OpenAI, task: str, inputs: Dict) -> str:
    """gpt-5-miniã«æŠ•ã’ã‚‹æ±ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    sys_prompt = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®å­¦è¡“ãƒ¬ãƒãƒ¼ãƒˆç·¨é›†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        "å…¥åŠ›ï¼ˆJSONï¼‰ã«åŸºã¥ãã€çŸ­ãè¦ç‚¹æ•´ç†ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯Markdownã§ã€‚"
    )
    user_msg = f"ã‚¿ã‚¹ã‚¯: {task}\nå…¥åŠ›JSON:\n```json\n{json.dumps(inputs, ensure_ascii=False, indent=2)}\n```"
    resp = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[{"role":"system","content":sys_prompt},{"role":"user","content":user_msg}],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()

def download_button_df(df: pd.DataFrame, label: str, file_name: str):
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    st.download_button(label, data=buf.getvalue().encode("utf-8-sig"), file_name=file_name, mime="text/csv")

# ========== ã‚µã‚¤ãƒ‰ãƒãƒ¼UI ==========
with st.sidebar:
    st.header("è¨­å®š")
    proc = st.selectbox("å‡¦ç†ã®ç¨®é¡", ["ï¼ˆï¼‘ï¼‰è¦‹å‡ºã—ãƒã‚§ãƒƒã‚¯", "ï¼ˆï¼’ï¼‰è¡¨è¨˜ã‚†ã‚Œ", "ï¼ˆï¼“ï¼‰å›³è¡¨â†”æœ¬æ–‡ã®ã‚ºãƒ¬"])
    option = st.radio("ã‚ªãƒ—ã‚·ãƒ§ãƒ³", ["æ¨™æº–", "å³å¯†ï¼ˆæ¤œå‡ºå¤šã‚ï¼‰", "GPTã«ä»»ã›ã‚‹"], index=0)
    st.caption("â€» GPTã«ä»»ã›ã‚‹ï¼šè¦ç‚¹è¦ç´„ãƒ»çµ±ä¸€æ¡ˆã®ç”Ÿæˆã« gpt-5-mini ã‚’ä½¿ã„ã¾ã™ã€‚")

uploaded = st.file_uploader("Wordï¼ˆ.docxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["docx"])

if not uploaded:
    st.info("ã¾ãš .docx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

if docx is None:
    st.error("python-docx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚requirements.txt ã‚’å‚ç…§ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ========== è§£æãƒ•ãƒ­ãƒ¼ ==========
with tempfile.TemporaryDirectory() as td:
    td = Path(td)
    docx_path = td / "input.docx"
    with open(docx_path, "wb") as f:
        f.write(uploaded.read())

    # DOCXèª­ã¿è¾¼ã¿
    doc = docx.Document(str(docx_path))
    paras = extract_paragraphs(doc)

    # å¯èƒ½ãªã‚‰PDFåŒ–ï¼ˆãƒšãƒ¼ã‚¸å‰²ã‚Šå½“ã¦ç”¨ï¼‰
    pdf_path = td / "input.pdf"
    got_pdf = try_docx_to_pdf(docx_path, pdf_path)

# ï¼ˆï¼‘ï¼‰è¦‹å‡ºã—ã¨ãƒšãƒ¼ã‚¸ä¸€è¦§
if proc.startswith("ï¼ˆï¼‘ï¼‰"):
    headings = list_headings(paras)
    if got_pdf:
        headings = guess_pages_from_pdf(pdf_path, headings)
    df = pd.DataFrame(headings)
    st.subheader("è¦‹å‡ºã—ä¸€è¦§")
    st.dataframe(df, use_container_width=True)
    download_button_df(df, "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè¦‹å‡ºã—ä¸€è¦§ï¼‰", "headings.csv")

    # GPTã«ã‚ˆã‚‹ã€Œè¦‹å‡ºã—ã®ä»˜ãæ–¹ã€ãƒã‚§ãƒƒã‚¯ï¼ˆéšå±¤ãƒ»é‡è¤‡ãƒ»ç²’åº¦ï¼‰
    if option == "GPTã«ä»»ã›ã‚‹":
        client = safe_openai_client()
        if client:
            inputs = {
                "headings": headings,
                "notes": "ãƒšãƒ¼ã‚¸ç•ªå·ã¯nullã®å ´åˆãŒã‚ã‚Šã¾ã™ï¼ˆPDFåŒ–ã§ããªã‹ã£ãŸç­‰ï¼‰ã€‚"
            }
            st.markdown("#### GPTã®ãƒã‚§ãƒƒã‚¯çµæœ")
            st.markdown(run_gpt_check(client, "è¦‹å‡ºã—æ§‹æˆã®é©å¦ãƒ»ä¸è¶³/é‡è¤‡ã®æŒ‡æ‘˜ãƒ»æ”¹å–„ææ¡ˆ", inputs))
        else:
            st.warning("OpenAI APIã‚­ãƒ¼ãŒæœªè¨­å®šã®ãŸã‚ã€GPTè§£æã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

# ï¼ˆï¼’ï¼‰è¡¨è¨˜ã‚†ã‚Œ
elif proc.startswith("ï¼ˆï¼’ï¼‰"):
    variants = detect_spelling_variants(paras)
    # é–¾å€¤ï¼ˆå³å¯†ãƒ¢ãƒ¼ãƒ‰ã¯çŸ­ã„èªã‚‚æ‹¾ã†ï¼‰
    min_len = 2 if option == "å³å¯†ï¼ˆæ¤œå‡ºå¤šã‚ï¼‰" else 3
    rows = []
    for k, vs in variants.items():
        if max(len(v) for v in vs) >= min_len:
            rows.append({"normalized_key": k, "variants": ", ".join(vs), "count": len(vs)})
    df = pd.DataFrame(sorted(rows, key=lambda r: (-r["count"], r["normalized_key"])))
    st.subheader("è¡¨è¨˜ã‚†ã‚Œå€™è£œï¼ˆãƒ«ãƒ¼ãƒ«æ¤œå‡ºï¼‰")
    st.dataframe(df, use_container_width=True)
    download_button_df(df, "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè¡¨è¨˜ã‚†ã‚Œå€™è£œï¼‰", "variants.csv")

    # GPTã«çµ±ä¸€æ¡ˆã‚’å‡ºã•ã›ã‚‹
    if option == "GPTã«ä»»ã›ã‚‹":
        client = safe_openai_client()
        if client and len(df):
            # ä¸Šä½ N ä»¶ã®ã¿æ¸¡ã™ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
            N = 60
            sample = df.head(N).to_dict(orient="records")
            inputs = {
                "variants_topN": sample,
                "policy_hint": "å­¦è¡“ãƒ¬ãƒãƒ¼ãƒˆã®æ—¥æœ¬èªè¡¨è¨˜ã€‚è‹±èªç”¨èªã¯å°æ–‡å­—ã®è‹±èªè¡¨è¨˜ï¼ˆä¾‹ï¼šæ±ºå®šæœ¨ï¼ˆdecision treeï¼‰ï¼‰ã‚’åŸºæœ¬ã€‚ç”¨èªã¯ä¸€è²«æ€§ã‚’å„ªå…ˆã€‚"
            }
            st.markdown("#### GPTã®çµ±ä¸€ææ¡ˆï¼ˆä¸Šä½å€™è£œï¼‰")
            st.markdown(run_gpt_check(client, "è¡¨è¨˜ã®ã‚†ã‚Œã®çµ±ä¸€ãƒªã‚¹ãƒˆï¼ˆæ¨å¥¨è¡¨è¨˜ãƒ»ç½®æ›ä¾‹ï¼‰ã‚’ç®‡æ¡æ›¸ãã§æç¤º", inputs))
        elif not client:
            st.warning("OpenAI APIã‚­ãƒ¼ãŒæœªè¨­å®šã®ãŸã‚ã€GPTè§£æã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

# ï¼ˆï¼“ï¼‰å›³è¡¨ç•ªå·ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡å¼•ç”¨ã®ã‚ºãƒ¬
else:
    figs_df, tabs_df = extract_fig_tab_captions(paras)
    refs_df = extract_references(paras)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### å›³ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³")
        st.dataframe(figs_df, use_container_width=True)
        if len(figs_df):
            download_button_df(figs_df, "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå›³ï¼‰", "figures.csv")
    with col2:
        st.markdown("#### è¡¨ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³")
        st.dataframe(tabs_df, use_container_width=True)
        if len(tabs_df):
            download_button_df(tabs_df, "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè¡¨ï¼‰", "tables.csv")

    st.markdown("#### æœ¬æ–‡å†…ã®å‚ç…§ï¼ˆå›³/è¡¨ï¼‰")
    st.dataframe(refs_df, use_container_width=True)
    if len(refs_df):
        download_button_df(refs_df, "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå‚ç…§ï¼‰", "references.csv")

    # ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
    issues = []
    # æœŸå¾…ã•ã‚Œã‚‹é€£ç•ªï¼ˆæ¬ ç•ªæ¤œå‡ºï¼‰
    def missing_numbers(df: pd.DataFrame) -> List[int]:
        if not len(df):
            return []
        nums = sorted(df["number"].unique().tolist())
        expected = list(range(1, max(nums)+1))
        return [n for n in expected if n not in nums]

    miss_fig = missing_numbers(figs_df)
    miss_tab = missing_numbers(tabs_df)

    # å‚ç…§ã¯ã‚ã‚‹ã®ã«å®šç¾©ãŒãªã„ã€ã¾ãŸã¯å®šç¾©ã¯ã‚ã‚‹ã®ã«æœ¬æ–‡ã§å‚ç…§ã•ã‚Œã¦ã„ãªã„
    ref_fig_nums = set(refs_df.loc[refs_df["label"]=="å›³","number"].tolist())
    ref_tab_nums = set(refs_df.loc[refs_df["label"]=="è¡¨","number"].tolist())
    cap_fig_nums = set(figs_df["number"].tolist())
    cap_tab_nums = set(tabs_df["number"].tolist())

    undef_fig = sorted(list(ref_fig_nums - cap_fig_nums))
    undef_tab = sorted(list(ref_tab_nums - cap_tab_nums))
    unused_fig = sorted(list(cap_fig_nums - ref_fig_nums))
    unused_tab = sorted(list(cap_tab_nums - ref_tab_nums))

    if miss_fig: issues.append(f"å›³ï¼šæ¬ ç•ª {miss_fig}")
    if miss_tab: issues.append(f"è¡¨ï¼šæ¬ ç•ª {miss_tab}")
    if undef_fig: issues.append(f"æœ¬æ–‡ã«å‚ç…§ãŒã‚ã‚‹ãŒæœªå®šç¾©ã®å›³ç•ªå·: {undef_fig}")
    if undef_tab: issues.append(f"æœ¬æ–‡ã«å‚ç…§ãŒã‚ã‚‹ãŒæœªå®šç¾©ã®è¡¨ç•ªå·: {undef_tab}")
    if unused_fig: issues.append(f"æœ¬æ–‡ã§å‚ç…§ã•ã‚Œã¦ã„ãªã„å›³ç•ªå·: {unused_fig}")
    if unused_tab: issues.append(f"æœ¬æ–‡ã§å‚ç…§ã•ã‚Œã¦ã„ãªã„è¡¨ç•ªå·: {unused_tab}")

    st.subheader("æ¤œå‡ºçµæœï¼ˆè¦ç´„ï¼‰")
    if issues:
        st.error(" / ".join(issues))
    else:
        st.success("å›³è¡¨ã¨æœ¬æ–‡å‚ç…§ã®æ•´åˆæ€§ã«å¤§ããªå•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # GPTã§ç°¡æ˜“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¤‡ãƒ»å‘½åè¦å‰‡ãƒ»èª¬æ˜æ€§ï¼‰
    if option == "GPTã«ä»»ã›ã‚‹":
        client = safe_openai_client()
        if client:
            inputs = {
                "figures": figs_df.to_dict(orient="records"),
                "tables":  tabs_df.to_dict(orient="records"),
                "refs":    refs_df.to_dict(orient="records"),
            }
            st.markdown("#### GPTã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå›³è¡¨å‘½åãƒ»å‚ç…§ã®ä»•æ–¹ï¼‰")
            st.markdown(run_gpt_check(client, "å›³è¡¨ç•ªå·ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã®ä»˜ã‘æ–¹ã¨å‚ç…§ã®ä»•æ–¹ã«é–¢ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼", inputs))
        else:
            st.warning("OpenAI APIã‚­ãƒ¼ãŒæœªè¨­å®šã®ãŸã‚ã€GPTè§£æã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
