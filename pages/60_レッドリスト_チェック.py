# -*- coding: utf-8 -*-
# pages/60_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆ_ãƒã‚§ãƒƒã‚¯.py
from __future__ import annotations

import io
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import pandas as pd
import streamlit as st

# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(page_title="ğŸ“š ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆå‚ç…§ï¼ˆç’°å¢ƒçœ/ç¦å³¶/åƒè‘‰ï¼‰", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š 32_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆå‚ç…§")
st.caption("ãƒ•ã‚©ãƒ«ãƒ€: data/redlist/{fukushima, MOE, prec, chiba} ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—â€¦")


# =========================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆlib/redlist/loaders ã‹ã‚‰ï¼‰
# =========================================================
from lib.redlist.loaders import load_all

DATA_ROOT = Path("data/redlist").resolve()

moe_dir   = DATA_ROOT / "MOE"
fuku_dir  = DATA_ROOT / "fukushima"
chiba_dir = DATA_ROOT / "chiba"

moe_df, fuku_df, chiba_df = load_all(DATA_ROOT)

# =========================================================
# ç°¡æ˜“ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ãƒ˜ãƒ«ãƒ‘
# =========================================================
def _safe_head(df: pd.DataFrame, n: int = 10) -> pd.DataFrame:
    """ç©ºã§ãªã„DataFrameã®å…ˆé ­nè¡Œã‚’è¿”ã™"""
    return df.head(n).copy() if not df.empty else df

# =========================================================
# èª­ã¿è¾¼ã¿ & ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­10è¡Œï¼‰
# =========================================================
with st.sidebar:
    st.subheader("ğŸ“ ã‚½ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€")
    st.code(str(DATA_ROOT))


st.subheader("ğŸ‘€ å…ˆé ­ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ãƒ¼ãƒˆã®æœ€åˆã®10è¡Œï¼‰")
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("**ç’°å¢ƒçœ (MOE)**")
    if moe_df.empty:
        st.info("ç’°å¢ƒçœãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(_safe_head(moe_df), use_container_width=True)

with col2:
    st.markdown("**ç¦å³¶çœŒ (fukusiima)**")
    if fuku_df.empty:
        st.info("ç¦å³¶çœŒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(_safe_head(fuku_df), use_container_width=True)

with col3:
    st.markdown("**åƒè‘‰çœŒ (chiba)**")
    if chiba_df.empty:
        st.info("åƒè‘‰çœŒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(_safe_head(chiba_df), use_container_width=True)


# =========================================================
# ğŸ“¥ å…¨ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå„ã‚½ãƒ¼ã‚¹ã‚’å€‹åˆ¥ã®xlsxã«åˆ†å‰²ï¼‰
# =========================================================
st.subheader("ğŸ“¥ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚½ãƒ¼ã‚¹åˆ¥ .xlsxï¼‰")

def _add_source(df: pd.DataFrame, name: str) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()
    out = df.copy()
    out.insert(0, "ã‚½ãƒ¼ã‚¹", name)
    return out

moe_all   = _add_source(moe_df,  "ç’°å¢ƒçœ")
fuku_all  = _add_source(fuku_df, "ç¦å³¶çœŒ")
chiba_all = _add_source(chiba_df,"åƒè‘‰çœŒ")

frames = [x for x in [moe_all, fuku_all, chiba_all] if not x.empty]
all_data = pd.concat(frames, ignore_index=True, sort=False) if frames else pd.DataFrame()

if not frames:
    st.info("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    import zipfile
    zbuf = io.BytesIO()

    with zipfile.ZipFile(zbuf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        # ç’°å¢ƒçœ
        if not moe_all.empty:
            b1 = io.BytesIO()
            with pd.ExcelWriter(b1, engine="xlsxwriter") as w:
                moe_all.to_excel(w, sheet_name="MOE", index=False)
            zf.writestr("MOE.xlsx", b1.getvalue())

        # ç¦å³¶çœŒ
        if not fuku_all.empty:
            b2 = io.BytesIO()
            with pd.ExcelWriter(b2, engine="xlsxwriter") as w:
                fuku_all.to_excel(w, sheet_name="Fukushima", index=False)
            zf.writestr("Fukushima.xlsx", b2.getvalue())

        # åƒè‘‰çœŒ
        if not chiba_all.empty:
            b3 = io.BytesIO()
            with pd.ExcelWriter(b3, engine="xlsxwriter") as w:
                chiba_all.to_excel(w, sheet_name="Chiba", index=False)
            zf.writestr("Chiba.xlsx", b3.getvalue())

        # å…¨ä»¶
        if not all_data.empty:
            b4 = io.BytesIO()
            with pd.ExcelWriter(b4, engine="xlsxwriter") as w:
                all_data.to_excel(w, sheet_name="All", index=False)
            zf.writestr("All.xlsx", b4.getvalue())

    st.download_button(
        "ğŸ“¥ ã™ã¹ã¦ã®xlsxã‚’ZIPã§ä¿å­˜ï¼ˆMOE/Fukushima/Chiba/Allï¼‰",
        data=zbuf.getvalue(),
        file_name="redlist_all_data_xlsx.zip",
        mime="application/zip",
        use_container_width=True,
    )



# =========================================================
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ãƒ¼ãƒˆé›†è¨ˆï¼ˆExcel + CSVï¼‰
# =========================================================
st.subheader("ğŸ§¾ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ãƒ¼ãƒˆé›†è¨ˆï¼ˆExcel + CSVï¼‰")

from typing import List  # å¿µã®ãŸã‚

def paths_by_type(folder: Path):
    """ãƒ•ã‚©ãƒ«ãƒ€å†…ã® Excel/CSV ã‚’åˆ†é¡ã—ã¦è¿”ã™ã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºã€‚"""
    if not folder or not folder.exists():
        return [], []
    files = [p for p in sorted(folder.glob("*")) if p.is_file()]
    excels = [p for p in files if p.suffix.lower() in (".xlsx", ".xls", ".xlsm")]
    csvs   = [p for p in files if p.suffix.lower() in (".csv",)]
    return excels, csvs

def file_sheet_stats(df: pd.DataFrame, source_name: str, folder: Path):
    """
    df: å‚ç…§ãƒ©ãƒ™ãƒ«åŒ–æ¸ˆã¿ & 'ãƒ•ã‚¡ã‚¤ãƒ«å','ã‚·ãƒ¼ãƒˆå' ã‚’æŒã¤ DFï¼ˆCSVã¯ã‚·ãƒ¼ãƒˆåãŒç©ºï¼‰
    æˆ»ã‚Šå€¤: (æ˜ç´°DF, ã‚µãƒãƒªãƒ¼DF)
    """
    excels, csvs = paths_by_type(folder)
    excel_cnt = len(excels)
    csv_cnt   = len(csvs)
    total_cnt = excel_cnt + csv_cnt

    if df.empty:
        detail = pd.DataFrame(columns=["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","ãƒ‡ãƒ¼ã‚¿æ•°"])
        summary = pd.DataFrame([{
            "ã‚½ãƒ¼ã‚¹": source_name,
            "Excelãƒ•ã‚¡ã‚¤ãƒ«æ•°": excel_cnt,
            "CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°": csv_cnt,
            "åˆè¨ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°": total_cnt,
            "ãƒ‡ãƒ¼ã‚¿ç·æ•°": 0,
        }])
        return detail, summary

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ãƒ¼ãƒˆã”ã¨ã®è¡Œæ•°
    detail = (
        df.groupby(["ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå"], dropna=False)
          .size()
          .reset_index(name="ãƒ‡ãƒ¼ã‚¿æ•°")
          .sort_values(["ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå"])
    )
    detail.insert(0, "ã‚½ãƒ¼ã‚¹", source_name)

    # ã‚µãƒãƒªãƒ¼
    summary = pd.DataFrame([{
        "ã‚½ãƒ¼ã‚¹": source_name,
        "Excelãƒ•ã‚¡ã‚¤ãƒ«æ•°": excel_cnt,
        "CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°": csv_cnt,
        "åˆè¨ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°": total_cnt,
        "ãƒ‡ãƒ¼ã‚¿ç·æ•°": int(df.shape[0]),
    }])
    return detail, summary

# --- æ˜ç´°ãƒ»ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ ---
detail_parts, summary_parts = [], []

d, s = file_sheet_stats(moe_df,  "ç’°å¢ƒçœ", moe_dir);   detail_parts.append(d); summary_parts.append(s)
d, s = file_sheet_stats(fuku_df, "ç¦å³¶çœŒ", fuku_dir);  detail_parts.append(d); summary_parts.append(s)
d, s = file_sheet_stats(chiba_df,"åƒè‘‰çœŒ", chiba_dir); detail_parts.append(d); summary_parts.append(s)

detail_all = (
    pd.concat([x for x in detail_parts if not x.empty], ignore_index=True)
    if any([not x.empty for x in detail_parts])
    else pd.DataFrame(columns=["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","ãƒ‡ãƒ¼ã‚¿æ•°"])
)

summary_all = (
    pd.concat([x for x in summary_parts if not x.empty], ignore_index=True)
    if any([not x.empty for x in summary_parts])
    else pd.DataFrame(columns=["ã‚½ãƒ¼ã‚¹","Excelãƒ•ã‚¡ã‚¤ãƒ«æ•°","CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°","åˆè¨ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°","ãƒ‡ãƒ¼ã‚¿ç·æ•°"])
)

# --- è¡¨ç¤ºï¼ˆä¸Šï¼šã‚µãƒãƒªãƒ¼ã€ä¸‹ï¼šæ˜ç´°ï¼‰ ---
st.markdown("**ğŸ“Œ ã‚µãƒãƒªãƒ¼ï¼ˆãƒ•ã‚©ãƒ«ãƒ€åˆ¥ï¼‰**")
if summary_all.empty:
    st.info("ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    st.dataframe(summary_all, use_container_width=True)

st.markdown("**ğŸ“„ æ˜ç´°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ãƒ¼ãƒˆåˆ¥ã®è¡Œæ•°ï¼‰**")
if detail_all.empty:
    st.info("æ˜ç´°ã‚’è¡¨ç¤ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    st.dataframe(detail_all, use_container_width=True)

# --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆExcelï¼‰ ---
if not summary_all.empty or not detail_all.empty:
    _buf = io.BytesIO()
    with pd.ExcelWriter(_buf, engine="xlsxwriter") as writer:
        if not summary_all.empty:
            summary_all.to_excel(writer, sheet_name="summary_by_source", index=False)
        if not detail_all.empty:
            detail_all.to_excel(writer, sheet_name="detail_by_file_sheet", index=False)
    st.download_button(
        "ğŸ“¥ é›†è¨ˆè¡¨ã‚’Excelã§ä¿å­˜",
        data=_buf.getvalue(),
        file_name="redlist_file_sheet_summary.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )


# =========================================================
# ğŸ“Š ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ã®é›†è¨ˆï¼ˆå˜ç´”å½¢å¼ï¼‹ã‚½ãƒ¼ã‚¹åˆ¥ åˆè¨ˆè¡Œï¼‹ç©ºè¡Œï¼‰
# =========================================================
st.subheader("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ã®é›†è¨ˆï¼ˆå˜ç´”å½¢å¼ï¼‰")

def _simple_counts_block(source: str, series: pd.Series) -> pd.DataFrame:
    """ç¸¦ä¸¦ã³ã®é›†è¨ˆã«ã€åˆè¨ˆè¡Œï¼‹ç©ºè¡Œã‚’ä»˜ã‘ã¦è¿”ã™"""
    cols = ["ã‚½ãƒ¼ã‚¹", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "ä»¶æ•°"]
    if series is None:
        return pd.DataFrame(columns=cols)

    # NaNã¯ãã®ã¾ã¾æ‰±ã†ã¨è¦‹ã¥ã‚‰ã„ã®ã§ä¸€å¿œã€Œæœªåˆ†é¡ã€ã«å¯„ã›ã¦ãŠãï¼ˆloaderså´ã§æ—¢ã«æœªåˆ†é¡åŒ–æ¸ˆã§ã‚‚å®‰å…¨ï¼‰
    ser = series.fillna("æœªåˆ†é¡").astype(str)

    vc = ser.value_counts(dropna=False).sort_index()
    block = vc.reset_index()
    block.columns = ["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "ä»¶æ•°"]
    block.insert(0, "ã‚½ãƒ¼ã‚¹", source)

    # åˆè¨ˆè¡Œã¨ç©ºè¡Œã‚’è¿½åŠ 
    total_row = pd.DataFrame([{"ã‚½ãƒ¼ã‚¹": source, "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·": "åˆè¨ˆ", "ä»¶æ•°": int(block["ä»¶æ•°"].sum())}])
    spacer    = pd.DataFrame([{"ã‚½ãƒ¼ã‚¹": "",     "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·": "",     "ä»¶æ•°": ""}])

    return pd.concat([block, total_row, spacer], ignore_index=True)[cols]

parts = []
if not moe_df.empty:
    parts.append(_simple_counts_block("ç’°å¢ƒçœ", moe_df["ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]))
if not fuku_df.empty:
    parts.append(_simple_counts_block("ç¦å³¶çœŒ", fuku_df["ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]))
if not chiba_df.empty:
    parts.append(_simple_counts_block("åƒè‘‰çœŒ", chiba_df["åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]))

if parts:
    summary_simple = pd.concat(parts, ignore_index=True)
    st.dataframe(summary_simple, use_container_width=True)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆExcelï¼‰
    _buf_simple = io.BytesIO()
    with pd.ExcelWriter(_buf_simple, engine="xlsxwriter") as writer:
        summary_simple.to_excel(writer, sheet_name="category_counts", index=False)
    st.download_button(
        "ğŸ“¥ é›†è¨ˆï¼ˆå˜ç´”å½¢å¼ï¼‹åˆè¨ˆè¡Œã¤ãï¼‰ã‚’Excelã§ä¿å­˜",
        data=_buf_simple.getvalue(),
        file_name="redlist_category_counts_simple.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )
else:
    st.info("é›†è¨ˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")



# =========================================================
# ğŸ›  ãƒ‡ãƒãƒƒã‚°ï¼šã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ï¼æœªåˆ†é¡ï¼å¤‰æ›ä¸èƒ½ï¼ˆï¼‹ç©ºæ¬„ï¼‰ã‚’æŠ½å‡ºï¼ˆè©³ç´°ç‰ˆï¼‰
# =========================================================
st.subheader("ğŸ›  ãƒ‡ãƒãƒƒã‚°ï¼šã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ãŒã€æœªåˆ†é¡ã€ã€å¤‰æ›ä¸èƒ½ã€ï¼ˆï¼‹ç©ºæ¬„ï¼‰ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")

DEBUG_TARGETS = {"æœªåˆ†é¡", "å¤‰æ›ä¸èƒ½", ""}  # ç©ºæ¬„ã‚‚æ‹¾ã†

def _mk_debug(
    df: pd.DataFrame,
    source: str,
    *,
    symbol_col: str,   # è¨˜å·åˆ—
    raw_col: str,      # å…ƒã‚«ãƒ†ã‚´ãƒªãƒ¼æ–‡å­—åˆ—
    name_col: str,     # å’Œå or ç¨®å
    row_offset: int,   # å…ƒExcelã®ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œï¼ˆMOE=2, ç¦å³¶=5, åƒè‘‰=3ï¼‰
    extra_cols: list[str] = None
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    æˆ»ã‚Šå€¤:
      (summary_df, detail_df)
        summary_df: ã‚½ãƒ¼ã‚¹Ã—è¨˜å·ã”ã¨ã®ä»¶æ•°
        detail_df : [ã‚½ãƒ¼ã‚¹, ãƒ•ã‚¡ã‚¤ãƒ«å, ã‚·ãƒ¼ãƒˆå, æ¨å®šå…ƒExcelè¡Œ, å’Œå/ç¨®å, å­¦å, å…ƒã‚«ãƒ†ã‚´ãƒªãƒ¼, è¨˜å·, ...]
    """
    extra_cols = extra_cols or []
    need_cols = {"ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå",symbol_col,raw_col}
    if df.empty or not need_cols.issubset(df.columns):
        return pd.DataFrame(), pd.DataFrame()

    sym = df[symbol_col].fillna("").astype(str)
    hit = df.loc[sym.isin(DEBUG_TARGETS)].copy()
    if hit.empty:
        return pd.DataFrame(), pd.DataFrame()

    # ãƒ•ã‚¡ã‚¤ãƒ«Ã—ã‚·ãƒ¼ãƒˆå†…ã§ã®è¡Œç•ªå·â†’å…ƒExcelè¡Œï¼ˆ= ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åŠ ç®—ï¼‰
    hit["_row_in_df"] = hit.groupby(["ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå"]).cumcount()
    hit["æ¨å®šå…ƒExcelè¡Œ"] = (hit["_row_in_df"].astype(int) + row_offset).astype(int)
    hit.drop(columns=["_row_in_df"], inplace=True)

    # ã‚µãƒãƒªãƒ¼
    s = (hit[symbol_col].fillna("")
         .value_counts(dropna=False)
         .sort_index()
         .reset_index())
    s.columns = ["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "ä»¶æ•°"]
    s.insert(0, "ã‚½ãƒ¼ã‚¹", source)

    # æ˜ç´°
    keep = ["ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","æ¨å®šå…ƒExcelè¡Œ"]
    if name_col in hit.columns:
        keep += [name_col]
    if "å­¦å" in hit.columns:
        keep += ["å­¦å"]
    keep += [raw_col, symbol_col]
    keep += [c for c in extra_cols if c in hit.columns]

    detail = hit[keep].copy()
    # åˆ—åã‚’è¦‹ã‚„ã™ã
    rename_map = {
        name_col: "å’Œå/ç¨®å",
        raw_col: "å…ƒã‚«ãƒ†ã‚´ãƒªãƒ¼",
        symbol_col: "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·",
    }
    detail.rename(columns=rename_map, inplace=True)
    detail.insert(0, "ã‚½ãƒ¼ã‚¹", source)

    # è¡Œç•ªå·ã§ä¸¦ã¹æ›¿ãˆ
    detail = detail.sort_values(["ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","æ¨å®šå…ƒExcelè¡Œ"])
    return s, detail

with st.expander("ãƒ‡ãƒãƒƒã‚°çµæœã‚’è¡¨ç¤ºï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰", expanded=False):
    dbg_sum_parts, dbg_det_parts = [], []

    # ç’°å¢ƒçœï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹=2è¡Œç›®ï¼‰
    s, d = _mk_debug(
        moe_df, "ç’°å¢ƒçœ",
        symbol_col="ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·",
        raw_col="ã‚«ãƒ†ã‚´ãƒªãƒ¼",
        name_col="å’Œå",
        row_offset=2,
        extra_cols=["åˆ†é¡ç¾¤"]
    )
    if not s.empty: dbg_sum_parts.append(s); dbg_det_parts.append(d)

    # ç¦å³¶çœŒï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹=5è¡Œç›®ï¼‰
    s, d = _mk_debug(
        fuku_df, "ç¦å³¶çœŒ",
        symbol_col="ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·",
        raw_col="ç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼",
        name_col="å’Œå",
        row_offset=5,
        extra_cols=["ç”Ÿç‰©ç¾¤","åˆ†é¡","ç§‘å","ãµãã—ã¾RL2022ã‚«ãƒ†ã‚´ãƒªãƒ¼"]
    )
    if not s.empty: dbg_sum_parts.append(s); dbg_det_parts.append(d)

    # åƒè‘‰çœŒï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹=3è¡Œç›®ï¼‰
    s, d = _mk_debug(
        chiba_df, "åƒè‘‰çœŒ",
        symbol_col="åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·",
        raw_col="ã‚«ãƒ†ã‚´ãƒªãƒ¼",
        name_col="ç¨®å",
        row_offset=3,
        extra_cols=["åˆ†é¡ç¾¤","ç›®ãƒ»ç§‘å","è¨˜å·"]
    )
    if not s.empty: dbg_sum_parts.append(s); dbg_det_parts.append(d)

    dbg_summary = (pd.concat(dbg_sum_parts, ignore_index=True)
                   if dbg_sum_parts else pd.DataFrame(columns=["ã‚½ãƒ¼ã‚¹","ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·","ä»¶æ•°"]))
    dbg_detail  = (pd.concat(dbg_det_parts, ignore_index=True)
                   if dbg_det_parts else pd.DataFrame(columns=["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","æ¨å®šå…ƒExcelè¡Œ","å’Œå/ç¨®å","å­¦å","å…ƒã‚«ãƒ†ã‚´ãƒªãƒ¼","ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]))

    st.markdown("**ğŸ“Œ ã‚µãƒãƒªãƒ¼ï¼ˆã‚½ãƒ¼ã‚¹Ã—è¨˜å·ï¼‰**")
    if dbg_summary.empty:
        st.info("æœªåˆ†é¡ãƒ»å¤‰æ›ä¸èƒ½ãƒ»ç©ºæ¬„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(dbg_summary, use_container_width=True)

    st.markdown("**ğŸ“„ æ˜ç´°ï¼ˆè©²å½“ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰**")
    if dbg_detail.empty:
        st.info("è©²å½“æ˜ç´°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(dbg_detail, use_container_width=True)

    if not dbg_summary.empty or not dbg_detail.empty:
        _buf_dbg = io.BytesIO()
        with pd.ExcelWriter(_buf_dbg, engine="xlsxwriter") as writer:
            if not dbg_summary.empty:
                dbg_summary.to_excel(writer, sheet_name="debug_summary", index=False)
            if not dbg_detail.empty:
                dbg_detail.to_excel(writer, sheet_name="debug_detail", index=False)
        st.download_button(
            "ğŸ“¥ ãƒ‡ãƒãƒƒã‚°æŠ½å‡ºã‚’Excelã§ä¿å­˜",
            data=_buf_dbg.getvalue(),
            file_name="redlist_debug_unclassified_unmappable.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )



# =========================================================
# æ¤œç´¢ï¼šã€Œå’Œåã€ï¼ˆåƒè‘‰ã¯ã€Œç¨®åã€ï¼‰
# =========================================================
st.subheader("ğŸ” å’Œå/ç¨®å æ¤œç´¢")
query = st.text_input("å’Œåï¼ˆç’°å¢ƒçœ/ç¦å³¶ï¼‰ã¾ãŸã¯ ç¨®åï¼ˆåƒè‘‰ï¼‰ ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", value="")

def normalize_name(s: str) -> str:
    if pd.isna(s):
        return ""
    # å‰å¾Œç©ºç™½ã¨å…¨è§’ç©ºç™½ã‚’å…±é€šå‡¦ç†
    s = str(s).replace("\u3000", " ").strip()
    return s

if query:
    q = normalize_name(query)

    results: List[pd.DataFrame] = []

    if not moe_df.empty:
        hit = moe_df[normalize_name(moe_df["å’Œå"]) == q].copy()
        if not hit.empty:
            hit.insert(0, "ã‚½ãƒ¼ã‚¹", "ç’°å¢ƒçœ")
            # è¡¨ç¤ºåˆ—ï¼ˆå‚ç…§ãƒ©ãƒ™ãƒ«é †ï¼‰
            cols = ["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","å’Œå","å­¦å","åˆ†é¡ç¾¤","ã‚«ãƒ†ã‚´ãƒªãƒ¼","ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
            for c in cols:
                if c not in hit.columns:
                    hit[c] = ""
            results.append(hit[cols])

    if not fuku_df.empty:
        hit = fuku_df[normalize_name(fuku_df["å’Œå"]) == q].copy()
        if not hit.empty:
            hit.insert(0, "ã‚½ãƒ¼ã‚¹", "ç¦å³¶çœŒ")
            cols = ["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","å’Œå","å­¦å","ç”Ÿç‰©ç¾¤","åˆ†é¡","ç§‘å","ç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼","ãµãã—ã¾RL2022ã‚«ãƒ†ã‚´ãƒªãƒ¼","ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
            for c in cols:
                if c not in hit.columns:
                    hit[c] = ""
            results.append(hit[cols])

    if not chiba_df.empty:
        hit = chiba_df[normalize_name(chiba_df["ç¨®å"]) == q].copy()
        if not hit.empty:
            hit.insert(0, "ã‚½ãƒ¼ã‚¹", "åƒè‘‰çœŒ")
            cols = ["ã‚½ãƒ¼ã‚¹","ãƒ•ã‚¡ã‚¤ãƒ«å","ã‚·ãƒ¼ãƒˆå","ç¨®å","å­¦å","åˆ†é¡ç¾¤","ç›®ãƒ»ç§‘å","ã‚«ãƒ†ã‚´ãƒªãƒ¼","è¨˜å·","åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"]
            for c in cols:
                if c not in hit.columns:
                    hit[c] = ""
            results.append(hit[cols])

    if results:
        res_df = pd.concat(results, ignore_index=True)
        st.dataframe(res_df, use_container_width=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆxlsxï¼‰
        buf2 = io.BytesIO()
        with pd.ExcelWriter(buf2, engine="xlsxwriter") as writer:
            res_df.to_excel(writer, sheet_name="search_results", index=False)
        st.download_button(
            "ğŸ“¥ æ¤œç´¢çµæœã‚’Excelã§ä¿å­˜",
            data=buf2.getvalue(),
            file_name=f"redlist_search_{q}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
    else:
        st.warning("ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# =========================================================
# å‚™è€ƒ
# =========================================================
with st.expander("â„¹ï¸ å‚™è€ƒï¼ˆå‚ç…§ãƒ©ãƒ™ãƒ«ã¨çŸ­ç¸®å½¢ã®æ‰±ã„ï¼‰", expanded=False):
    st.markdown("""
- è¡¨ç¤ºãƒ»ä¿å­˜ã®åˆ—åã¯ **å‚ç…§ãƒ©ãƒ™ãƒ«**ï¼ˆä¾‹ï¼š*ã‚«ãƒ†ã‚´ãƒªãƒ¼, åˆ†é¡ç¾¤, å’Œå, å­¦å, ãƒ•ã‚¡ã‚¤ãƒ«å, ã‚·ãƒ¼ãƒˆå*ï¼‰ã§çµ±ä¸€ã—ã¦ã„ã¾ã™ã€‚  
- æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã«é™ã‚Šã€çŸ­ç¸®å½¢ï¼ˆA, B, C, D ãªã©ï¼‰ã‚’ä½¿ã„ã¾ã™ãŒã€æœ¬ãƒšãƒ¼ã‚¸ã®å‡ºåŠ›ã¯æ˜ç¢ºã•ã®ãŸã‚å‚ç…§ãƒ©ãƒ™ãƒ«ã‚’æ—¢å®šã¨ã—ã¦ã„ã¾ã™ã€‚  
- ãƒ•ã‚©ãƒ«ãƒ€å **fukusiima** ã¯ã”æŒ‡å®šã©ãŠã‚Šï¼ˆtypoå«ã‚€ï¼‰ã§èªè­˜ã—ã¦ã„ã¾ã™ã€‚  
- åƒè‘‰çœŒã€Œæƒ…å ±ä¸è¶³ã€ã¯ä¸€è²«æ€§ã®ãŸã‚ **DD** ã«æ­£è¦åŒ–ã—ã¦ã„ã¾ã™ã€‚  
- ç¦å³¶çœŒã®ã‚«ãƒ†ã‚´ãƒªãƒ¼å¤‰æ›ã¯ã”æŒ‡å®šã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«å³å¯†æº–æ‹ ã—ã€è©²å½“ã—ãªã„å€¤ã¯ **ã€Œå¤‰æ›ä¸èƒ½ã€** ã¨ã—ã¦ã„ã¾ã™ã€‚  
    """)

