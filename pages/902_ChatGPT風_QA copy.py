# -*- coding: utf-8 -*-
# pages/24_ChatGPTé¢¨_QA.py
from __future__ import annotations

import os
import io
import json
from typing import Optional

import streamlit as st
import docx

from openai import OpenAI

from config.config import has_gemini_api_key, DEFAULT_USDJPY, estimate_tokens_from_text
from lib.gemini_responder import GeminiResponder
from lib.costs_new import estimate_chat_cost, ChatUsage


# ============================================================
# Page
# ============================================================
st.set_page_config(
    page_title="ğŸ’¬ è³ªå•ãƒšãƒ¼ã‚¸ï¼ˆWord / ãƒ†ã‚­ã‚¹ãƒˆ / JSON / Markdownï¼‰",
    page_icon="ğŸ’¬",
    layout="wide",
)

st.title("ğŸ’¬ ChatGPTé¢¨ï¼šæ–‡æ›¸ã‚’èª­ã¾ã›ã¦è³ªå•")

st.caption(
    "Wordï¼ˆ.docxï¼‰ã ã‘ã§ãªãã€.txt / .json / .md ã‚‚èª­ã¿è¾¼ã‚“ã§è³ªå•ã§ãã¾ã™ã€‚"
    " ãã®ã¾ã¾ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ã‚‚OKã§ã™ã€‚"
)

# ============================================================
# Sidebar: model / cost settings
# ============================================================
with st.sidebar:
    st.header("è¨­å®š")

    # --- ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆGPT / Geminiï¼‰ ---
    OPENAI_MODELS = ["gpt-5-mini", "gpt-5-nano"]
    GEMINI_MODELS = ["gemini-2.0-flash"]

    model_options = list(OPENAI_MODELS)
    if has_gemini_api_key():
        model_options += GEMINI_MODELS

    chat_model = st.radio(
        "ãƒ¢ãƒ‡ãƒ«",
        model_options,
        index=0,
        help="Gemini ã¯ API ã‚­ãƒ¼è¨­å®šæ™‚ã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚",
    )

    max_output_tokens = st.number_input(
        "æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¸Šé™ï¼‰",
        min_value=256,
        max_value=20000,
        value=4000,
        step=256,
    )

    st.caption(f"ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼ˆæ¦‚ç®—ï¼‰: {DEFAULT_USDJPY:.2f} JPY/USD")


# ============================================================
# API clients (OpenAI / Gemini)
# ============================================================
def get_openai_client() -> OpenAI:
    api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
    if not api_key:
        raise RuntimeError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`.streamlit/secrets.toml` ã« OPENAI_API_KEY ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    return OpenAI(api_key=api_key)

def is_gemini_model(m: str) -> bool:
    return m.startswith("gemini-")


# ============================================================
# å…¥åŠ›ã‚½ãƒ¼ã‚¹ï¼šãƒ•ã‚¡ã‚¤ãƒ« or ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘
# ============================================================
st.subheader("1ï¸âƒ£ æ–‡æ›¸ã®å…¥åŠ›æ–¹æ³•")

tab_file, tab_text = st.tabs(["ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥è²¼ã‚Šä»˜ã‘"])

source_text: str = ""
source_kind: str = ""

with tab_file:
    uploaded = st.file_uploader(
        "Word / ãƒ†ã‚­ã‚¹ãƒˆ / JSON / Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["docx", "txt", "json", "md"],
    )

    if uploaded is not None:
        file_name = uploaded.name
        ext = file_name.lower().rsplit(".", 1)[-1]

        try:
            if ext == "docx":
                doc = docx.Document(uploaded)
                source_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                source_kind = "Word(.docx)"

            elif ext in ("txt", "md"):
                raw = uploaded.read()
                source_text = raw.decode("utf-8", errors="ignore")
                source_kind = f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.{ext})"

            elif ext == "json":
                raw = uploaded.read()
                obj = json.loads(raw.decode("utf-8", errors="ignore"))
                source_text = json.dumps(obj, ensure_ascii=False, indent=2)
                source_kind = "JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.jsonï¼‰"

        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        if source_text:
            st.success(f"âœ… {source_kind} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆç´„ {len(source_text)} æ–‡å­—ï¼‰")
            with st.expander("èª­ã¿è¾¼ã‚“ã æœ¬æ–‡ï¼ˆå…ˆé ­éƒ¨åˆ†ã‚’ç¢ºèªï¼‰", expanded=False):
                preview = source_text[:1000]
                if len(source_text) > 1000:
                    preview += "\nâ€¦ï¼ˆçœç•¥ï¼‰"
                st.code(preview, language="text")

with tab_text:
    pasted = st.text_area(
        "ãƒ†ã‚­ã‚¹ãƒˆ / JSON / Markdown ã‚’ç›´æ¥è²¼ã‚Šä»˜ã‘",
        height=250,
        placeholder="ã“ã“ã«ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚ï¼ˆWord ã‹ã‚‰ã‚³ãƒ”ãƒš / JSON / Markdown ãªã©ï¼‰",
    )
    if pasted.strip():
        source_text = pasted
        source_kind = "è²¼ã‚Šä»˜ã‘ãƒ†ã‚­ã‚¹ãƒˆ"
        st.info(f"ğŸ“Œ ç¾åœ¨ã®è³ªå•å¯¾è±¡ã¯ã€Œ{source_kind}ã€ï¼ˆç´„ {len(source_text)} æ–‡å­—ï¼‰ã§ã™ã€‚")


# ============================================================
# è³ªå•å…¥åŠ› & å®Ÿè¡Œ
# ============================================================
st.subheader("2ï¸âƒ£ è³ªå•ã™ã‚‹")

question = st.text_area(
    "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
    placeholder="ä¾‹ï¼šã“ã®æ–‡æ›¸ã§æ”¹è¨‚ã•ã‚ŒãŸéƒ¨åˆ†ã¯ã©ã“ï¼Ÿ / ã“ã®JSONã§é‡è¦ãªã‚­ãƒ¼ã¯ï¼Ÿ / ã“ã®Markdownã®æ§‹æˆã‚’è¦ç´„ã—ã¦ ãªã©",
    height=120,
)

col_run, col_info = st.columns([1, 2])

with col_run:
    run = st.button("AIã«è³ªå•ã™ã‚‹", type="primary", use_container_width=True)

with col_info:
    st.caption(
        "â€» å…¥åŠ›ã‚½ãƒ¼ã‚¹ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€è²¼ã‚Šä»˜ã‘ãƒ†ã‚­ã‚¹ãƒˆãŒå„ªå…ˆã•ã‚Œã¾ã™ã€‚"
        " ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§è³ªå•ã—ãŸã„å ´åˆã¯ã€è²¼ã‚Šä»˜ã‘æ¬„ã‚’ç©ºã«ã—ã¦ãã ã•ã„ã€‚"
    )


# ============================================================
# å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
# ============================================================
def build_prompt(source_kind: str, question: str, used_text: str, max_chars: int) -> str:
    return f"""
ã‚ãªãŸã¯æ–‡æ›¸ç·¨é›†ã¨ãƒ‡ãƒ¼ã‚¿è§£é‡ˆã«è©³ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æœ¬æ–‡ï¼ˆ{source_kind}ï¼‰ã‚’èª­ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{question}

ã€æœ¬æ–‡ï¼ˆå…ˆé ­ã€œæœ€å¤§{max_chars}æ–‡å­—ã¾ã§ï¼‰ã€‘
{used_text}
""".strip()


if run:
    if not source_text.strip():
        st.error("å…ˆã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")
        st.stop()
    if not question.strip():
        st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # é•·ã™ãã‚‹å ´åˆã¯ã‚«ãƒƒãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ä¿è­·ç”¨ï¼‰
    max_chars = 15000
    used_text = source_text[:max_chars]
    prompt = build_prompt(source_kind, question.strip(), used_text, max_chars)

    answer = ""
    used_gemini = is_gemini_model(chat_model)

    # usageï¼ˆå–ã‚Œãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§ Optionalï¼‰
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    note = ""

    with st.spinner("AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
        if used_gemini:
            # --- Gemini ---
            if not has_gemini_api_key():
                st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Gemini ã‚’ä½¿ã†ã«ã¯ã‚­ãƒ¼è¨­å®šãŒå¿…è¦ã§ã™ã€‚")
                st.stop()

            responder = GeminiResponder()
            result = responder.complete(
                model=chat_model,
                system_instruction="ã‚ãªãŸã¯ä¸å¯§ãªæ—¥æœ¬èªã§èª¬æ˜ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
                user_content=prompt,
                max_output_tokens=int(max_output_tokens),
            )
            answer = (result.text or "").strip()

            # Gemini ã¯ usage ãŒå–ã‚Œãªã„ã“ã¨ãŒå¤šã„ã®ã§æ¨å®š
            input_tokens = int(estimate_tokens_from_text(prompt))
            output_tokens = int(estimate_tokens_from_text(answer))
            note = "ï¼ˆGemini: ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼‰"

        else:
            # --- OpenAI (GPT) ---
            try:
                client = get_openai_client()
            except Exception as e:
                st.error(str(e))
                st.stop()

            res = client.chat.completions.create(
                model=chat_model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ä¸å¯§ãªæ—¥æœ¬èªã§èª¬æ˜ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                # äº’æ›ã®ãŸã‚ max_tokens ã‚’æŒ‡å®šï¼ˆOpenAIå´ä»•æ§˜å¤‰æ›´ãŒã‚ã£ã¦ã‚‚å®‰å…¨ã«å€’ã™ï¼‰
                max_tokens=int(max_output_tokens),
            )

            answer = (res.choices[0].message.content or "").strip()

            # usage ãŒå–ã‚Œã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ã†
            try:
                u = res.usage
                if u is not None:
                    # SDKã«ã‚ˆã‚Šã‚­ãƒ¼åãŒé•ã†å ´åˆãŒã‚ã‚‹ã®ã§ä¸¡å¯¾å¿œ
                    input_tokens = int(getattr(u, "prompt_tokens", None) or getattr(u, "input_tokens", 0) or 0)
                    output_tokens = int(getattr(u, "completion_tokens", None) or getattr(u, "output_tokens", 0) or 0)
            except Exception:
                input_tokens = None
                output_tokens = None

            # å–ã‚Œãªã‘ã‚Œã°æ¨å®šã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not input_tokens:
                input_tokens = int(estimate_tokens_from_text(prompt))
                note = "ï¼ˆOpenAI: ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼‰"
            if not output_tokens:
                output_tokens = int(estimate_tokens_from_text(answer))
                note = "ï¼ˆOpenAI: ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼‰"

    # --- å›ç­” ---
    st.markdown("### ğŸ§­ å›ç­”")
    st.write(answer)

    # --- æ–™é‡‘è¡¨ç¤ºï¼ˆæ¦‚ç®—ï¼‰ ---
    try:
        cost = estimate_chat_cost(
            chat_model,
            ChatUsage(input_tokens=int(input_tokens or 0), output_tokens=int(output_tokens or 0)),
            rate=DEFAULT_USDJPY,
        )
        st.markdown("### ğŸ’° æ–™é‡‘ï¼ˆæ¦‚ç®—ï¼‰")
        st.write(f"- ãƒ¢ãƒ‡ãƒ«: **{chat_model}**")
        st.write(f"- Input tokens: {int(input_tokens or 0):,}")
        st.write(f"- Output tokens: {int(output_tokens or 0):,}")
        st.info(f"ğŸ“Š æ¦‚ç®—: **Â¥{cost['jpy']:,.2f}**ï¼ˆ${cost['usd']:.6f}ï¼‰{note}")
    except Exception as e:
        st.warning(f"æ–™é‡‘è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

if not source_text:
    st.info("ã¾ãšãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")
