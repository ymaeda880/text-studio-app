# -*- coding: utf-8 -*-
# pages/16_wordè§£æ.py
#
# Word(.docx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…éƒ¨æ§‹é€ ã‚’ã–ã£ãã‚Šè§£æã—ã€
# - æœ¬æ–‡ / å›³ / è¡¨ / ç›®æ¬¡å€™è£œ / è¦‹å‡ºã— ã‚’åˆ†é¡
# - ã€Œç”ŸæˆAIã¸ã®å…¥åŠ›ç”¨ã€ã®ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆã‚’ 1 ã¤ç”Ÿæˆã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
#   * è¦‹å‡ºã—     : === HEADING[3-1-2] ã‚¿ã‚¤ãƒˆãƒ« ===  ã®ã‚ˆã†ã«ç« ç•ªå·ä»˜ãã§å‡ºåŠ›
#   * æœ¬æ–‡       : ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
#   * è¡¨         : è¡¨ç•ªå·ï¼‹ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸‹ã« JSON ã‚’åŸ‹ã‚è¾¼ã‚€
#   * å›³         : å›³ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼‹ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å
# - ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ ZIP ã§ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
#
# â€» base_chapterï¼ˆã“ã®ç« ãŒç¬¬ä½•ç« ã‹ï¼‰ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰æŒ‡å®š

from __future__ import annotations

from pathlib import Path
import sys

_THIS = Path(__file__).resolve()
PROJECTS_ROOT = _THIS.parents[3] ## appã®æ™‚ã¯[2]
if str(PROJECTS_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECTS_ROOT))


from io import BytesIO
from typing import List, Tuple, Dict, Any
import json

import streamlit as st

# from pathlib import Path
# import sys

# =========================
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ sys.path ã«è¿½åŠ 
# =========================
# PROJECTS_ROOT = Path(__file__).resolve().parents[3]
# if str(PROJECTS_ROOT) not in sys.path:
#     sys.path.insert(0, str(PROJECTS_ROOT))

# _THIS = Path(__file__).resolve()
# PROJECTS_ROOT = _THIS.parents[3] ## appã®æ™‚ã¯[2]
# if str(PROJECTS_ROOT) not in sys.path:
#     sys.path.insert(0, str(PROJECTS_ROOT))

# ==== python-docx é–¢é€£ ====
try:
    from docx import Document
    from docx.text.paragraph import Paragraph
    from docx.table import Table
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

# ==== è‡ªä½œãƒ©ã‚¤ãƒ–ãƒ©ãƒª ====
from lib.word_analysis.blocks import (
    Block,
    iter_block_items,
    classify_paragraph,
    classify_block,
)
from lib.word_analysis.headings import (
    is_heading_paragraph,
    detect_heading_level,
    format_heading_id,
)
from lib.word_analysis.tables import (
    table_to_json,
)
from lib.word_analysis.images import (
    get_image_filenames_from_paragraph,
    collect_images_as_zip,
)

# --- Inboxã¸ä¿å­˜ï¼ˆcommon_lib.inbox.*ï¼‰---
from common_lib.auth.auth_helpers import require_login
from common_lib.inbox.inbox_ops.ingest import ingest_to_inbox
from common_lib.inbox.inbox_common.types import (
    IngestRequest,
    InboxNotAvailable,
    QuotaExceeded,
    IngestFailed,
)

from lib.word_analysis.explanation import render_word_analysis_help_expander

from common_lib.ui.ui_basics import subtitle
from common_lib.ui.banner_lines import render_banner_line_by_key

# =========================
# ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰
# =========================
def build_intermediate_text(
    doc: Document,
    base_chapter: int,
    #mode: str = "standard",  # "standard" or "simple"
    mode: str = "detailed",  # "simple" | "standard" | "detailed"

    
) -> Tuple[str, Dict[str, int]]:

    simple_mode = (mode == "simple")

    lines: List[str] = []
    stats = {"heading": 0, "paragraph": 0, "table": 0, "figure": 0}

    heading_counters = [0, 0, 0, 0]
    prev_block: Block | None = None
    pending_table_caption: Paragraph | None = None


    def _looks_like_heading_line(s: str) -> bool:
        """
        ã€Œè¦‹å‡ºã—ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ã€ã®ç°¡æ˜“åˆ¤å®šï¼ˆæ¨™æº–ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰
        æ¡ä»¶ï¼ˆã‚ãªãŸã®è¦ä»¶ï¼‰ï¼š
          - å¥èª­ç‚¹ãŒãªã„ï¼ˆã€‚ã€ã€Œã€ï¼Œï¼ ãªã©ï¼‰
          - 1è¡Œã®ã¿ï¼ˆ= æ”¹è¡Œãªã—ï¼‰
          - çŸ­ã™ããšé•·ã™ããªã„ï¼ˆå®‰å…¨å´ã®ã‚¬ãƒ¼ãƒ‰ï¼‰
        """
        if not s:
            return False

        t = s.strip()
        if not t:
            return False

        # 1è¡Œã®ã¿ï¼ˆå¿µã®ãŸã‚ï¼‰
        if "\n" in t or "\r" in t:
            return False

        # å¥èª­ç‚¹ãŒãªã„ï¼ˆå¿…è¦ãªã‚‰å¢—ã‚„ã—ã¦OKï¼‰
        for ch in ["ã€‚", "ã€", "ï¼Œ", "ï¼", ".", ",", "!", "?", "ï¼", "ï¼Ÿ", ":", "ï¼š", ";", "ï¼›"]:
            if ch in t:
                return False

        # é•·ã•ã‚¬ãƒ¼ãƒ‰ï¼ˆãŠå¥½ã¿ã§èª¿æ•´ï¼‰
        if len(t) < 2:
            return False
        if len(t) > 80:
            return False

        return True


    def append_blank():
        """ç°¡ç´ ãƒ¢ãƒ¼ãƒ‰ã§ã€HEADING/FIGURE/TABLE ã®ä»£ã‚ã‚Šã«å…¥ã‚Œã‚‹ç©ºè¡Œ"""
        if simple_mode:
            lines.append("")

    for block_idx, block in enumerate(iter_block_items(doc), start=1):

        # --------------------
        # Paragraph
        # --------------------
        if isinstance(block, Paragraph):

            # ---- è¦‹å‡ºã— ----
            if is_heading_paragraph(block):
                level = detect_heading_level(block)
                heading_id = format_heading_id(base_chapter, heading_counters, level)
                text = (block.text or "").strip()

                if text:
                    # mode ã¯ simple / standard / detailed
                    if mode == "simple":
                        append_blank()
                        lines.append(text)

                    elif mode == "standard":
                        # â˜…é‡è¦ï¼š=== HEADING ã®ã‚¹ã‚¿ã‚¤ãƒ«ã§ã‚‚æœ¬æ–‡ã‚’æ‹¬ã£ã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹
                        # â†’ ã€Œè¦‹å‡ºã—ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ã€ã ã‘ã‚’è¦‹å‡ºã—ã¨ã—ã¦æ‹¬ã‚‹
                        if _looks_like_heading_line(text):
                            lines.append("<ã“ã“ã‹ã‚‰è¦‹å‡ºã—>")
                            lines.append(text)
                            lines.append("<ã“ã“ã¾ã§è¦‹å‡ºã—>")
                            lines.append("")
                            stats["heading"] += 1
                        else:
                            # è¦‹å‡ºã—ã£ã½ããªã‘ã‚Œã°æœ¬æ–‡æ‰±ã„ã¨ã—ã¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã§å‡ºã™
                            lines.append(text)
                            stats["paragraph"] += 1

                    else:  # mode == "detailed"
                        lines.append(f"=== HEADING[{heading_id}] {text} ===")
                        stats["heading"] += 1

                prev_block = block
                pending_table_caption = None
                continue






            # ---- è¦‹å‡ºã—ä»¥å¤–ã®åˆ†é¡ ----
            cat = classify_paragraph(block)

            if cat == "toc":
                prev_block = block
                pending_table_caption = None
                continue

            if cat == "table_caption":
                pending_table_caption = block
                prev_block = block
                continue

            # ---- å›³ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ----
            if cat == "figure":
                caption = (block.text or "").strip()
                img_files = get_image_filenames_from_paragraph(block)
                stats["figure"] += 1
                fig_id = stats["figure"]

                if mode == "simple":
                    append_blank()
                    if caption:
                        lines.append(caption)

                elif mode == "standard":
                    # æ¨™æº–ï¼šè¦‹å‡ºã—ã»ã©å³å¯†ã«ã—ãªã„ãŒã€åˆ†ã‹ã‚Šã‚„ã™ã
                    if caption:
                        lines.append("<ã“ã“ã‹ã‚‰å›³>")
                        lines.append(caption)
                        lines.append("<ã“ã“ã¾ã§å›³>")
                    else:
                        lines.append("<ã“ã“ã‹ã‚‰å›³>")
                        lines.append("<ã“ã“ã¾ã§å›³>")

                    # image_files è¡Œã¯ä¸¡ãƒ¢ãƒ¼ãƒ‰å…±é€šã®æƒ…å ±ã¨ã—ã¦æ®‹ã™ï¼ˆé‹ç”¨ä¸Šä¾¿åˆ©ï¼‰
                    if img_files:
                        lines.append(f"image_files: {', '.join(img_files)}")
                    else:
                        lines.append("image_files: (none)")

                else:  # detailed
                    lines.append(f"=== FIGURE[{fig_id}] {caption} ===")
                    if img_files:
                        lines.append(f"image_files: {', '.join(img_files)}")
                    else:
                        lines.append("image_files: (none)")


                lines.append("")
                prev_block = block
                pending_table_caption = None
                continue

            # ---- æœ¬æ–‡ ----
            text = (block.text or "").strip()
            if text:
                lines.append(text)
                stats["paragraph"] += 1

            prev_block = block
            pending_table_caption = None

        # --------------------
        # Table
        # --------------------
        elif isinstance(block, Table):
            cat = classify_block(block, prev_block)

            if cat == "table":
                stats["table"] += 1
                tbl_json = table_to_json(
                    block,
                    pending_table_caption,
                    use_same_left_placeholder=use_same_left_placeholder,
                )

                if mode in ("simple", "standard"):
                    # ç°¡ç´ ãƒ¢ãƒ¼ãƒ‰ï¼š
                    # - ã€Œ=== TABLE ... ===ã€ã€Œ=== END_TABLE ===ã€ã®ä»£ã‚ã‚Šã«
                    #   <ã“ã“ã‹ã‚‰è¡¨> ï½ <ã“ã“ã¾ã§è¡¨> ã‚’å‡ºã™
                    # - table_number/title/cells: ã®è¦‹å‡ºã—è¡Œã¯å‡ºã•ãªã„
                    # - å„è¡Œã¯ [a, b, c, ...] ã® 1 è¡Œè¡¨è¨˜
                    # - ã‚»ãƒ«å†…æ”¹è¡Œã¯ã‚¹ãƒšãƒ¼ã‚¹ã«çµ±åˆ
                    append_blank()

                    cells = tbl_json.get("cells", [])

                    lines.append("<ã“ã“ã‹ã‚‰è¡¨>")
                    for row in cells:
                        processed = []
                        for x in row:
                            s = str(x).replace("\n", " ").replace("\r", " ")
                            s = " ".join(s.split())
                            processed.append(s)

                        row_text = ", ".join(processed)
                        lines.append(f"[{row_text}]")
                    lines.append("<ã“ã“ã¾ã§è¡¨>")
                    lines.append("")

                else:  # mode == "standard" or "detailed"ï¼ˆTABLEã¯åŒã˜æ‰±ã„ï¼‰
                    # æ¨™æº– / è©³ç´°ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ä»•æ§˜ãã®ã¾ã¾ï¼‰
                    lines.append("")
                    tbl_num = tbl_json.get("table_number", "unknown")
                    tbl_title = tbl_json.get("title", "")
                    lines.append(f"=== TABLE {tbl_num} {tbl_title} ===")
                    lines.append("```json")
                    lines.append(json.dumps(tbl_json, ensure_ascii=False, indent=2))
                    lines.append("```")
                    lines.append("=== END_TABLE ===")
                    lines.append("")

                pending_table_caption = None
                prev_block = block
                continue


            prev_block = block
            pending_table_caption = None

        else:
            prev_block = block
            pending_table_caption = None

    intermediate_text = "\n".join(lines).strip() + "\n"
    return intermediate_text, stats


def split_text_by_heading_markers(text: str, limit: int) -> List[str]:
    """
    <ã“ã“ã‹ã‚‰è¦‹å‡ºã—> ã®ç›´å‰ã‚’ã€Œå€™è£œã®åˆ‡ã‚Œç›®ã€ã¨ã—ã¦ã€
    1ãƒãƒ£ãƒ³ã‚¯ãŒ limit æ–‡å­—ã‚’è¶…ãˆãªã„ã‚ˆã†ã«åˆ†å‰²ã™ã‚‹ã€‚

    - <ã“ã“ã‹ã‚‰è¦‹å‡ºã—> ãŒç„¡ã„å ´åˆã¯ã€æœ€å¾Œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å˜ç´”åˆ†å‰²ã€‚
    - å…ˆé ­ã«è¦‹å‡ºã—ãƒãƒ¼ã‚«ãƒ¼ãŒæ¥ã¦ã‚‚ç©ºãƒãƒ£ãƒ³ã‚¯ã¯ä½œã‚‰ãªã„ã€‚
    """
    if not text:
        return []

    t = text.strip()
    if not t:
        return []

    marker = "<ã“ã“ã‹ã‚‰è¦‹å‡ºã—>"

    # è¦‹å‡ºã—ãƒãƒ¼ã‚«ãƒ¼ãŒç„¡ã„ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå›ºå®šé•·ã§å‰²ã‚‹ï¼‰
    if marker not in t:
        chunks: List[str] = []
        i = 0
        while i < len(t):
            chunks.append(t[i : i + limit].strip() + "\n")
            i += limit
        return [c for c in chunks if c.strip()]

    # ãƒãƒ¼ã‚«ãƒ¼ä½ç½®ã§åˆ†å‰²å€™è£œã‚’ä½œã‚‹ï¼ˆmarker ã‚’ä¿æŒã—ãŸã¾ã¾ï¼‰
    parts = t.split(marker)

    # parts[0] ã¯ marker ã‚ˆã‚Šå‰ï¼ˆæœ¬æ–‡ãªã©ï¼‰ã€‚parts[1:] ã¯ marker ã®å¾Œã‚æ–­ç‰‡
    # å¾Œã‚æ–­ç‰‡ã‚’ marker ä»˜ãã«æˆ»ã™
    segments: List[str] = []
    if parts[0].strip():
        segments.append(parts[0].strip())

    for p in parts[1:]:
        seg = (marker + p).strip()
        if seg:
            segments.append(seg)

    # segments ã‚’ limit ä»¥å†…ã«ãªã‚‹ã‚ˆã†ã«æŸã­ã‚‹ï¼ˆåŸºæœ¬ã¯ marker ã®ç›´å‰ã§åˆ‡ã‚Œã‚‹ï¼‰
    chunks: List[str] = []
    buf = ""

    def flush():
        nonlocal buf
        if buf.strip():
            chunks.append(buf.strip() + "\n")
        buf = ""

    for seg in segments:
        # seg å˜ä½“ãŒ limit ã‚’è¶…ãˆã‚‹å ´åˆï¼šã“ã“ã¯ä¾‹å¤–ã€‚seg ã‚’å†…éƒ¨ã§ã•ã‚‰ã«å‰²ã‚‹ï¼ˆã§ã‚‚ marker ã¯å…ˆé ­ç¶­æŒï¼‰
        if len(seg) > limit:
            # ã¾ãš buf ã‚’ç¢ºå®š
            flush()
            # seg ã‚’ç„¡ç†ã‚„ã‚Šå‰²ã‚‹
            j = 0
            while j < len(seg):
                chunks.append(seg[j : j + limit].strip() + "\n")
                j += limit
            continue

        # buf ã«è¶³ã—ã¦ limit ã‚’è¶…ãˆã‚‹ãªã‚‰ã€ã“ã“ã§åˆ‡ã‚‹ï¼ˆseg ã¯æ–°ãƒãƒ£ãƒ³ã‚¯ã¸ï¼‰
        if buf and (len(buf) + 1 + len(seg) > limit):
            flush()
            buf = seg
        else:
            buf = (buf + "\n" + seg) if buf else seg

    flush()
    return [c for c in chunks if c.strip()]



# =========================
# Streamlit UI
# =========================
st.set_page_config(
    page_title="ğŸ“„ Word è§£æ â†’ ç”ŸæˆAIç”¨ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆ",
    page_icon="ğŸ“„",
    layout="wide",
)

# ============================================================
# ãƒãƒŠãƒ¼ / ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬æº–æ‹ ï¼‰
# ============================================================
render_banner_line_by_key("purple_light")

# ============================================================
# session_state keysï¼ˆè§£æçµæœã‚’ rerun ã§ã‚‚ä¿æŒã™ã‚‹ï¼‰
# ============================================================
SS_TEXT = "word15_intermediate_text"
SS_STATS = "word15_stats"
SS_TXT_NAME = "word15_txt_name"
SS_SOURCE = "word15_source_filename"



#st.title("ğŸ“„ Word è§£æ â†’ ç”ŸæˆAIå…¥åŠ›ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")

# --- Inboxä¿å­˜ã®ãŸã‚ãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆ ---
sub = require_login(st)
if not sub:
    st.stop()
left, right = st.columns([2, 1])
with left:
    st.title("ğŸ“„ Word è§£æ")
with right:
    st.success(f"âœ… ãƒ­ã‚°ã‚¤ãƒ³ä¸­: **{sub}**")


subtitle("ç”ŸæˆAIå…¥åŠ›ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")

st.caption("Wordæ›¸é¡ã®æ–‡ç« æ ¡æ­£ã‚’è¡Œã†å‰å‡¦ç†ã¨ã—ã¦ï¼ŒWordæ›¸é¡ã‚’AIãŒèª­ã‚ã‚‹ã‚ˆã†ã«ã—ãŸä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ï¼"
           "inboxã«å¯¾å¿œã—ã¦ã„ã¾ã™ã®ã§ï¼Œä½œæˆã•ã‚ŒãŸä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’inboxã«ä¿å­˜ã—ã¦ï¼Œæ–‡ç« æ ¡æ­£ã«é€²ã‚€ã“ã¨ãŒã§ãã¾ã™ï¼"
           "Wordæ›¸é¡ã®å­—æ•°ãŒå¤šã„æ™‚ã¯ï¼Œä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã¯30,000å­—ç¨‹åº¦ã«åŒºåˆ‡ã£ãŸè¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã™ï¼")

st.caption("ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯åŸå‰‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å•é¡Œã‚ã‚Šã¾ã›ã‚“ï¼")

render_word_analysis_help_expander()


if not HAS_DOCX:
    st.error("python-docx ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚`python-docx` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
with st.sidebar:
    st.header("ğŸ”§ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    st.caption("ã€Œå‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«ã€ã¯ï¼Œç¶šã‘ã¦ã€Œæ–‡ç« æ ¡æ­£ã€ã‚’è¡Œã†æ™‚ã¯ã€Œæ¨™æº–ã€ã§ä½¿ç”¨ã—ã¦ãã ã„ï¼")

    # å‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«é¸æŠï¼ˆç°¡ç´  / æ¨™æº– / è©³ç´°ï¼‰
    output_mode_label = st.radio(
        "å‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«",
        options=["ç°¡ç´ ", "æ¨™æº–", "è©³ç´°"],
        index=1,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç°¡ç´ 
        horizontal=True,
        help=(
            "ç°¡ç´ ï¼šãƒ—ãƒ¬ãƒ¼ãƒ³å¯„ã‚Š\n"
            "æ¨™æº–ï¼šè¦‹å‡ºã—/è¡¨ãªã©ã‚’èª­ã¿ã‚„ã™ãæ•´å½¢ï¼ˆè¦‹å‡ºã—å€™è£œã‚’ <ã“ã“ã‹ã‚‰è¦‹å‡ºã—> ã§æ‹¬ã‚‹ï¼‰\n"
            "è©³ç´°ï¼šå¾“æ¥ã® === HEADING[...] === ç­‰ã®ãƒãƒ¼ã‚«ãƒ¼ä»˜ã"
        ),
    )

    if output_mode_label == "ç°¡ç´ ":
        output_mode = "simple"
    elif output_mode_label == "æ¨™æº–":
        output_mode = "standard"
    else:
        output_mode = "detailed"

    st.caption(
            "ã€Œã“ã®ç« ã®ç« ç•ªå·ã€ã¯ï¼Œ1ã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„"
        )
    base_chapter = st.number_input(
        "ã“ã®ç« ã®ç« ç•ªå· (base_chapter)",
        min_value=1,
        max_value=50,
        value=1,
        step=1,
        help="è¦‹å‡ºã—IDã®å…ˆé ­ã«ä»˜ã‘ã‚‹ç« ç•ªå·ã§ã™ï¼ˆä¾‹: 3 â†’ HEADING[3-1-2]ï¼‰ã€‚",
    )

    st.caption(
        "ã€Œçµåˆã‚»ãƒ«ã®æ‰±ã„ã€ã¯ï¼Œç¶šã‘ã¦ã€Œæ–‡ç« æ ¡æ­£ã€ã‚’è¡Œã†æ™‚ã¯ã€Œæ¨ªçµåˆã‚»ãƒ«ã‚’<åŒå·¦>ã«ã™ã‚‹ã€ã§ä½¿ç”¨ã—ã¦ãã ã„ï¼"
    )

    # --- è¡¨ã®çµåˆã‚»ãƒ«å‡¦ç†ã®é¸æŠ ---
    merge_label = st.radio(
        "çµåˆã‚»ãƒ«ã®æ‰±ã„",
        options=["ãã®ã¾ã¾", "æ¨ªçµåˆã‚»ãƒ«ã‚’ <åŒå·¦> ã«ã™ã‚‹"],
        index=1,
        help="æ¨ªæ–¹å‘ã«çµåˆã•ã‚Œã¦ã„ã‚‹ã‚»ãƒ«ã‚’ <åŒå·¦> ã§åŸ‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚",
    )
    use_same_left_placeholder = (merge_label == "æ¨ªçµåˆã‚»ãƒ«ã‚’ <åŒå·¦> ã«ã™ã‚‹")

     # --- Inboxã¸é€ã‚‹æ™‚ã®åˆ†å‰²ä¸Šé™ï¼ˆæ–‡å­—æ•°ï¼‰---
    st.caption(
        "ã€ŒInboxã¸é€ã‚‹æ™‚ã®åˆ†å‰²ä¸Šé™ã€ã¯ï¼Œç¶šã‘ã¦ã€Œæ–‡ç« æ ¡æ­£ã€ã‚’è¡Œã†æ™‚ã¯30,000ã§ä½¿ç”¨ã—ã¦ãã ã„ï¼"
    )
    chunk_char_limit = st.slider(
        "ğŸ“ Inboxé€ä¿¡ç”¨ åˆ†å‰²ä¸Šé™ï¼ˆæ–‡å­—æ•°ï¼‰",
        min_value=10000,
        max_value=50000,
        value=30000,     # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 30000
        step=5000,
        help="ã€Œ<ã“ã“ã‹ã‚‰è¦‹å‡ºã—>ã€ã®ç›´å‰ã§åˆ‡ã£ã¦ã€1ãƒ•ã‚¡ã‚¤ãƒ«ãŒã“ã®æ–‡å­—æ•°ã‚’è¶…ãˆãªã„ã‚ˆã†ã«åˆ†å‰²ã—ã¾ã™ã€‚",
    )
   


uploaded_file = st.file_uploader("Word ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.docxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["docx"])

col_btn1, _ = st.columns([1, 3])
with col_btn1:
    run = st.button("ğŸ” è§£æã—ã¦ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ", type="primary")


if uploaded_file is None:
    st.info("ã¾ãš .docx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# è§£æçµæœãŒæ—¢ã« session_state ã«ã‚ã‚Œã°ã€run=False ã§ã‚‚è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
has_cached = bool(st.session_state.get(SS_TEXT)) and (st.session_state.get(SS_SOURCE) == uploaded_file.name)

if (not run) and (not has_cached):
    st.stop()

# =========================
# è§£ææœ¬ä½“
# =========================
try:
    src_doc = Document(uploaded_file)
except Exception as e:
    st.error(f"Word ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# è§£ææ¸ˆã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹ãªã‚‰ãã‚Œã‚’ä½¿ã„ã€ç„¡ã‘ã‚Œã° run ã§è§£æã™ã‚‹
has_cached = bool(st.session_state.get(SS_TEXT)) and (st.session_state.get(SS_SOURCE) == uploaded_file.name)

if run or (not has_cached):
    with st.status("è§£æä¸­ã§ã™â€¦", expanded=False) as status:
        intermediate_text, stats = build_intermediate_text(
            src_doc,
            base_chapter=int(base_chapter),
            mode=output_mode,
        )
        status.update(label="è§£æå®Œäº†", state="complete")

    # session_state ã«ä¿æŒï¼ˆrerun ã§ã‚‚æ¶ˆãˆãªã„ï¼‰
    st.session_state[SS_TEXT] = intermediate_text
    st.session_state[SS_STATS] = stats
    st.session_state[SS_SOURCE] = uploaded_file.name

    base_name = uploaded_file.name.rsplit(".", 1)[0]

    if output_mode == "simple":
        mode_jp = "ç°¡ç´ "
    elif output_mode == "standard":
        mode_jp = "æ¨™æº–"
    else:
        mode_jp = "è©³ç´°"

    st.session_state[SS_TXT_NAME] = f"{base_name}_intermediate_{mode_jp}.txt"

else:
    intermediate_text = st.session_state[SS_TEXT]
    stats = st.session_state[SS_STATS]


# =========================
# çµæœè¡¨ç¤º
# =========================
st.subheader("ğŸ“Š åˆ†é¡çµæœï¼ˆãƒ–ãƒ­ãƒƒã‚¯æ•°ï¼‰")

c1, c2, c3, c4 = st.columns(4)
c1.metric("è¦‹å‡ºã—æ•°", stats.get("heading", 0))
c2.metric("æœ¬æ–‡æ®µè½æ•°", stats.get("paragraph", 0))
c3.metric("è¡¨ãƒ–ãƒ­ãƒƒã‚¯æ•°", stats.get("table", 0))
c4.metric("å›³ãƒ–ãƒ­ãƒƒã‚¯æ•°", stats.get("figure", 0))

st.markdown("---")

st.subheader("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­éƒ¨åˆ†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
# ============================================================
# ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä»¥é™ã§å…±é€šåˆ©ç”¨ï¼‰
# ============================================================
txt_name = st.session_state.get(SS_TXT_NAME) or "intermediate.txt"


st.code(intermediate_text[:3000], language="text")  # é•·ããªã‚Šã™ããªã„ã‚ˆã†ã«é ­ã ã‘

st.markdown("---")

# ============================================================
# åˆ†å‰²å¾Œã®ã€Œç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆäºˆå®šï¼‰ã€ä¸€è¦§ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‰ã«è¡¨ç¤ºï¼‰
# ============================================================
chunks_preview = split_text_by_heading_markers(intermediate_text, int(chunk_char_limit))

def _split_filename(name: str) -> tuple[str, str]:
    if "." in name:
        base, ext = name.rsplit(".", 1)
        return base, "." + ext
    return name, ""

# è§£ææ™‚ã«æ±ºã‚ãŸ txt_name ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€Œäºˆå®šãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚’ä½œã‚‹
base_fn_preview, ext_fn_preview = _split_filename(txt_name)

planned_names: List[str] = []
if chunks_preview:
    total_preview = len(chunks_preview)
    for idx in range(1, total_preview + 1):
        if total_preview == 1:
            fn = txt_name
        else:
            fn = f"{base_fn_preview}_part{idx:03d}{ext_fn_preview or '.txt'}"
        planned_names.append(fn)

    st.subheader("ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆåˆ†å‰²å¾Œãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼‰")
    st.caption("â€» æ–‡å­—æ•°ä¸Šé™ã¨ <ã“ã“ã‹ã‚‰è¦‹å‡ºã—> ã®ç›´å‰ã‚’åŸºæº–ã«åˆ†å‰²ã—ãŸå ´åˆã®ã€ä¿å­˜ãƒ»é‹ç”¨ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§ã§ã™ã€‚")
    st.code("\n".join(planned_names), language="text")
else:
    st.subheader("ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆåˆ†å‰²å¾Œãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼‰")
    st.caption("â€» ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãŸã‚ã€åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã€‚")

st.markdown("---")

# =========================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================
st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

# --- ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆ (.txt) ---
buf_txt = intermediate_text.encode("utf-8")

st.download_button(
    label="â¬‡ï¸ ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ.txtï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=buf_txt,
    file_name=txt_name,
    mime="text/plain; charset=utf-8",
)

# --- ç”»åƒ ZIP ---
img_zip_buf = collect_images_as_zip(src_doc)

# uploaded_file ç”±æ¥ã® base åã‚’ session_state ã‹ã‚‰å¾©å…ƒ
_src = st.session_state.get(SS_SOURCE)
if _src:
    _base = _src.rsplit(".", 1)[0]
else:
    _base = "word_images"

zip_name = f"{_base}_images.zip"

st.download_button(
    label="â¬‡ï¸ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ ZIP ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=img_zip_buf,
    file_name=zip_name,
    mime="application/zip",
)

# â˜… rerunã§ã‚‚å¿…ãšå®šç¾©ã•ã‚Œã‚‹ã‚ˆã†ã«ã“ã“ã§å†è¨ˆç®—
if output_mode == "simple":
    mode_jp = "ç°¡ç´ "
elif output_mode == "standard":
    mode_jp = "æ¨™æº–"
else:
    mode_jp = "è©³ç´°"


# tags / originï¼ˆé‹ç”¨ã§æ¤œç´¢ãƒ»è¿½è·¡ã—ã‚„ã™ãã™ã‚‹ï¼‰
tags_json = '["word_analysis/intermediate"]'
origin = {
    "app": "text_studio_app",          # å¿…è¦ãªã‚‰å®Ÿéš›ã®APPåã«ç½®æ›
    "page": "15_wordè§£æ",
    "action": "word_intermediate_text",
    "source_filename": (uploaded_file.name if uploaded_file is not None else ""),
    "mode": mode_jp,
    "base_chapter": int(base_chapter),
}

if st.button("ğŸ“¥ ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆã‚’ Inbox ã«ä¿å­˜", type="primary"):
    try:
        # â˜… åˆ†å‰²ï¼ˆ<ã“ã“ã‹ã‚‰è¦‹å‡ºã—> ã®ç›´å‰ã§åˆ‡ã‚‹ï¼‰
        chunks = chunks_preview

        if not chunks:
            st.error("âŒ ä¿å­˜å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ã€‚")
            st.stop()

        base_fn, ext_fn = _split_filename(txt_name)

        total = len(chunks)
        saved_names: List[str] = []

        for idx, chunk in enumerate(chunks, start=1):
            if total == 1:
                fn = inbox_txt_name
            else:
                fn = f"{base_fn}_part{idx:03d}{ext_fn or '.txt'}"

            # origin ã«åˆ†å‰²æƒ…å ±ã‚’å…¥ã‚Œã‚‹ï¼ˆè¿½è·¡ç”¨ï¼‰
            origin2 = dict(origin)
            origin2.update(
                {
                    "chunk_char_limit": int(chunk_char_limit),
                    "chunk_index": idx,
                    "chunk_total": total,
                }
            )

            ingest_to_inbox(
                projects_root=PROJECTS_ROOT,
                req=IngestRequest(
                    user_sub=sub,
                    filename=fn,
                    data=chunk.encode("utf-8"),
                    tags_json=tags_json,
                    origin=origin2,
                ),
            )
            saved_names.append(fn)

        if total == 1:
            st.success("Inbox ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        else:
            st.success(f"Inbox ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆ{total}åˆ†å‰²ï¼‰ã€‚")
            st.caption("ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼š")
            st.code("\n".join(saved_names), language="text")

    except InboxNotAvailable:
        st.error("âŒ Inbox ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    except QuotaExceeded as e:
        st.error(
            f"âŒ å®¹é‡ã‚ªãƒ¼ãƒãƒ¼ã§ã™ã€‚"
            f" ç¾åœ¨={e.current} / è¿½åŠ ={e.incoming} / ä¸Šé™={e.quota}"
        )

    except IngestFailed as e:
        st.error(f"âŒ Inbox ã¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


st.caption(
    "ã“ã®ä¸­é–“ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ GPT API ã«æŠ•ã’ã‚‹ã“ã¨ã§ã€"
    "è¦‹å‡ºã—æ§‹é€ ãƒ»è¡¨ï¼ˆJSONï¼‰ãƒ»å›³æƒ…å ±ã‚’å«ã‚“ã å½¢ã§è¿½åŠ è§£æã«ä½¿ãˆã¾ã™ã€‚"
)
