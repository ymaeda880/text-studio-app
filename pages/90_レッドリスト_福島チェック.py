# -*- coding: utf-8 -*-
# pages/90_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆ_ç¦å³¶ãƒã‚§ãƒƒã‚¯.py
# _app/redlist/{fukushima, MOE, prec, chiba} ã«ã‚ã‚‹ CSV / Excel ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦å…ˆé ­Nè¡Œã‚’è¡¨ç¤º
# ã•ã‚‰ã« prec Ã—ï¼ˆç’°å¢ƒçœ å’Œåï¼ç¦å³¶çœŒ å’Œåï¼‰ã§ç…§åˆã—ã€å…ƒãƒ•ã‚¡ã‚¤ãƒ«åãƒ»ã‚·ãƒ¼ãƒˆåãƒ»è¡Œç•ªå·ã‚‚ä½µè¨˜

from __future__ import annotations
import io
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import streamlit as st
import pandas as pd
import re  # â† è¿½åŠ 


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒšãƒ¼ã‚¸è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ğŸ“š ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆï¼ˆCSV/Excelï¼‰ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆï¼ˆCSV / Excelï¼‰ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
st.caption("_app/redlist ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­è¡Œã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.markdown("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    app_root_default = Path(__file__).resolve().parents[1]
    base_dir_str = st.text_input("ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", value=str(app_root_default / "data" / "redlist"))
    n_rows = st.number_input("è¡¨ç¤ºè¡Œæ•°ï¼ˆå…ˆé ­ï¼‰", min_value=5, max_value=100, value=10, step=5)
    show_sheet_all = st.checkbox("Excelã¯å…¨ã‚·ãƒ¼ãƒˆã‚’è¡¨ç¤ºï¼ˆæ—¢å®šã¯å…ˆé ­ã‚·ãƒ¼ãƒˆã®ã¿ï¼‰", value=False)
    st.markdown("---")
    st.caption("ãƒ•ã‚©ãƒ«ãƒ€å â†’ è¡¨ç¤ºåã®å¯¾å¿œ")
    st.write({
        "fukushima": "ç¦å³¶çœŒ",
        "MOE": "ç’°å¢ƒçœ",
        "prec": "ãƒ—ãƒ¬ãƒƒã‚¯",
        "chiba": "åƒè‘‰çœŒ",
    })

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FOLDERS = [
    ("fukushima", "ç¦å³¶çœŒ"),
    ("MOE",       "ç’°å¢ƒçœ"),
    ("prec",      "ãƒ—ãƒ¬ãƒƒã‚¯"),
    ("chiba",     "åƒè‘‰çœŒ"),
]

CSV_EXTS  = {".csv"}
XLSX_EXTS = {".xlsx", ".xls"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSV / Excel èª­ã¿è¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _read_csv_head(path: Path, n: int) -> Optional[pd.DataFrame]:
    """CSVã‚’å…ˆé ­nè¡Œã ã‘èª­ã¿è¾¼ã‚€ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    for enc in ("utf-8-sig", "cp932", "utf-8"):
        try:
            return pd.read_csv(path, nrows=n, encoding=enc)
        except Exception:
            continue
    try:
        return pd.read_csv(path, nrows=n, encoding="utf-8", errors="ignore")
    except Exception as e:
        st.warning(f"CSVèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
        return None


def _read_excel_heads(path: Path, n: int, all_sheets: bool) -> List[Tuple[str, pd.DataFrame]]:
    """Excelã‚’å…ˆé ­nè¡Œèª­ã¿è¾¼ã‚€ï¼ˆä¸€èˆ¬ç”¨ï¼‰"""
    out: List[Tuple[str, pd.DataFrame]] = []
    try:
        xls = pd.ExcelFile(path)
        targets = xls.sheet_names if all_sheets else xls.sheet_names[:1]
        for s in targets:
            try:
                df = pd.read_excel(path, sheet_name=s, nrows=n)
                out.append((s, df))
            except Exception as e:
                st.warning(f"Excelã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} [{s}] ({e})")
    except Exception as e:
        st.warning(f"Excelèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
    return out


def _read_excel_heads_fukushima(path: Path, n: int, all_sheets: bool) -> List[Tuple[str, pd.DataFrame]]:
    """ç¦å³¶å°‚ç”¨ï¼šæœ€åˆã®3è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èª­ã‚€"""
    out: List[Tuple[str, pd.DataFrame]] = []
    try:
        xls = pd.ExcelFile(path)
        targets = xls.sheet_names if all_sheets else xls.sheet_names[:1]
        for s in targets:
            try:
                df = pd.read_excel(path, sheet_name=s, skiprows=3, dtype=str)
                out.append((s, df.head(n)))
            except Exception as e:
                st.warning(f"Excelã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} [{s}] ({e})")
    except Exception as e:
        st.warning(f"Excelèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
    return out


def _read_excel_all_fukushima(path: Path) -> List[Tuple[str, pd.DataFrame]]:
    """ç¦å³¶å°‚ç”¨ï¼šskiprows=3 ã§ã™ã¹ã¦ã®ã‚·ãƒ¼ãƒˆã‚’èª­ã‚€"""
    out: List[Tuple[str, pd.DataFrame]] = []
    try:
        xls = pd.ExcelFile(path)
        for s in xls.sheet_names:
            try:
                df = pd.read_excel(path, sheet_name=s, skiprows=3, dtype=str)
                out.append((s, df))
            except Exception as e:
                st.warning(f"Excelã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} [{s}] ({e})")
    except Exception as e:
        st.warning(f"Excelèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
    return out

# å…ˆé ­ã«è¿½åŠ ï¼šprecç”¨ã®2æ®µãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ãƒ©ãƒƒãƒˆåŒ–
def _flatten_prec_columns(mi) -> list[str]:
    """
    MultiIndex columns (é€šå¸¸2æ®µ) ã‚’å˜å±¤åŒ–ã€‚
    ãƒ»'Unnamed: ...' ã‚„ nan/ç©ºã¯ç„¡è¦–
    ãƒ»æ®‹ã£ãŸè¦ç´ ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã§çµåˆï¼ˆä¾‹ï¼š'é¸å®šåŸºæº– ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰'ï¼‰
    ãƒ»ä¸¡æ–¹ç©ºãªã‚‰ 'col{i}'
    """
    out = []
    for i, col in enumerate(mi.to_list()):
        parts = []
        for c in (col if isinstance(col, (list, tuple)) else [col]):
            s = "" if c is None else str(c).strip()
            if not s or s.lower() == "nan" or s.startswith("Unnamed"):
                continue
            parts.append(s)
        name = " ".join(parts).strip()
        if not name:
            name = f"col{i}"
        out.append(name)
    return out

def _read_excel_heads_prec(path: Path, n: int, all_sheets: bool):
    out = []
    try:
        xls = pd.ExcelFile(path)
        targets = xls.sheet_names if all_sheets else xls.sheet_names[:1]
        for s in targets:
            try:
                df = pd.read_excel(path, sheet_name=s, header=[0, 1], dtype=str)
                df.columns = _flatten_prec_columns(df.columns)   # â˜…ã“ã“ã ã‘å·®ã—æ›¿ãˆ
                base = [c for c in ["åˆ†é¡ç¾¤","ç›®å","ç§‘å","ç¨®å","å­¦å","å’Œå"] if c in df.columns]
                if base:
                    df = df[~df[base].isna().all(axis=1)]
                out.append((s, df.head(n)))
            except Exception as e:
                st.warning(f"Excelã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} [{s}] ({e})")
    except Exception as e:
        st.warning(f"Excelèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
    return out

def _read_excel_all_prec(path: Path):
    out = []
    try:
        xls = pd.ExcelFile(path)
        for s in xls.sheet_names:
            try:
                df = pd.read_excel(path, sheet_name=s, header=[0, 1], dtype=str)
                df.columns = _flatten_prec_columns(df.columns)   # â˜…ã“ã“ã ã‘å·®ã—æ›¿ãˆ
                base = [c for c in ["åˆ†é¡ç¾¤","ç›®å","ç§‘å","ç¨®å","å­¦å","å’Œå"] if c in df.columns]
                if base:
                    df = df[~df[base].isna().all(axis=1)]
                out.append((s, df))
            except Exception as e:
                st.warning(f"Excelã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} [{s}] ({e})")
    except Exception as e:
        st.warning(f"Excelèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¡ã‚¤ãƒ³ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base_dir = Path(base_dir_str).expanduser().resolve()
if not base_dir.exists():
    st.error(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_dir}")
    st.stop()

for folder, label in FOLDERS:
    target_dir = base_dir / folder
    st.header(f"ğŸ“ {label} â€” {folder}")
    st.caption(str(target_dir))

    if not target_dir.exists():
        st.warning(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Šã¾ã›ã‚“: {target_dir}")
        continue

    files = sorted([p for p in target_dir.iterdir() if p.is_file() and p.suffix.lower() in (CSV_EXTS | XLSX_EXTS)],
                   key=lambda p: p.name)
    if not files:
        st.info("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ï¼ˆCSV/Excelï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        continue

    for f in files:
        ext = f.suffix.lower()
        st.markdown(f"#### ğŸ“„ {f.name}")

        if ext in CSV_EXTS:
            df = _read_csv_head(f, n_rows)
            if df is not None:
                st.dataframe(df, use_container_width=True)
            else:
                st.error("èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

        elif ext in XLSX_EXTS:
            if folder == "fukushima":
                heads = _read_excel_heads_fukushima(f, n_rows, show_sheet_all)
            elif folder == "prec":
                heads = _read_excel_heads_prec(f, n_rows, show_sheet_all)
            else:
                heads = _read_excel_heads(f, n_rows, show_sheet_all)

            if not heads:
                st.error("èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                for sheet_name, df in heads:
                    st.markdown(f"- **Sheet:** `{sheet_name}`")
                    st.dataframe(df, use_container_width=True)

        else:
            st.error("æœªå¯¾å¿œã®æ‹¡å¼µå­ã§ã™ã€‚")

        st.divider()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç…§åˆå‡¦ç†ï¼ˆprec Ã— MOE Ã— ç¦å³¶ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ğŸ” ç…§åˆï¼ˆprecÃ—ç’°å¢ƒçœÃ—ç¦å³¶çœŒï¼‰")

def pick_cols(df: pd.DataFrame, wanted: Dict[str, List[str]],
              passthrough_cols: List[str] = ["_src_file", "_src_sheet", "_src_row"]) -> pd.DataFrame:
    def _norm(s: str) -> str:
        return _norm_text(s)   # â† ã“ã“ã ã‘å·®ã—æ›¿ãˆ

    cols = {}
    norm_map = {_norm(c): c for c in df.columns}
    for dst, cands in wanted.items():
        found = None
        for c in cands:
            key = _norm(c)
            if key in norm_map:
                found = norm_map[key]
                break
        if found:
            cols[dst] = df[found]
        else:
            cols[dst] = pd.Series([None] * len(df))
            st.info(f"åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: '{cands[0]}'")
    for c in passthrough_cols:
        if c in df.columns:
            cols[c] = df[c]
    return pd.DataFrame(cols)


def _read_csv_any(path: Path) -> Optional[pd.DataFrame]:
    for enc in ("utf-8-sig", "cp932", "utf-8"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return None


def _read_excel_all(path: Path) -> List[Tuple[str, pd.DataFrame]]:
    out: List[Tuple[str, pd.DataFrame]] = []
    try:
        xls = pd.ExcelFile(path)
        for s in xls.sheet_names:
            try:
                df = pd.read_excel(path, sheet_name=s)
                out.append((s, df))
            except Exception as e:
                st.warning(f"Excelã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} [{s}] ({e})")
    except Exception as e:
        st.warning(f"Excelèª­ã¿è¾¼ã¿å¤±æ•—: {path.name} ({e})")
    return out


def _add_src_info(df: pd.DataFrame, *, file: Path, sheet: Optional[str] = None, row_offset: int = 0) -> pd.DataFrame:
    g = df.copy()
    g["_src_file"] = file.name
    if sheet is not None:
        g["_src_sheet"] = sheet
    g["_src_row"] = g.index + 2 + row_offset
    return g


# ---- æ­£è¦åŒ–ï¼ˆæœ€å°é™ï¼šç©ºç™½é™¤å»/å…¨è§’â†’åŠè§’/ãƒ­ãƒ¼ãƒæ•°å­—â†’è‹±å­—/Iã¨Aæ··åœ¨è£œæ­£/å°æ–‡å­—åŒ–ï¼‰----
def _norm_text(s: Optional[str]) -> str:
    if s is None:
        return ""
    t = str(s)

    # ç©ºç™½é¡ã‚’å‰Šé™¤
    for ch in ("\u3000", "\u00A0", "\u200B", "\uFEFF", "\n", "\t"):
        t = t.replace(ch, " ")
    t = t.strip().replace(" ", "")

    # å…¨è§’è‹±æ•° â†’ åŠè§’
    z = "ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
    h = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789"
    t = t.translate(str.maketrans(z, h))

    # ãƒ­ãƒ¼ãƒæ•°å­— â†’ ãƒ©ãƒ†ãƒ³ï¼ˆâ… /â…¡/â…¢â€¦ï¼‰
    roman_map = {
        "â… ": "I", "â…¡": "II", "â…¢": "III", "â…£": "IV", "â…¤": "V",
        "â…¥": "VI", "â…¦": "VII", "â…§": "VIII", "â…¨": "IX", "â…©": "X",
        "â…ª": "XI", "â…«": "XII",
    }
    for k, v in roman_map.items():
        t = t.replace(k, v)

    # I ã¨ A ã®å…¨è§’/åŠè§’æ··åœ¨è£œæ­£ï¼ˆä¾‹ï¼šâ… ï¼¡ â†’ IA, Iï¼¡ â†’ IA, â… A â†’ IAï¼‰
    t = re.sub(r"([IVX]+)[ï¼¡A]", lambda m: m.group(1) + "A", t, flags=re.IGNORECASE)

    return t.lower()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘  prec
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘  precï¼ˆExcelï¼‰â†’ ã€Œç¨®åã€ã€Œç’°å¢ƒçœãƒ¬ãƒƒãƒ‰ã€ã€ŒçœŒãƒ¬ãƒƒãƒ‰ã€
prec_dir = base_dir / "prec"
prec_frames = []
if prec_dir.exists():
    for p in sorted(prec_dir.iterdir()):
        if p.suffix.lower() in {".xlsx", ".xls"}:
            # âŒ æ—§: for sheet_name, df in _read_excel_all(p):
            for sheet_name, df in _read_excel_all_prec(p):  # âœ… å°‚ç”¨é–¢æ•°ã«
                df = _add_src_info(df, file=p, sheet=sheet_name)
                sub = pick_cols(df, {
                    "precç¨®å": ["ç¨®å", "å’Œå"],  # â† ç¨®å or å’Œå ã©ã¡ã‚‰ã§ã‚‚æ‹¾ãˆã‚‹ã‚ˆã†ã«
                    "precç’°å¢ƒçœãƒ¬ãƒƒãƒ‰": ["ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰", "é¸å®šåŸºæº– ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰"],
                    "precçœŒãƒ¬ãƒƒãƒ‰":   ["çœŒãƒ¬ãƒƒãƒ‰",   "é¸å®šåŸºæº– çœŒãƒ¬ãƒƒãƒ‰"],
                })
                prec_frames.append(sub)

else:
    st.warning(f"prec ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prec_dir}")

              

prec_df = pd.concat(prec_frames, ignore_index=True) if prec_frames else pd.DataFrame(columns=["precç¨®å","precç’°å¢ƒçœãƒ¬ãƒƒãƒ‰","precçœŒãƒ¬ãƒƒãƒ‰"])
prec_df["_ord"] = range(len(prec_df))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘¡ MOEï¼ˆç’°å¢ƒçœï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
moe_dir = base_dir / "MOE"
moe_frames = []
if moe_dir.exists():
    for p in sorted(moe_dir.iterdir()):
        if p.suffix.lower() == ".csv":
            df = _read_csv_any(p)
            if df is not None:
                df = _add_src_info(df, file=p)
                sub = pick_cols(df, {"ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼": ["ã‚«ãƒ†ã‚´ãƒªãƒ¼"], "ç’°å¢ƒçœå’Œå": ["å’Œå"]})
                moe_frames.append(sub)
moe_df = pd.concat(moe_frames, ignore_index=True) if moe_frames else pd.DataFrame(columns=["ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼","ç’°å¢ƒçœå’Œå"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘¢ ç¦å³¶
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fuku_dir = base_dir / "fukushima"
fuku_frames = []
if fuku_dir.exists():
    for p in sorted(fuku_dir.iterdir()):
        if p.suffix.lower() in {".xlsx", ".xls"}:
            for sheet_name, df in _read_excel_all_fukushima(p):
                df = _add_src_info(df, file=p, sheet=sheet_name, row_offset=3)
                sub = pick_cols(df, {
                    "ç¦å³¶çœŒå’Œå": ["å’Œå"],
                    "ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼": ["ãµãã—ã¾RL2024ã‚«ãƒ†ã‚´ãƒªãƒ¼", "ç¦å³¶RL2024ã‚«ãƒ†ã‚´ãƒªãƒ¼", "RL2024ã‚«ãƒ†ã‚´ãƒªãƒ¼"],
                })
                fuku_frames.append(sub)
fuku_df = pd.concat(fuku_frames, ignore_index=True) if fuku_frames else pd.DataFrame(columns=["ç¦å³¶çœŒå’Œå","ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç…§åˆå‡¦ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
moe_map = {}
if not moe_df.empty:
    for _, r in moe_df.dropna(subset=["ç’°å¢ƒçœå’Œå"]).iterrows():
        moe_map[_norm_text(r["ç’°å¢ƒçœå’Œå"])] = {"cat": r.get("ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼"), "file": r.get("_src_file"), "row": r.get("_src_row")}

fuku_map = {}
if not fuku_df.empty:
    for _, r in fuku_df.dropna(subset=["ç¦å³¶çœŒå’Œå"]).iterrows():
        fuku_map[_norm_text(r["ç¦å³¶çœŒå’Œå"])] = {"cat": r.get("ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼"), "file": r.get("_src_file"), "sheet": r.get("_src_sheet"), "row": r.get("_src_row")}

rows = []
for _, r in prec_df.iterrows():
    name = r.get("precç¨®å")
    key = _norm_text(name)
    moe_hit, fuku_hit = moe_map.get(key), fuku_map.get(key)
    if moe_hit or fuku_hit:
        rows.append({
            "precç¨®å": name,
            "precç’°å¢ƒçœãƒ¬ãƒƒãƒ‰": r.get("precç’°å¢ƒçœãƒ¬ãƒƒãƒ‰"),
            "precçœŒãƒ¬ãƒƒãƒ‰": r.get("precçœŒãƒ¬ãƒƒãƒ‰"),
            "ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼": moe_hit.get("cat") if moe_hit else None,
            "MOEå…ƒãƒ•ã‚¡ã‚¤ãƒ«": moe_hit.get("file") if moe_hit else None,
            "MOEè¡Œç•ªå·": moe_hit.get("row") if moe_hit else None,
            "ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼": fuku_hit.get("cat") if fuku_hit else None,
            "ç¦å³¶å…ƒãƒ•ã‚¡ã‚¤ãƒ«": fuku_hit.get("file") if fuku_hit else None,
            "ç¦å³¶å…ƒã‚·ãƒ¼ãƒˆ": fuku_hit.get("sheet") if fuku_hit else None,
            "ç¦å³¶è¡Œç•ªå·": fuku_hit.get("row") if fuku_hit else None,
            "precå…ƒãƒ•ã‚¡ã‚¤ãƒ«": r.get("_src_file"),
            "precå…ƒã‚·ãƒ¼ãƒˆ": r.get("_src_sheet"),
            "precè¡Œç•ªå·": r.get("_src_row"),
            "_ord": r.get("_ord", 0),
        })

# èµ°æŸ»çµæœ â†’ DataFrame
result_df = pd.DataFrame(rows)

# _ord ãŒã‚ã‚Œã°ä¸¦ã¹æ›¿ãˆï¼†å‰Šé™¤
if "_ord" in result_df.columns:
    result_df = result_df.sort_values("_ord").drop(columns=["_ord"], errors="ignore")


# ã€ˆè¿½åŠ /ä¿®æ­£ã€‰ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ã®ç”Ÿæˆã¨æ¯”è¼ƒï¼ˆç’°å¢ƒçœï¼ç¦å³¶çœŒï¼‰
def _extract_moe_code(s: Optional[str]) -> Optional[str]:
    """ä¾‹: 'çµ¶æ»…å±æƒ§IAé¡ï¼ˆCRï¼‰' â†’ 'CR'ã€‚ä¸¸æ‹¬å¼§/å…¨è§’æ‹¬å¼§ã®ä¸­ã®è‹±å­—1-3æ–‡å­—ã‚’æŠ½å‡ºã€‚ãªã‘ã‚Œã° Noneã€‚"""
    if s is None or (isinstance(s, float) and pd.isna(s)):
        return None
    m = re.search(r"[ï¼ˆ(]\s*([A-Za-z]{1,3})\s*[)ï¼‰]", str(s))
    return m.group(1).upper() if m else None

def _to_fukushima_code(s: Optional[str]) -> Optional[str]:
    """
    ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼ã®æ—¥æœ¬èªè¡¨è¨˜â†’è¨˜å·ã«å¤‰æ›ã€‚
    å®Œå…¨ä¸€è‡´ã§åˆ¤å®šã™ã‚‹ã€‚
    å¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«: çµ¶æ»…å±æƒ§â… ï¼¡é¡â†’CRï¼Œæº–çµ¶æ»…å±æƒ§â†’NTï¼Œæƒ…å ±ä¸è¶³â†’DDï¼Œçµ¶æ»…â†’EX
    ä¸Šè¨˜ä»¥å¤–ã¯ 'å¤‰æ›è¦å‰‡ä¸æ˜' ã‚’è¿”ã™ã€‚
    """
    if s is None or (isinstance(s, float) and pd.isna(s)):
        return None

    key = _norm_text(s)  # ç©ºç™½é™¤å»ãƒ»å…¨è§’â†’åŠè§’ãƒ»å°æ–‡å­—åŒ–

    mapping = {
        "çµ¶æ»…å±æƒ§iaé¡": "CR",
        "çµ¶æ»…å±æƒ§iié¡": "VU",
        "æº–çµ¶æ»…å±æƒ§":   "NT",
        "æƒ…å ±ä¸è¶³":     "DD",
        "çµ¶æ»…":        "EX",
        "çµ¶æ»…å±æƒ§ibé¡": "EN",
    }

    # å®Œå…¨ä¸€è‡´ã§åˆ¤å®š
    return mapping.get(key, "å¤‰æ›è¦å‰‡ä¸æ˜")

def _norm_code(x) -> Optional[str]:
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return None
    return str(x).strip().upper()

def _cmp_codes(left, right) -> Optional[str]:
    """
    left/right ã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒã€‚
    ä¸¡æ–¹ None â†’ None
    ã©ã¡ã‚‰ã‹ã ã‘ None â†’ 'ä¸ä¸€è‡´'
    ä¸¡æ–¹ã‚ã‚Šä¸€è‡´ â†’ 'ä¸€è‡´'
    ä¸¡æ–¹ã‚ã‚Šä¸ä¸€è‡´ â†’ 'ä¸ä¸€è‡´'
    """
    lx = _norm_code(left)
    rx = _norm_code(right)
    if lx is None and rx is None:
        return None
    return "ä¸€è‡´" if (lx is not None and rx is not None and lx == rx) else "ä¸ä¸€è‡´"

if not result_df.empty:
    # ï¼ˆï¼‘ï¼‰ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·
    result_df["ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = result_df["ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼"].apply(_extract_moe_code)

    # ï¼ˆï¼’ï¼‰ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·
    result_df["ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = result_df["ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼"].apply(_to_fukushima_code)

    # ï¼ˆï¼“ï¼‰ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰æ¯”è¼ƒï¼ˆprecç’°å¢ƒçœãƒ¬ãƒƒãƒ‰ vs ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ï¼‰â€” Noneå¯¾å¿œ
    result_df["ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"] = [
        _cmp_codes(x, y) for x, y in zip(result_df["precç’°å¢ƒçœãƒ¬ãƒƒãƒ‰"], result_df["ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"])
    ]

    # ï¼ˆï¼”ï¼‰ç¦å³¶çœŒãƒ¬ãƒƒãƒ‰æ¯”è¼ƒï¼ˆprecçœŒãƒ¬ãƒƒãƒ‰ vs ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·ï¼‰â€” Noneå¯¾å¿œ
    result_df["ç¦å³¶çœŒãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"] = [
        _cmp_codes(x, y) for x, y in zip(result_df["precçœŒãƒ¬ãƒƒãƒ‰"], result_df["ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"])
    ]

    # é›†è¨ˆï¼ˆNone ã¯ NaN ã¨ã—ã¦å…¥ã‚‹ã®ã§ isna() ã§ã‚«ã‚¦ãƒ³ãƒˆï¼‰
    moe_counts = {
        "ä¸€è‡´": int((result_df["ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"] == "ä¸€è‡´").sum()),
        "ä¸ä¸€è‡´": int((result_df["ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"] == "ä¸ä¸€è‡´").sum()),
        "None": int(result_df["ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"].isna().sum()),
    }
    fuk_counts = {
        "ä¸€è‡´": int((result_df["ç¦å³¶çœŒãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"] == "ä¸€è‡´").sum()),
        "ä¸ä¸€è‡´": int((result_df["ç¦å³¶çœŒãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"] == "ä¸ä¸€è‡´").sum()),
        "None": int(result_df["ç¦å³¶çœŒãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ"].isna().sum()),
    }
    fuk_unknown = int((result_df["ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] == "å¤‰æ›è¦å‰‡ä¸æ˜").sum())


# â”€â”€ ã“ã“ã‹ã‚‰çµæœè¡¨ç¤ºï¼ˆ1å›ã ã‘ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("âœ… ç…§åˆçµæœï¼ˆä¸€è‡´ã—ãŸã‚‚ã®ï¼‰")
if result_df.empty:
    st.info("ä¸€è‡´ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    st.dataframe(result_df, use_container_width=True)

    buf = io.StringIO()
    result_df.to_csv(buf, index=False)
    st.download_button(
        "ğŸ“¥ ç…§åˆçµæœã‚’CSVã§ä¿å­˜",
        data=buf.getvalue().encode("utf-8-sig"),
        file_name="redlist_match_result.csv",
        mime="text/csv",
        use_container_width=True,
    )

# â† ã“ã“ã«é›†è¨ˆè¡¨ç¤ºã‚’è¿½åŠ 
st.markdown("#### ğŸ§® é›†è¨ˆ")
st.write({
    "ç’°å¢ƒçœãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ": {
        "ä¸€è‡´": int(moe_counts.get("ä¸€è‡´", 0)),
        "ä¸ä¸€è‡´": int(moe_counts.get("ä¸ä¸€è‡´", 0)),
         "None": moe_counts["None"],
    },
    "ç¦å³¶çœŒãƒ¬ãƒƒãƒ‰æ¯”è¼ƒ": {
        "ä¸€è‡´": int(fuk_counts.get("ä¸€è‡´", 0)),
        "ä¸ä¸€è‡´": int(fuk_counts.get("ä¸ä¸€è‡´", 0)),
         "None": fuk_counts["None"],
        "å¤‰æ›è¦å‰‡ä¸æ˜": fuk_unknown,
    },
})

st.caption(
    f"precå…¥åŠ›: {len(prec_df)} ä»¶ / ç’°å¢ƒçœ: {len(moe_df)} ä»¶ / "
    f"ç¦å³¶çœŒ: {len(fuku_df)} ä»¶ / ä¸€è‡´: {len(result_df)} ä»¶"
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
