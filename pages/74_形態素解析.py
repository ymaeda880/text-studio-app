# -*- coding: utf-8 -*-
# pages/74_å½¢æ…‹ç´ è§£æ.py
from __future__ import annotations

import io
from collections import Counter
from typing import List, Tuple

import pandas as pd
import streamlit as st

# ============== ãƒšãƒ¼ã‚¸è¨­å®š ==============
st.set_page_config(page_title="ğŸ§© å½¢æ…‹ç´ è§£æï¼ˆåè©ã®ã¿æŠ½å‡ºï¼‰", page_icon="ğŸ§©", layout="wide")
st.title("ğŸ§© 52_ å½¢æ…‹ç´ è§£æï¼ˆåè©ã®ã¿æŠ½å‡ºï¼‰")
st.caption("textarea ã«å…¥åŠ›ã—ãŸæ–‡ç« ã¸å½¢æ…‹ç´ è§£æã‚’ã‹ã‘ã€åè©ã®ã¿ã‚’æŠ½å‡ºã—ã¾ã™ï¼ˆJanome ä½¿ç”¨ï¼‰ã€‚")

# ============== Janome ã®èª­ã¿è¾¼ã¿ ==============
try:
    from janome.tokenizer import Tokenizer  # pip install janome
    tokenizer = Tokenizer()
except Exception as e:
    st.error(
        "Janome ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `pip install janome` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n\n"
        f"è©³ç´°: {e}"
    )
    st.stop()

# ============== ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ ==============
with st.sidebar:
    st.subheader("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    min_len = st.number_input("æœ€å°æ–‡å­—æ•°ï¼ˆåè©ï¼‰", min_value=1, max_value=20, value=2, step=1)
    uniq_only = st.checkbox("é‡è¤‡ã‚’é™¤ãï¼ˆåè©ã®ä¸€è¦§ï¼‰", value=False)
    use_base = st.checkbox("åŸå½¢ã§é›†è¨ˆï¼ˆä¾‹ï¼šè¤‡åˆèªã®æºã‚Œã‚’æŠ‘ãˆã‚‹ï¼‰", value=True)
    show_pos_detail = st.checkbox("å“è©ç´°åˆ†é¡ã‚’è¡¨ç¤º", value=False)
    st.markdown("---")
    stopwords_raw = st.text_area(
        "ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šï¼‰", value="ã“ã¨\nã‚‚ã®\nãã‚Œ\nã“ã‚Œ\nãŸã‚\nã‚ˆã†\nã¨ã“ã‚",
        height=120
    )
    stopwords = {w.strip() for w in stopwords_raw.splitlines() if w.strip()}

# ============== å…¥åŠ›ã‚¨ãƒªã‚¢ ==============
sample = "ç”ŸæˆAIã®æ´»ç”¨ãŒé€²ã‚€ä¸­ã§ã€ã‚¬ãƒãƒŠãƒ³ã‚¹ã€è‘—ä½œæ¨©ã€å€‹äººæƒ…å ±ä¿è­·ã€é€æ˜æ€§ã®ç¢ºä¿ãŒé‡è¦ã¨ãªã£ã¦ã„ã‚‹ã€‚ä¼æ¥­ã¯ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã ã‘ã§ãªãã€èª¬æ˜å¯èƒ½æ€§ã‚„ç›£æŸ»å¯èƒ½æ€§ã«ã‚‚é…æ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚"
text = st.text_area("ğŸ“¥ è§£æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„", value=sample, height=180)

# ============== è§£æãƒœã‚¿ãƒ³ ==============
run = st.button("ğŸš€ è§£æã™ã‚‹")

# ============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==============
def extract_nouns(s: str) -> List[Tuple[str, str, str, int]]:
    """
    æ–‡ç« ã‹ã‚‰åè©ã®ã¿ã‚’æŠ½å‡ºã€‚
    Returns: List of (surface, base, pos, start_index)
    """
    res: List[Tuple[str, str, str, int]] = []
    ofs = 0
    # âœ… stream=True ã¯å‰Šé™¤ã€‚é€šå¸¸ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§OKã€‚
    for token in tokenizer.tokenize(s):
        surf = token.surface
        base = token.base_form if token.base_form != "*" else surf
        pos = token.part_of_speech  # ä¾‹: åè©,ä¸€èˆ¬,*,*
        start_idx = ofs
        ofs += len(surf)
        if pos.startswith("åè©"):
            res.append((surf, base, pos, start_idx))
    return res


def normalize_token(t: str) -> str:
    return t.replace("\u3000", " ").strip()

# ============== å®Ÿè¡Œ ==============
if run:
    s = (text or "").strip()
    if not s:
        st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    tokens = extract_nouns(s)

    # æœ€å°é•·ãƒ»ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    filtered = []
    for surf, base, pos, idx in tokens:
        key = base if use_base else surf
        key = normalize_token(key)
        if len(key) < min_len:
            continue
        if key in stopwords:
            continue
        filtered.append((surf, base, pos, idx, key))

    # ===== åè©ä¸€è¦§ =====
    st.subheader("ğŸ“ åè©ä¸€è¦§")
    if uniq_only:
        seen = set()
        uniq_rows = []
        for surf, base, pos, idx, key in filtered:
            if key in seen:
                continue
            seen.add(key)
            uniq_rows.append((surf, base, pos, idx))
        rows = uniq_rows
    else:
        rows = [(surf, base, pos, idx) for surf, base, pos, idx, _ in filtered]

    cols = ["è¡¨å±¤å½¢", "åŸå½¢", "å“è©", "é–‹å§‹ä½ç½®"]
    if not show_pos_detail:
        # å“è©ã®æœ€åˆã®ã‚«ãƒ†ã‚´ãƒªã ã‘ï¼ˆ"åè©"ï¼‰ã«å˜ç´”åŒ–
        rows_disp = [(a, b, (c.split(",")[0] if c else ""), d) for (a, b, c, d) in rows]
    else:
        rows_disp = rows

    df_tokens = pd.DataFrame(rows_disp, columns=cols)
    st.dataframe(df_tokens, use_container_width=True, height=320)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåè©ä¸€è¦§ï¼‰
    buf_tok = io.StringIO()
    df_tokens.to_csv(buf_tok, index=False, encoding="utf-8-sig")
    st.download_button(
        "ğŸ“¥ åè©ä¸€è¦§ã‚’CSVã§ä¿å­˜",
        data=buf_tok.getvalue().encode("utf-8-sig"),
        file_name="nouns_list.csv",
        mime="text/csv",
        use_container_width=True,
    )

    # ===== å‡ºç¾é »åº¦ =====
    st.subheader("ğŸ“Š å‡ºç¾é »åº¦ï¼ˆä¸Šä½ï¼‰")
    counter = Counter([key for *_, key in filtered])
    freq_rows = counter.most_common()
    df_freq = pd.DataFrame(freq_rows, columns=["åè©ï¼ˆã‚­ãƒ¼ï¼‰", "é »åº¦"])
    st.dataframe(df_freq, use_container_width=True, height=320)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé »åº¦è¡¨ï¼‰
    buf_freq = io.StringIO()
    df_freq.to_csv(buf_freq, index=False, encoding="utf-8-sig")
    st.download_button(
        "ğŸ“¥ é »åº¦è¡¨ã‚’CSVã§ä¿å­˜",
        data=buf_freq.getvalue().encode("utf-8-sig"),
        file_name="nouns_freq.csv",
        mime="text/csv",
        use_container_width=True,
    )

    # ===== ã¡ã‚‡ã„å¯è¦–åŒ–ï¼ˆä¸Šä½20ä»¶ã®æ£’ã‚°ãƒ©ãƒ•ï¼‰ =====
    try:
        import matplotlib.pyplot as plt

        topn = min(20, len(df_freq))
        if topn > 0:
            st.subheader("ğŸ“ˆ é »åº¦ï¼ˆä¸Šä½20ï¼‰")
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.barh(df_freq["åè©ï¼ˆã‚­ãƒ¼ï¼‰"].iloc[:topn][::-1], df_freq["é »åº¦"].iloc[:topn][::-1])
            ax.set_xlabel("é »åº¦")
            ax.set_ylabel("åè©ï¼ˆã‚­ãƒ¼ï¼‰")
            ax.set_title("åè©ã®å‡ºç¾é »åº¦ï¼ˆä¸Šä½20ï¼‰")
            st.pyplot(fig)
    except Exception as e:
        st.info(f"ç°¡æ˜“ã‚°ãƒ©ãƒ•ã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆmatplotlib æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã©ï¼‰ã€‚è©³ç´°: {e}")

else:
    st.info("å·¦ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«æ–‡ç« ã‚’å…¥åŠ›ã—ã€[ğŸš€ è§£æã™ã‚‹] ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
