# -*- coding: utf-8 -*-
# pages/14_å›³è¡¨ãƒã‚§ãƒƒã‚¯.pyï¼ˆæ”¹è‰¯ç‰ˆï¼šå¼·èª¿è¡¨ç¤ºãƒ»excerptä»˜ã + ç•ªå·å“è³ª/çªãåˆã‚ã› + XLSXå‡ºåŠ›ï¼‰
from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import io, re, tempfile

import streamlit as st
import pandas as pd

# === å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆlib/ï¼‰ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ===
from lib.text_normalizer import (
    z2h_numhy,
    normalize_strict,
    HY,
)
from lib.toc_check.toc_segments import (
    pdf_to_text_per_page,
    extract_single_page_label,
)
from lib.chart_check.explanation import render_numbering_logic_expander  # â˜…è¿½åŠ 
from lib.chart_check.helpers import (
    base_key,
    index_pages_by_key,
    ref_aggregate_for_view,
    caption_info_first_by_key,
    aggregate_ref_info,
    make_crosscheck_rows,
    protect_for_excel_csv,
    protect_for_excel_xlsx,
)

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ–¼ï¸ å›³è¡¨ æŠ½å‡ºï¼ˆè¡Œé ­/åŠ©è©/å¥ç‚¹ãƒ«ãƒ¼ãƒ« + é ãƒ©ãƒ™ãƒ«ï¼‰", page_icon="ğŸ–¼ï¸", layout="wide")
st.title("ğŸ–¼ï¸ å›³è¡¨ã®å‚ç…§ç…§åˆãƒã‚§ãƒƒã‚¯")
st.caption(
    "å›³è¡¨ç•ªå·ã¨æœ¬æ–‡ã®å‚ç…§ã«å¯¾ã—ã¦ç…§åˆã—ã¦ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã¾ã™ã€‚"
    "ï¼ˆï¼‘ï¼‰å›³è¡¨ãŒã‚ã‚‹ã®ã«æœ¬æ–‡ã§å‚ç…§ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ï¼Œï¼ˆï¼’ï¼‰æœ¬æ–‡ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã®ã«å›³è¡¨ã®æœ¬ä½“ãŒãªã„ã‚‚ã®,"
    "ï¼ˆï¼“ï¼‰å›³è¡¨ç•ªå·ãŒé‡è¤‡ã—ã¦ã„ã‚‹ã‚‚ã®ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ï¼"
)
st.caption("AIã¯ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ï¼å®‰å¿ƒã—ã¦pdfã‚’ä¸¸ã”ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼")

# ğŸ‘‡ åˆ©ç”¨è€…å‘ã‘ã®ãƒ­ã‚¸ãƒƒã‚¯èª¬æ˜ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
render_numbering_logic_expander()

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=None)
run = st.button("â–¶ è§£æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

# --- å®Ÿè¡Œãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¦ã„ãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„ ---
if not run:
    st.stop()

# --- ãƒ•ã‚¡ã‚¤ãƒ«æœªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
if uploaded is None:
    st.warning("PDF ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.pdfï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼š.pdf ä»¥å¤–ãªã‚‰ã‚¨ãƒ©ãƒ¼ï¼‰---

suffix = Path(uploaded.name).suffix.lower()
if suffix != ".pdf":
    st.error("PDF ä»¥å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚PDFï¼ˆ.pdfï¼‰ã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

with st.sidebar:
    st.markdown("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    ctx_chars  = st.slider("å‚ç…§ã®å‰å¾Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°ï¼ˆexcerpt ç”¨ï¼‰", 10, 300, 60, 5)
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰ã‚’è¡¨ç¤º", value=False)


# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
with tempfile.TemporaryDirectory() as td:
    pdf_path = Path(td) / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")

# =========================
# å›³è¡¨æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸å›ºæœ‰ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
# =========================
# DOT = r"[\.ï¼ãƒ»ï½¥]"
# NUM_ZH = r"[0-9ï¼-ï¼™]+"
# NUM_TOKEN = rf"""
# (
#     # ä¾‹ï¼š4.2-1(1/6), 4.2-1ï¼ˆ1ï¼ï¼–ï¼‰ ãªã©
#     {NUM_ZH}
#     (?:\s*(?:{DOT}|{HY})\s*{NUM_ZH})*
#     (?:\s*[ï¼ˆ(]?\s*{NUM_ZH}\s*[\/ï¼]\s*{NUM_ZH}\s*[ï¼‰)])?
#     |
#     # ä¾‹ï¼š(1), ï¼ˆï¼’ï¼‰
#     [ï¼ˆ(]\s*{NUM_ZH}\s*[ï¼‰)]
# )
# """

DOT = r"[\.ï¼ãƒ»ï½¥]"
NUM_ZH = r"[0-9ï¼-ï¼™]+"
NUM_TOKEN = rf"""
(
    # ä¾‹ï¼š4.2-1(1/6), 4.2-1ï¼ˆ1ï¼ï¼–ï¼‰, 2.2.1-3(1) ãªã©
    {NUM_ZH}
    (?:\s*(?:{DOT}|{HY})\s*{NUM_ZH})*
    (?:\s*[ï¼ˆ(]\s*
        (?:                               # æœ«å°¾æ‹¬å¼§ï¼šç¶šã(i/n) or ã‚µãƒ–ç•ªå·(i)
            {NUM_ZH}\s*[\/ï¼]\s*{NUM_ZH}  # ç¶šãï¼š1/3
            |
            {NUM_ZH}                      # ã‚µãƒ–ç•ªå·ï¼š1
        )
    \s*[ï¼‰)]\s*)?
    |
    # ä¾‹ï¼š(1), ï¼ˆï¼’ï¼‰
    [ï¼ˆ(]\s*{NUM_ZH}\s*[ï¼‰)]
)
"""


EXTRACT_RE = re.compile(
    rf"(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})",
    re.X
)


def canon_num(num: str) -> str:
    # å…¨è§’ â†’ åŠè§’
    s = num.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰", "0123456789()"))

    # ãƒ‰ãƒƒãƒˆé¡ â†’ "."
    s = re.sub(DOT, ".", s)

    # ãƒã‚¤ãƒ•ãƒ³é¡ â†’ "-"
    s = re.sub(HY, "-", s)

    # "." ã¨ "-" ã®å‰å¾Œã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
    s = re.sub(r"\s*\.\s*", ".", s)
    s = re.sub(r"\s*-\s*", "-", s)

    # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ â†’ 1 å€‹
    s = re.sub(r"[ \u3000]+", " ", s)

    # æ‹¬å¼§å†…ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
    s = re.sub(r"\(\s*", "(", s)
    s = re.sub(r"\s*\)", ")", s)

    return s.strip()


def canon_label(kind: str, num: str) -> str:
    return f"{kind}{canon_num(num)}"


try:
    import regex as re2
except Exception:
    re2 = re

PARTICLES_RE = re2.compile(r"(?:ã«|ã‚’|ã¯|ã¸|ã§|ã¨|ã®|ãªã©|ç­‰|ã¾ãŸã¯|åˆã¯|ãŠã‚ˆã³|åŠã³|ã‹ã¤)")


# ===== è¡ŒæŠ½å‡ºè£œåŠ©é–¢æ•° =====
def extract_line_covering_match(full: str, start: int, end: int) -> Tuple[int, str, int, int]:
    """ãƒãƒƒãƒã‚’å¿…ãšå«ã‚€è¡Œï¼ˆæ”¹è¡Œã¾ãŸãå¯¾å¿œï¼‰ã‚’è¿”ã™"""
    line_start = full.rfind("\n", 0, start)
    line_start = 0 if line_start == -1 else line_start + 1
    line_end = full.find("\n", end)
    if line_end == -1:
        line_end = len(full)
    line_txt = full[line_start:line_end].rstrip("\r\n")
    approx_lineno = full.count("\n", 0, line_start) + 1
    return approx_lineno, line_txt, line_start, line_end


# ===== ãƒšãƒ¼ã‚¸å˜ä½ã®æŠ½å‡º =====
def judge_hits_in_page(page_text: str, ctx: int) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    captions, refs = [], []
    full = page_text.replace("\r\n", "\n").replace("\r", "\n")

    for m in EXTRACT_RE.finditer(full):
        kind, num, raw = m.group("kind"), m.group("num"), m.group(0)
        lineno, line_txt, line_start, line_end = extract_line_covering_match(full, m.start(), m.end())

        is_line_head = (full[line_start:m.start()].strip() == "")
        rel_end = (m.start() - line_start) + len(raw)
        after_on_line = line_txt[rel_end:] if rel_end <= len(line_txt) else ""
        particle_follow = bool(re2.match(rf"\s*{PARTICLES_RE.pattern}", after_on_line))
        has_period = ("ã€‚" in line_txt)
        is_reference = (not is_line_head) or particle_follow or has_period

        # å¼·èª¿ã¨excerpt
        highlighted = line_txt.replace(raw, f"âŸª{raw}âŸ«", 1)
        left  = max(0, m.start() - ctx)
        right = min(len(full), m.end() + ctx)
        excerpt = full[left:m.start()] + f"âŸª{raw}âŸ«" + full[m.end():right]

        if is_reference:
            refs.append({
                "è¡Œç•ªå·": lineno,
                "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": raw.strip(),
                "å›³è¡¨ç¨®é¡": kind,
                "å›³è¡¨ç•ªå·": f"{kind}{z2h_numhy(num)}",
                "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
                "excerpt": excerpt,
                "è¡Œãƒ†ã‚­ã‚¹ãƒˆ": line_txt,
                "è¡Œãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)": highlighted,
                "åˆ¤å®š": "å‚ç…§",
                "rule(ç†ç”±)": (
                    "è¡Œé ­ã§ãªã„â†’å‚ç…§" if not is_line_head else
                    ("ç›´å¾ŒãŒåŠ©è©/æ¥ç¶šèªâ†’å‚ç…§" if particle_follow else "è¡Œã«å¥ç‚¹ã‚ã‚Šâ†’å‚ç…§")
                ),
            })
        else:
            title = re.sub(r"^[\s:ï¼š.\-ï¼ã€ãƒ»]+", "", after_on_line).strip()
            captions.append({
                "è¡Œç•ªå·": lineno,
                "å›³è¡¨ç¨®é¡": kind,
                "å›³è¡¨ç•ªå·": f"{kind}{z2h_numhy(num)}",
                "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
                "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«": title,
                "matched_line": line_txt,
                "matched_line(å¼·èª¿)": highlighted,
                "excerpt": excerpt,
                "åˆ¤å®š": "ã‚¿ã‚¤ãƒˆãƒ«",
                "rule(ç†ç”±)": "ãã®ä»–â†’ã‚¿ã‚¤ãƒˆãƒ«",
            })
    return captions, refs


# =========================
# å…¨ãƒšãƒ¼ã‚¸èµ°æŸ»
# =========================
page_labels, per_page_rows = [], []
for i, ptxt in enumerate(pages_text, start=1):
    label, matched = extract_single_page_label(ptxt)
    page_labels.append(label)
    per_page_rows.append({
        "pdf_page": i,
        "page_label": label or "-",
        "matched_line": matched or "-",
        "has_label": label is not None,
    })
df_per_page_labels = pd.DataFrame(per_page_rows)

caption_rows, ref_rows = [], []
for i, ptxt in enumerate(pages_text, start=1):
    page_label = page_labels[i-1] if i-1 < len(page_labels) and page_labels[i-1] else "-"
    captions, refs = judge_hits_in_page(ptxt, ctx=ctx_chars)
    for h in captions:
        caption_rows.append({"pdf_page": i, "page_label": page_label, **h})
    for r in refs:
        ref_rows.append({"pdf_page": i, "page_label": page_label, **r})

df_captions = pd.DataFrame(caption_rows)
df_refs     = pd.DataFrame(ref_rows)

# =========================
# è¡¨ç¤º
# =========================
st.subheader("ğŸ“‘ å„ãƒšãƒ¼ã‚¸ã®é ãƒ©ãƒ™ãƒ«ï¼ˆ1é =é«˜ã€…1ï¼‰")
st.dataframe(df_per_page_labels, use_container_width=True)

st.subheader("ğŸ–¼ï¸ å›³/è¡¨/å›³è¡¨ è¦‹å‡ºã—ï¼ˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
st.dataframe(df_captions, use_container_width=True)

# =========================
# å›³è¡¨ç•ªå·ã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡ãƒ»é£›ã³ãƒ»é–‹å§‹ç•ªå·ãƒ»ç¶šãåˆ¤å®šï¼‰
# =========================
st.markdown("### âœ… å›³è¡¨ç•ªå·ã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡ãƒ»é£›ã³ãƒ»é–‹å§‹ï¼‰")

if df_captions.empty or "å›³è¡¨ç•ªå·" not in df_captions.columns:
    st.info("å›³è¡¨ç•ªå·ã®æ¤œæŸ»å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆdf_captions ãŒç©ºã§ã™ï¼‰ã€‚")
    cont_rows, dup_rows = [], []
else:

    # æœ«å°¾ã®ï¼ˆä»»æ„ãƒ©ãƒ™ãƒ« + i/nï¼‰ã‚„ï¼ˆç¶šãï¼‰ã‚’é™¤å»ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«æœ¬ä½“ã«ã™ã‚‹
    def _title_base(s: str) -> str:
        if s is None:
            return ""
        t = str(s)
        # ä¾‹ï¼š...(ãƒ ã‚¯ãƒ‰ãƒª1/5) / ...(A 2/3) / ...(ãƒ©ãƒ™ãƒ« ï¼“ï¼ï¼•) ãªã©ï¼ˆå…¨è§’æ‹¬å¼§ãƒ»å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
        t = re.sub(r"[\(ï¼ˆ][^()ï¼ˆï¼‰]*?(\d+)\s*[\/ï¼]\s*(\d+)\s*[\)ï¼‰]\s*$", "", t)
        # æœ«å°¾ã®ï¼ˆç¶šãï¼‰ã‚‚é™¤å»ï¼ˆä»»æ„ï¼‰
        t = re.sub(r"[\(ï¼ˆ]\s*ç¶šã\s*[\)ï¼‰]\s*$", "", t)
        # ç©ºç™½æ­£è¦åŒ–
        t = re.sub(r"\s+", " ", t).strip()
        return t

    # æœ«å°¾æ‹¬å¼§ã‹ã‚‰ (i/n) ã‚’æŠ½å‡ºï¼ˆæ‹¬å¼§å†…ã«ä»»æ„ãƒ©ãƒ™ãƒ«è¨±å®¹ã€å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    def _part_tuple(s: str) -> Tuple[Optional[int], Optional[int]]:
        if not s:
            return (None, None)
        m = re.search(r"[\(ï¼ˆ][^()ï¼ˆï¼‰]*?(\d+)\s*[\/ï¼]\s*(\d+)\s*[\)ï¼‰]\s*$", str(s))
        if not m:
            return (None, None)
        return (int(m.group(1)), int(m.group(2)))

    def _is_continuation_group(g: pd.DataFrame) -> bool:
        """åŒå›³è¡¨ã‚­ãƒ¼ã®è¤‡æ•°è¦‹å‡ºã—ãŒã€ç¶šãã€ã‹ã‚’åˆ¤å®š"""
        if g.shape[0] <= 1:
            return False
        g2 = g.sort_values("pdf_page")
        bases = {_title_base(x) for x in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("")}
        if len(bases) != 1:
            return False
        pages = g2["pdf_page"].dropna().astype(int).tolist()
        if not pages:
            return False
        diffs = [b - a for a, b in zip(pages, pages[1:])]
        if not diffs or max(diffs) != 1:   # å®Œå…¨é€£ç¶š
            return False
        parts = [_part_tuple(t) for t in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("")]
        if any(p[0] is None or p[1] is None for p in parts):
            return True  # (i/n) ãŒç„¡ãã¦ã‚‚é€£ç¶šãªã‚‰ç¶šãæ‰±ã„
        nums = [p[0] for p in parts]
        totals = {p[1] for p in parts}
        if len(totals) != 1:
            return False
        return nums == list(range(min(nums), min(nums)+len(nums)))

    # ---- ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«åˆ†é¡ ----
    cont_rows, dup_rows = [], []
    for k, g in df_captions.groupby("å›³è¡¨ã‚­ãƒ¼"):
        if len(g) <= 1:
            continue
        g2 = g.sort_values("pdf_page")
        # --- ç¶šãåˆ¤å®š ---
        if _is_continuation_group(g2):
            cont_rows.append({
                "å›³è¡¨ã‚­ãƒ¼": k,
                "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": " | ".join([str(x) for x in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("").tolist()]),
                "pdfé ä¸€è¦§": ",".join([str(int(x)) for x in g2["pdf_page"].dropna().astype(int).tolist()]),
                "é ãƒ©ãƒ™ãƒ«ä¸€è¦§": ",".join([str(x) for x in g2["page_label"].fillna("").tolist()]),
                "å‚™è€ƒ": "ï¼ˆç¶šãã®ãƒšãƒ¼ã‚¸ã¨ã¿ãªã™ï¼‰"
            })
        # --- çœŸã®é‡è¤‡ ---
        else:
            dup_rows.append({
                "å›³è¡¨ã‚­ãƒ¼": k,
                "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§": " | ".join([str(x) for x in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("").tolist()]),
                "pdfé ä¸€è¦§": ",".join([str(int(x)) for x in g2["pdf_page"].dropna().astype(int).tolist()]),
                "é ãƒ©ãƒ™ãƒ«ä¸€è¦§": ",".join([str(x) for x in g2["page_label"].fillna("").tolist()]),
                "å‚™è€ƒ": "ï¼ˆçœŸã®é‡è¤‡ã®å¯èƒ½æ€§ï¼‰"
            })

    # ç”»é¢è¡¨ç¤º
    if cont_rows:
        st.info("ğŸ”µ ä»¥ä¸‹ã¯ **åŒç•ªå·ã®é€£ç¶šãƒšãƒ¼ã‚¸** ã¨åˆ¤å®šã—ã¾ã—ãŸï¼ˆé‡è¤‡æ‰±ã„ã—ã¾ã›ã‚“ï¼‰ã€‚")
        st.dataframe(pd.DataFrame(cont_rows), use_container_width=True)

    if dup_rows:
        st.warning("ğŸŸ  å›³è¡¨ç•ªå·ã®**çœŸã®é‡è¤‡**ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        st.dataframe(pd.DataFrame(dup_rows), use_container_width=True)
    elif not cont_rows:
        st.success("ğŸŸ¢ å›³è¡¨ç•ªå·ã®é‡è¤‡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    # ---- é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆæ¬ ç•ª/é–‹å§‹1ã§ãªã„ï¼‰----
    def _numeric_core_from_key(key: str) -> str:
        # â˜… base_key ã‚’é€šã—ã¦ã‹ã‚‰ã€ã€Œå›³è¡¨/å›³/è¡¨ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šã‚‹
        s = base_key(key)
        return re.sub(r"^(å›³è¡¨|å›³|è¡¨)", "", s)

    def _series_and_index(key: str) -> Tuple[str, Optional[int], str]:
        m = re.match(r"^(å›³è¡¨|å›³|è¡¨)", key)
        kind = m.group(1) if m else ""
        num_part = _numeric_core_from_key(key)
        if "-" in num_part:
            series, last = num_part.rsplit("-", 1)
        else:
            series, last = "", num_part
        try:
            idx = int(last)
        except Exception:
            idx = None
        return series, idx, kind


    from collections import defaultdict
    series_map = defaultdict(list)  # (kind, series) -> [(idx, key, pdf), ...]
    for _, r in df_captions.iterrows():
        key = str(r.get("å›³è¡¨ã‚­ãƒ¼"))
        pdfp = r.get("pdf_page")
        series, idx, kind = _series_and_index(key)
        if idx is None:
            continue
        series_map[(kind, series)].append((int(idx), key, int(pdfp) if pd.notna(pdfp) else 10**9))

    gap_rows, start_rows = [], []
    for (kind, series), items in series_map.items():
        items_sorted = sorted(items, key=lambda x: (x[0], x[2]))
        idxs = [i for i, _, _ in items_sorted]
        starts_at = idxs[0] if idxs else None
        if starts_at is not None and starts_at != 1:
            start_rows.append({
                "ç¨®åˆ¥": kind,
                "ç³»åˆ—": series or "(å˜ä¸€ç•ªå·)",
                "é–‹å§‹ç•ªå·": starts_at,
                "æœŸå¾…": 1,
                "å­˜åœ¨ç•ªå·": ",".join(map(str, idxs))
            })
        if len(idxs) >= 2:
            missing = []
            for a, b in zip(idxs, idxs[1:]):
                if b - a > 1:
                    missing.extend(range(a+1, b))
            if missing:
                gap_rows.append({
                    "ç¨®åˆ¥": kind,
                    "ç³»åˆ—": series or "(å˜ä¸€ç•ªå·)",
                    "æ¬ ç•ª": ",".join(map(str, missing)),
                    "å­˜åœ¨ç•ªå·": ",".join(map(str, idxs)),
                })

    if not gap_rows and not start_rows:
        st.success("ğŸŸ¢ é€£ç•ªã®é£›ã³ã¯ç„¡ãã€å„ç³»åˆ—ã®é–‹å§‹ç•ªå·ã‚‚ 1 ã«ãªã£ã¦ã„ã¾ã™ã€‚")
    else:
        if start_rows:
            st.warning("ğŸŸ  ç³»åˆ—ã®é–‹å§‹ç•ªå·ãŒ 1 ã§ãªã„ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            st.dataframe(pd.DataFrame(start_rows), use_container_width=True)
        if gap_rows:
            st.warning("ğŸŸ  é€£ç•ªã«æ¬ ç•ªï¼ˆé£›ã³ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            st.dataframe(pd.DataFrame(gap_rows), use_container_width=True)

# ===== æœ¬æ–‡å‚ç…§ã®è¡¨ç¤º =====
st.subheader("ğŸ”— æœ¬æ–‡ä¸­ã® å›³/è¡¨/å›³è¡¨ å‚ç…§ï¼ˆexcerptä»˜ï¼‰")
st.dataframe(df_refs, use_container_width=True)



# =========================
# çªãåˆã‚ã›ï¼šå›³è¡¨è¦‹å‡ºã— â†” æœ¬æ–‡å‚ç…§
# =========================
cap_idx = index_pages_by_key(df_captions)  # å›³è¡¨ã‚­ãƒ¼ â†’ pdf_page ä¸€è¦§
ref_idx = index_pages_by_key(df_refs)

# â‘  ãƒ•ãƒ«ã®å›³è¡¨ã‚­ãƒ¼é›†åˆï¼ˆDataFrame ã‹ã‚‰ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«å–å¾—ï¼‰
cap_keys_full: set[str] = set()
ref_keys_full: set[str] = set()
if not df_captions.empty and "å›³è¡¨ã‚­ãƒ¼" in df_captions.columns:
    cap_keys_full = set(df_captions["å›³è¡¨ã‚­ãƒ¼"].dropna().astype(str))
if not df_refs.empty and "å›³è¡¨ã‚­ãƒ¼" in df_refs.columns:
    ref_keys_full = set(df_refs["å›³è¡¨ã‚­ãƒ¼"].dropna().astype(str))

# â‘¡ ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼é›†åˆï¼ˆ(1/3) ã ã‘è½ã¨ã—ãŸã‚‚ã®ï¼‰
cap_base_keys = {base_key(k) for k in cap_keys_full}
ref_base_keys = {base_key(k) for k in ref_keys_full}

#############################################
# --- æœªå¼•ç”¨ãƒã‚§ãƒƒã‚¯ï¼ˆè¦‹å‡ºã—å´ï¼‰ ---
#   ãƒ»è¦‹å‡ºã—ã®ãƒ•ãƒ«ã‚­ãƒ¼ k ãŒæœ¬æ–‡ã§ä¸€åº¦ã‚‚å‡ºã¦ã“ãªã„
#   ãƒ»ã¾ãŸã¯ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ã§ã‚‚ãƒãƒƒãƒã—ãªã„
#############################################
missing_in_refs = sorted(
    k for k in cap_keys_full
    if not (
        (k in ref_keys_full)              # å®Œå…¨ä¸€è‡´å¼•ç”¨ã‚ã‚Š
        or (base_key(k) in ref_base_keys) # ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ä¸€è‡´ã§å¼•ç”¨ã‚ã‚Šï¼ˆè¡¨3.1.5-1(1/3) vs è¡¨3.1.5-1ï¼‰
    )
)

#############################################
# --- è¦‹å‡ºã—ãªã—å‚ç…§ï¼ˆæœ¬æ–‡å´ï¼‰ ---
#   ãƒ»æœ¬æ–‡ã®ãƒ•ãƒ«ã‚­ãƒ¼ k ã«å¯¾å¿œã™ã‚‹è¦‹å‡ºã—ãŒãªã„
#   ãƒ»ãŸã ã—ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ä¸€è‡´ã§è¦‹å‡ºã—ãŒã‚ã‚Œã° OK
#############################################
missing_in_captions = sorted(
    k for k in ref_keys_full
    if not (
        (k in cap_keys_full)              # å®Œå…¨ä¸€è‡´ã§è¦‹å‡ºã—ã‚ã‚Š
        or (base_key(k) in cap_base_keys) # ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ä¸€è‡´ã§è¦‹å‡ºã—ã‚ã‚Š
    )
)

all_captions_referenced  = (len(missing_in_refs) == 0)
has_refs_without_caption = (len(missing_in_captions) > 0)




st.subheader("ğŸ” çªãåˆã‚ã›çµæœï¼ˆè¦‹å‡ºã— â†” å‚ç…§ï¼‰")

c1, c2 = st.columns(2)
with c1:
    st.markdown("**â‘  å…¨ã¦ã®å›³è¡¨è¦‹å‡ºã—ãŒæœ¬æ–‡ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ**")
    st.write("â†’ **{}**".format("ã¯ã„ï¼ˆå…¨ã¦å¼•ç”¨ã‚ã‚Šï¼‰âœ…" if all_captions_referenced else "ã„ã„ãˆï¼ˆæœªå¼•ç”¨ã‚ã‚Šï¼‰âš ï¸"))
with c2:
    st.markdown("**â‘¡ æœ¬æ–‡ã«å‚ç…§ãŒã‚ã‚‹ãŒè¦‹å‡ºã—ãŒç„¡ã„ã‚‚ã®ã¯ãªã„ã‹ï¼Ÿ**")
    st.write("â†’ **{}**".format(
        "ã¯ã„ï¼ˆå…¨ã¦è¦‹å‡ºã—ã‚ã‚Šï¼‰âœ…" if not has_refs_without_caption else "ã„ã„ãˆï¼ˆè¦‹å‡ºã—ãªã—ã®å‚ç…§ã‚ã‚Šï¼‰âš ï¸"
    ))


# ç”»é¢è¡¨ç¤ºç”¨ï¼šå‚ç…§æƒ…å ±ã‚’ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ã§é›†ç´„
ref_lbls, ref_pdfs, ref_texts, ref_hi = ref_aggregate_for_view(df_refs, base_key_func=base_key)


def _first_caption_row(df_cap: pd.DataFrame, key: str):
    grp = df_cap[df_cap["å›³è¡¨ã‚­ãƒ¼"] == key]
    if grp.empty:
        return {"å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": "", "é ": "", "pdfé ": 10**9}
    row = grp.sort_values("pdf_page").iloc[0]
    return {
        "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": row.get("è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«", "") or "",
        "é ": row.get("page_label", "") or "",
        "pdfé ": int(row.get("pdf_page")) if pd.notna(row.get("pdf_page")) else 10**9,
    }


# ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ãŒå‚ç…§å´ã«å­˜åœ¨ã™ã‚‹å›³è¡¨ã‚­ãƒ¼ã‚’ã€Œå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹è¦‹å‡ºã—ã€ã¨ã¿ãªã™
ref_base_keys_view = set(ref_lbls.keys())
referenced_keys = sorted(
    k for k in cap_idx.keys()
    if base_key(k) in ref_base_keys_view
)

rows = []
for k in referenced_keys:
    ci = _first_caption_row(df_captions, k)
    bk = base_key(k)
    rows.append({
        "å›³è¡¨ã‚­ãƒ¼": k,
        "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": ci["å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«"],
        "é ": ci["é "],
        "pdfé ": ci["pdfé "],
        "å‚ç…§é ãƒ©ãƒ™ãƒ«": ref_lbls.get(bk, ""),
        "å‚ç…§pdfé ": ref_pdfs.get(bk, ""),
        "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": ref_texts.get(bk, ""),
        "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)": ref_hi.get(bk, ""),
        "_sort": ci["pdfé "],
    })

df_referenced_view = (
    pd.DataFrame(
        rows,
        columns=[
            "å›³è¡¨ã‚­ãƒ¼", "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«", "é ", "pdfé ",
            "å‚ç…§é ãƒ©ãƒ™ãƒ«", "å‚ç…§pdfé ", "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ", "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)", "_sort"
        ]
    )
    .sort_values("_sort", kind="mergesort")
    .drop(columns=["_sort"])
)

st.markdown("#### ğŸ”µ å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹è¦‹å‡ºã—ï¼ˆè¦‹å‡ºã—ã‚ã‚Šï¼‹æœ¬æ–‡å‚ç…§ã‚ã‚Šï¼‰")
st.dataframe(df_referenced_view, use_container_width=True)

st.markdown("#### ğŸŸ  æœªå¼•ç”¨ã®è¦‹å‡ºã—ï¼ˆè¦‹å‡ºã—ã¯ã‚ã‚‹ãŒæœ¬æ–‡å‚ç…§ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼‰")
df_missing_caption_refs = pd.DataFrame(
    [{"å›³è¡¨ã‚­ãƒ¼": k, "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸": cap_idx.get(k, []), "å‚ç…§ãƒšãƒ¼ã‚¸": ref_idx.get(k, [])}
     for k in missing_in_refs]
)
st.dataframe(df_missing_caption_refs, use_container_width=True)

st.markdown("#### ğŸ”µ è¦‹å‡ºã—ãªã—ã®å‚ç…§ï¼ˆæœ¬æ–‡ã«å‚ç…§ã¯ã‚ã‚‹ãŒå¯¾å¿œã™ã‚‹è¦‹å‡ºã—ãŒç„¡ã„ï¼‰")
df_orphan_refs = pd.DataFrame(
    [{"å›³è¡¨ã‚­ãƒ¼": k, "å‚ç…§ãƒšãƒ¼ã‚¸": ref_idx.get(k, []), "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸": cap_idx.get(k, [])}
     for k in missing_in_captions]
)
st.dataframe(df_orphan_refs, use_container_width=True)

# =========================
# CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================
with st.sidebar:
    st.markdown("### CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

    for df, name in [
        (df_per_page_labels, "per_page_labels.csv"),
        (df_captions, "figure_table_captions.csv"),
        (df_refs, "figure_table_references.csv"),
    ]:
        if not df.empty:
            df2 = df.copy()
            if "page_label" in df2.columns:
                df2["page_label"] = df2["page_label"].map(protect_for_excel_csv)
            buf = io.StringIO()
            df2.to_csv(buf, index=False)
            st.download_button(
                f"ğŸ“¥ {name}",
                data=buf.getvalue().encode("utf-8-sig"),
                file_name=name,
                mime="text/csv",
                use_container_width=True,
            )

# =========================
# XLSXï¼ˆçªãåˆã‚ã›ï¼‹é‡è¤‡/ç¶šãï¼‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================
from io import BytesIO

cap_info = caption_info_first_by_key(df_captions, df_per_page_labels)
ref_page_labels, ref_pdf_pages, ref_texts, ref_highlight_texts = aggregate_ref_info(
    df_refs,
    base_key_func=base_key,
)

# XLSX ç”¨ã® referenced_keys ã‚‚ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ã§åˆ¤å®š
ref_base_keys_x = set(ref_page_labels.keys())
referenced_keys_x = sorted(
    k for k in cap_idx.keys()
    if base_key(k) in ref_base_keys_x
)

df_referenced_captions_x = make_crosscheck_rows(
    referenced_keys_x,
    caption_src=True,
    cap_info=cap_info,
    ref_page_labels=ref_page_labels,
    ref_pdf_pages=ref_pdf_pages,
    ref_texts=ref_texts,
    ref_highlight_texts=ref_highlight_texts,
    base_key_func=base_key,
)
df_missing_caption_refs_x = make_crosscheck_rows(
    sorted(missing_in_refs),
    caption_src=True,
    cap_info=cap_info,
    ref_page_labels=ref_page_labels,
    ref_pdf_pages=ref_pdf_pages,
    ref_texts=ref_texts,
    ref_highlight_texts=ref_highlight_texts,
    base_key_func=base_key,
)
df_orphan_refs_x = make_crosscheck_rows(
    sorted(missing_in_captions),
    caption_src=False,
    cap_info=cap_info,
    ref_page_labels=ref_page_labels,
    ref_pdf_pages=ref_pdf_pages,
    ref_texts=ref_texts,
    ref_highlight_texts=ref_highlight_texts,
    base_key_func=base_key,
)

# çœŸã®é‡è¤‡ & ç¶šãåˆ¤å®šã®DF
dup_df = pd.DataFrame(dup_rows) if dup_rows else pd.DataFrame(
    columns=["å›³è¡¨ã‚­ãƒ¼", "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§", "pdfé ä¸€è¦§", "é ãƒ©ãƒ™ãƒ«ä¸€è¦§", "å‚™è€ƒ"]
)
cont_df = pd.DataFrame(cont_rows) if cont_rows else pd.DataFrame(
    columns=["å›³è¡¨ã‚­ãƒ¼", "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæœ¬ä½“ï¼‰", "pdfé ä¸€è¦§", "é ãƒ©ãƒ™ãƒ«ä¸€è¦§", "å‚™è€ƒ"]
)

with st.sidebar:
    st.markdown("### ğŸ”— çªãåˆã‚ã›çµæœï¼ˆxlsxï¼‰")
    output = BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        # ã‚µãƒãƒªãƒ¼
        pd.DataFrame({
            "é …ç›®": ["å…¨ã¦å¼•ç”¨æ¸ˆã¿ã‹ï¼Ÿ", "è¦‹å‡ºã—ãªã—ã®å‚ç…§ã¯ãªã„ã‹ï¼Ÿ"],
            "çµæœ": [
                "ã¯ã„ï¼ˆå…¨ã¦å¼•ç”¨ã‚ã‚Šï¼‰âœ…" if all_captions_referenced else "ã„ã„ãˆï¼ˆæœªå¼•ç”¨ã‚ã‚Šï¼‰âš ï¸",
                "ã¯ã„ï¼ˆè¦‹å‡ºã—ãªã—å‚ç…§ãªã—ï¼‰âœ…" if not has_refs_without_caption else "ã„ã„ãˆï¼ˆè¦‹å‡ºã—ãªã—å‚ç…§ã‚ã‚Šï¼‰âš ï¸",
            ]
        }).to_excel(writer, sheet_name="ã‚µãƒãƒªãƒ¼", index=False)

        # çªãåˆã‚ã› 3 ã‚·ãƒ¼ãƒˆ
        df_referenced_captions_x.to_excel(writer, sheet_name="å¼•ç”¨è¦‹å‡ºã—", index=False)
        df_missing_caption_refs_x.to_excel(writer, sheet_name="æœªå¼•ç”¨è¦‹å‡ºã—", index=False)
        df_orphan_refs_x.to_excel(writer, sheet_name="è¦‹å‡ºã—ãªã—å‚ç…§", index=False)

        # é‡è¤‡/ç¶šã åˆ¤å®š 2 ã‚·ãƒ¼ãƒˆ
        dup_df.to_excel(writer, sheet_name="é‡è¤‡ï¼ˆç–‘ã„ï¼‰", index=False)
        cont_df.to_excel(writer, sheet_name="ç¶šãåˆ¤å®š", index=False)

        # é€£ç•ªãƒã‚§ãƒƒã‚¯
        if 'start_rows' in locals():
            (
                pd.DataFrame(start_rows)
                if start_rows else
                pd.DataFrame(columns=["ç¨®åˆ¥", "ç³»åˆ—", "é–‹å§‹ç•ªå·", "æœŸå¾…", "å­˜åœ¨ç•ªå·"])
            ).to_excel(writer, sheet_name="é–‹å§‹ç•ªå·ãƒã‚§ãƒƒã‚¯", index=False)
        if 'gap_rows' in locals():
            (
                pd.DataFrame(gap_rows)
                if gap_rows else
                pd.DataFrame(columns=["ç¨®åˆ¥", "ç³»åˆ—", "æ¬ ç•ª", "å­˜åœ¨ç•ªå·"])
            ).to_excel(writer, sheet_name="æ¬ ç•ªãƒã‚§ãƒƒã‚¯", index=False)

    base = uploaded.name.rsplit(".", 1)[0]
    xlsx_filename = f"å›³è¡¨ç…§åˆ_{base}.xlsx"

    st.download_button(
        "ğŸ“˜ çªãåˆã‚ã›çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=output.getvalue(),
        file_name=xlsx_filename,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )

# =========================
# ãƒ‡ãƒãƒƒã‚°
# =========================
if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.code(f"EXTRACT_RE = {EXTRACT_RE.pattern}")
    st.caption("ãƒãƒƒãƒã‚’å«ã‚€1è¡ŒæŠ½å‡ºï¼‹âŸªå¼·èª¿âŸ«ï¼‹excerptä»˜ãã€‚")
