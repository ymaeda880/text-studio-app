# -*- coding: utf-8 -*-
# pages/35_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢.py
from __future__ import annotations

import io, re
from pathlib import Path
from typing import List, Iterable
import pandas as pd
import streamlit as st

# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(page_title="ğŸ” ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” 72_ãƒ¬ãƒƒãƒ‰ãƒªã‚¹ãƒˆæ¤œç´¢")
st.caption("ãƒ•ã‚©ãƒ«ãƒ€: data/redlist/{fukushima, MOE, chiba} ã‚’èª­ã¿è¾¼ã¿ã€å’Œå/ç¨®åã‚’è¤‡æ•°èªã¾ã¨ã‚ã¦æ¨ªæ–­æ¤œç´¢ã—ã¾ã™ã€‚")

# =========================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =========================================================
from lib.redlist.loaders import load_all
DATA_ROOT = Path("data/redlist").resolve()
moe_df, fuku_df, chiba_df = load_all(DATA_ROOT)

# =========================================================
# ã‚½ãƒ¼ã‚¹é¸æŠï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# =========================================================
with st.sidebar:
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ")
    use_moe = st.checkbox("ç’°å¢ƒçœ", value=True)
    use_fuku = st.checkbox("ç¦å³¶çœŒ", value=False)
    use_chiba = st.checkbox("åƒè‘‰çœŒ", value=False)

    st.markdown("---")
    st.subheader("ğŸ“Š èª­ã¿è¾¼ã¿ä»¶æ•°ï¼ˆå‚è€ƒï¼‰")
    st.code(str(DATA_ROOT))
    st.write({
        "ç’°å¢ƒçœ": 0 if moe_df.empty else int(moe_df.shape[0]),
        "ç¦å³¶çœŒ": 0 if fuku_df.empty else int(fuku_df.shape[0]),
        "åƒè‘‰çœŒ": 0 if chiba_df.empty else int(chiba_df.shape[0]),
    })

# =========================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼
# =========================================================
def normalize_name(s: str) -> str:
    return "" if pd.isna(s) else str(s).replace("\u3000", " ").strip()

def parse_queries(raw: str) -> List[str]:
    """æ”¹è¡Œãƒ»ã‚«ãƒ³ãƒãƒ»ã‚¹ãƒšãƒ¼ã‚¹ãªã©ã§åŒºåˆ‡ã£ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆé †åºä¿æŒï¼‰"""
    if not raw:
        return []
    raw = raw.replace("\u3000", " ")
    parts = re.split(r"[,\s]+", raw)
    seen, uniq = set(), []
    for p in parts:
        p = normalize_name(p)
        if p and p not in seen:
            seen.add(p)
            uniq.append(p)
    return uniq

def mask_for_queries(series: pd.Series, qs: Iterable[str], exact: bool) -> pd.Series:
    s_norm = series.fillna("").map(normalize_name)
    if exact:
        return s_norm.isin(list(qs))
    mask = pd.Series(False, index=series.index)
    s_low = s_norm.str.lower()
    for q in qs:
        ql = q.lower()
        mask = mask | s_low.str.contains(re.escape(ql), na=False)
    return mask

def ensure_line_numbers(hit: pd.DataFrame) -> pd.Series:
    """ãƒ­ãƒ¼ãƒ€ãƒ¼ã§è¡Œç•ªå·ãŒã‚ã‚Œã°ä½¿ç”¨ã€‚ç„¡ã‘ã‚Œã°cumcountã§è£œå®Œ (+2: è¦‹å‡ºã—1è¡Œæƒ³å®š)"""
    if "è¡Œç•ªå·" in hit.columns:
        try:
            return pd.to_numeric(hit["è¡Œç•ªå·"], errors="coerce").fillna("").astype("Int64")
        except Exception:
            pass
    if "ãƒ•ã‚¡ã‚¤ãƒ«å" in hit.columns and "ã‚·ãƒ¼ãƒˆå" in hit.columns:
        seq = hit.groupby(["ãƒ•ã‚¡ã‚¤ãƒ«å", "ã‚·ãƒ¼ãƒˆå"], sort=False).cumcount()
    elif "ãƒ•ã‚¡ã‚¤ãƒ«å" in hit.columns:
        seq = hit.groupby(["ãƒ•ã‚¡ã‚¤ãƒ«å"], sort=False).cumcount()
    else:
        seq = pd.Series(range(len(hit)), index=hit.index)
    return (seq + 2).astype(int)

def build_output(hit: pd.DataFrame, source: str, name_col: str) -> pd.DataFrame:
    """å‡ºåŠ›åˆ—ã®æ§‹ç¯‰"""
    hit = hit.copy()
    out = pd.DataFrame(index=hit.index)
    out["å’Œå"] = hit[name_col].astype(str).map(normalize_name)
    out["ã‚½ãƒ¼ã‚¹"] = source

    if "ãƒ•ã‚¡ã‚¤ãƒ«å" in hit.columns:
        out["ãƒ•ã‚¡ã‚¤ãƒ«å"] = hit["ãƒ•ã‚¡ã‚¤ãƒ«å"].astype(str).map(normalize_name)
    if "ã‚·ãƒ¼ãƒˆå" in hit.columns:
        out["ã‚·ãƒ¼ãƒˆå"] = hit["ã‚·ãƒ¼ãƒˆå"].astype(str).map(normalize_name)
    out["è¡Œç•ªå·"] = ensure_line_numbers(hit)
    if "å­¦å" in hit.columns:
        out["å­¦å"] = hit["å­¦å"].astype(str).map(normalize_name)

    if source == "ç’°å¢ƒçœ":
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = hit.get("ç’°å¢ƒçœã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "")
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = hit.get("ã‚«ãƒ†ã‚´ãƒªãƒ¼", "")
    elif source == "ç¦å³¶çœŒ":
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = hit.get("ç¦å³¶çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "")
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = hit.get("ç¦å³¶ã‚«ãƒ†ã‚´ãƒªãƒ¼", "")
    elif source == "åƒè‘‰çœŒ":
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = hit.get("åƒè‘‰çœŒã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "")
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = hit.get("ã‚«ãƒ†ã‚´ãƒªãƒ¼", "")
    else:
        out["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"] = ""
        out["å…ƒã‚«ãƒ†ã‚´ãƒª"] = ""

    first = ["å’Œå", "ã‚½ãƒ¼ã‚¹", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "å…ƒã‚«ãƒ†ã‚´ãƒª", "ãƒ•ã‚¡ã‚¤ãƒ«å", "è¡Œç•ªå·", "ã‚·ãƒ¼ãƒˆå", "å­¦å"]
    rest = [c for c in out.columns if c not in first]
    out = out[first + rest]
    return out.reset_index(drop=True)

# =========================================================
# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
# =========================================================
st.subheader("ğŸ§­ å’Œå/ç¨®å æ¤œç´¢ï¼ˆè¤‡æ•°èªå¯¾å¿œï¼‰")
with st.form("search_form"):
    col_q1, col_q2 = st.columns([3, 1])
    with col_q1:
        multi_query_raw = st.text_area(
            "å’Œåï¼ˆç’°å¢ƒçœ/ç¦å³¶ï¼‰ãƒ»ç¨®åï¼ˆåƒè‘‰ï¼‰ã‚’è¤‡æ•°å…¥åŠ›å¯èƒ½",
            height=120,
            placeholder="ä¾‹ï¼‰\nãƒ‹ãƒ›ãƒ³ã‚¶ãƒ«\nãƒ¤ãƒãƒ, ãƒˆã‚­\nãƒ¤ãƒ³ãƒãƒ«ã‚¯ã‚¤ãƒŠ",
        )
    with col_q2:
        mode_exact = st.toggle("å®Œå…¨ä¸€è‡´", True, help="OFFã§éƒ¨åˆ†ä¸€è‡´")
    do_search = st.form_submit_button("ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œ")

queries = parse_queries(multi_query_raw) if do_search else []
if do_search and queries:
    st.caption(f"ğŸ” æ¤œç´¢èªï¼š{', '.join(queries)}ï¼ˆ{len(queries)}èªï¼‰")

# =========================================================
# æ¤œç´¢å‡¦ç†
# =========================================================
if do_search and queries:
    parts: List[pd.DataFrame] = []
    with st.spinner("æ¤œç´¢ä¸­â€¦"):

        # ç’°å¢ƒçœï¼ˆå’Œåï¼‰
        if use_moe and not moe_df.empty and "å’Œå" in moe_df.columns:
            m = mask_for_queries(moe_df["å’Œå"], queries, mode_exact)
            hit = moe_df.loc[m]
            if not hit.empty:
                parts.append(build_output(hit, "ç’°å¢ƒçœ", "å’Œå"))

        # ç¦å³¶çœŒï¼ˆå’Œåï¼‰
        if use_fuku and not fuku_df.empty and "å’Œå" in fuku_df.columns:
            m = mask_for_queries(fuku_df["å’Œå"], queries, mode_exact)
            hit = fuku_df.loc[m]
            if not hit.empty:
                parts.append(build_output(hit, "ç¦å³¶çœŒ", "å’Œå"))

        # åƒè‘‰çœŒï¼ˆç¨®åï¼‰
        if use_chiba and not chiba_df.empty and "ç¨®å" in chiba_df.columns:
            m = mask_for_queries(chiba_df["ç¨®å"], queries, mode_exact)
            hit = chiba_df.loc[m]
            if not hit.empty:
                parts.append(build_output(hit, "åƒè‘‰çœŒ", "ç¨®å"))

    matched_df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()

    # --- æœªãƒ’ãƒƒãƒˆèª ---
    missing_df = pd.DataFrame()
    if mode_exact:
        present_names = set(matched_df["å’Œå"].astype(str).map(normalize_name)) if not matched_df.empty else set()
        missing = [q for q in queries if normalize_name(q) not in present_names]
        if missing:
            missing_df = pd.DataFrame({
                "å’Œå": [q for q in missing],
                "ã‚½ãƒ¼ã‚¹": ["â€”"] * len(missing),
                "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·": ["è©²å½“ãªã—"] * len(missing),
                "å…ƒã‚«ãƒ†ã‚´ãƒª": [""] * len(missing),
                "ãƒ•ã‚¡ã‚¤ãƒ«å": [""] * len(missing),
                "è¡Œç•ªå·": [pd.NA] * len(missing),
                "ã‚·ãƒ¼ãƒˆå": [""] * len(missing),
                "å­¦å": [""] * len(missing),
            })

    if matched_df.empty and missing_df.empty:
        st.warning("ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        results_df = pd.concat([matched_df, missing_df], ignore_index=True, sort=False)

        order_map = {q: i for i, q in enumerate(queries)}
        results_df["__name_norm"] = results_df["å’Œå"].astype(str).map(normalize_name)
        results_df["__ord"] = results_df["__name_norm"].map(order_map).fillna(len(queries) + 1)

        if "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·" in results_df.columns:
            is_missing_row = results_df["ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·"].astype(str).eq("è©²å½“ãªã—")
            results_df.loc[is_missing_row, "__ord"] = (
                len(queries) + results_df.index.to_series()[is_missing_row].rank(method="first").astype(int)
            )

        results_df = results_df.sort_values(["__ord"], kind="stable").drop(columns=["__ord", "__name_norm"])

        if "å’Œå" in results_df.columns:
            dup_mask = results_df["å’Œå"].eq(results_df["å’Œå"].shift())
            results_df.loc[dup_mask, "å’Œå"] = ""

        base = ["å’Œå", "ã‚½ãƒ¼ã‚¹", "ã‚«ãƒ†ã‚´ãƒªãƒ¼è¨˜å·", "å…ƒã‚«ãƒ†ã‚´ãƒª", "ãƒ•ã‚¡ã‚¤ãƒ«å", "è¡Œç•ªå·", "ã‚·ãƒ¼ãƒˆå", "å­¦å"]
        results_df = results_df[[c for c in base if c in results_df.columns]]
        st.dataframe(results_df, use_container_width=True)

        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
            results_df.to_excel(w, index=False, sheet_name="Search_Results")

        st.download_button(
            "ğŸ“¥ æ¤œç´¢çµæœã‚’Excelã§ä¿å­˜",
            buf.getvalue(),
            file_name="redlist_search_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )
else:
    st.info("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ã€ğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

# =========================================================
# å‚™è€ƒ
# =========================================================
with st.expander("â„¹ï¸ å‚™è€ƒ", expanded=False):
    st.markdown("""
- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ**ï¼šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é¸æŠã—ã¾ã™ã€‚åˆæœŸå€¤ã¯ã€Œç’°å¢ƒçœã€ã®ã¿ã€‚
- **ä¸¦ã¹æ›¿ãˆ**ï¼šå…¥åŠ›ã—ãŸæ¤œç´¢èªã®é †ç•ªã«çµæœã‚’ä¸¦ã¹ã¦ã„ã¾ã™ã€‚
- **é€£ç¶šé‡è¤‡ã®ç©ºç™½åŒ–**ï¼šåŒã˜â€œå’Œåâ€ãŒé€£ç¶šã™ã‚‹å ´åˆã€2è¡Œç›®ä»¥é™ã®â€œå’Œåâ€ã¯ç©ºç™½è¡¨ç¤ºã—ã¾ã™ï¼ˆExcelã«ã‚‚åæ˜ ï¼‰ã€‚
- **è©²å½“ãªã—**ã¯å®Œå…¨ä¸€è‡´ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿åˆ¤å®šã—ã¾ã™ã€‚
    """)
