# -*- coding: utf-8 -*-
# pages/24_å›³è¡¨ãƒã‚§ãƒƒã‚¯.pyï¼ˆæ”¹è‰¯ç‰ˆï¼šå¼·èª¿è¡¨ç¤ºãƒ»excerptä»˜ã + ç•ªå·å“è³ª/çªãåˆã‚ã› + XLSXå‡ºåŠ›ï¼‰
from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import io, re, tempfile

import streamlit as st
import pandas as pd

# === å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆlib/ï¼‰ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ===
from lib.text_normalizer import (
    z2h_numhy,
    normalize_strict,
    HY,
)
from lib.toc_segments import (
    pdf_to_text_per_page,
    extract_single_page_label,
)

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ¡ã‚¤ãƒ³UI
# =========================
st.set_page_config(page_title="ğŸ–¼ï¸ å›³è¡¨ æŠ½å‡ºï¼ˆè¡Œé ­/åŠ©è©/å¥ç‚¹ãƒ«ãƒ¼ãƒ« + é ãƒ©ãƒ™ãƒ«ï¼‰", page_icon="ğŸ–¼ï¸", layout="wide")
st.title("ğŸ–¼ï¸ å›³è¡¨ æŠ½å‡º â€” ã‚¿ã‚¤ãƒˆãƒ«/å‚ç…§ï¼ˆè¡Œé ­ãƒ»ç›´å¾ŒåŠ©è©ãƒ»å¥ç‚¹ãƒ«ãƒ¼ãƒ«ï¼‰ï¼‹ é ãƒ©ãƒ™ãƒ«ä½µè¨˜")

uploaded = st.file_uploader("PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
run = st.button("â–¶ è§£æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True)

with st.sidebar:
    st.markdown("### ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    ctx_chars  = st.slider("å‚ç…§ã®å‰å¾Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°ï¼ˆexcerpt ç”¨ï¼‰", 10, 300, 60, 5)
    show_debug = st.checkbox("å†…éƒ¨æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰ã‚’è¡¨ç¤º", value=False)

if not uploaded or not run:
    st.stop()

# =========================
# PDF â†’ ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ
# =========================
with tempfile.TemporaryDirectory() as td:
    pdf_path = Path(td) / "input.pdf"
    pdf_path.write_bytes(uploaded.getvalue())
    pages_text: List[str] = pdf_to_text_per_page(pdf_path)

st.success(f"PDF èª­ã¿è¾¼ã¿å®Œäº†ï¼šãƒšãƒ¼ã‚¸æ•° {len(pages_text)}")

# =========================
# å›³è¡¨æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸å›ºæœ‰ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
# =========================
DOT = r"[\.ï¼ãƒ»ï½¥]"
NUM_ZH = r"[0-9ï¼-ï¼™]+"
NUM_TOKEN = rf"""
(
    {NUM_ZH}
    (?:\s*(?:{DOT}|{HY})\s*{NUM_ZH})*
    |
    [ï¼ˆ(]\s*{NUM_ZH}\s*[ï¼‰)]
)
"""
EXTRACT_RE = re.compile(
    rf"(?P<kind>å›³è¡¨|å›³|è¡¨)\s*(?P<num>{NUM_TOKEN})",
    re.X
)

def canon_num(num: str) -> str:
    s = num.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰", "0123456789()"))
    s = re.sub(DOT, ".", s)
    s = re.sub(HY, "-", s)
    s = re.sub(r"[()ï¼ˆï¼‰]", "", s)
    s = re.sub(r"\s*\.\s*", ".", s)
    s = re.sub(r"\s*-\s*", "-", s)
    s = re.sub(r"\s+", "", s)
    return s

def canon_label(kind: str, num: str) -> str:
    return f"{kind}{canon_num(num)}"

try:
    import regex as re2
except Exception:
    re2 = re

PARTICLES_RE = re2.compile(r"(?:ã«|ã‚’|ã¯|ã¸|ã§|ã¨|ã®|ãªã©|ç­‰|ã¾ãŸã¯|åˆã¯|ãŠã‚ˆã³|åŠã³|ã‹ã¤)")

# ===== è¡ŒæŠ½å‡ºè£œåŠ©é–¢æ•° =====
def extract_line_covering_match(full: str, start: int, end: int) -> Tuple[int, str, int, int]:
    """ãƒãƒƒãƒã‚’å¿…ãšå«ã‚€è¡Œï¼ˆæ”¹è¡Œã¾ãŸãå¯¾å¿œï¼‰ã‚’è¿”ã™"""
    line_start = full.rfind("\n", 0, start)
    line_start = 0 if line_start == -1 else line_start + 1
    line_end = full.find("\n", end)
    if line_end == -1:
        line_end = len(full)
    line_txt = full[line_start:line_end].rstrip("\r\n")
    approx_lineno = full.count("\n", 0, line_start) + 1
    return approx_lineno, line_txt, line_start, line_end

# ===== ãƒšãƒ¼ã‚¸å˜ä½ã®æŠ½å‡º =====
def judge_hits_in_page(page_text: str, ctx: int) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    captions, refs = [], []
    full = page_text.replace("\r\n", "\n").replace("\r", "\n")

    for m in EXTRACT_RE.finditer(full):
        kind, num, raw = m.group("kind"), m.group("num"), m.group(0)
        lineno, line_txt, line_start, line_end = extract_line_covering_match(full, m.start(), m.end())

        is_line_head = (full[line_start:m.start()].strip() == "")
        rel_end = (m.start() - line_start) + len(raw)
        after_on_line = line_txt[rel_end:] if rel_end <= len(line_txt) else ""
        particle_follow = bool(re2.match(rf"\s*{PARTICLES_RE.pattern}", after_on_line))
        has_period = ("ã€‚" in line_txt)
        is_reference = (not is_line_head) or particle_follow or has_period

        # å¼·èª¿ã¨excerpt
        highlighted = line_txt.replace(raw, f"âŸª{raw}âŸ«", 1)
        left  = max(0, m.start() - ctx)
        right = min(len(full), m.end() + ctx)
        excerpt = full[left:m.start()] + f"âŸª{raw}âŸ«" + full[m.end():right]

        if is_reference:
            refs.append({
                "è¡Œç•ªå·": lineno,
                "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": raw.strip(),
                "å›³è¡¨ç¨®é¡": kind,
                "å›³è¡¨ç•ªå·": f"{kind}{z2h_numhy(num)}",
                "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
                "excerpt": excerpt,
                "è¡Œãƒ†ã‚­ã‚¹ãƒˆ": line_txt,
                "è¡Œãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)": highlighted,
                "åˆ¤å®š": "å‚ç…§",
                "rule(ç†ç”±)": (
                    "è¡Œé ­ã§ãªã„â†’å‚ç…§" if not is_line_head else
                    ("ç›´å¾ŒãŒåŠ©è©/æ¥ç¶šèªâ†’å‚ç…§" if particle_follow else "è¡Œã«å¥ç‚¹ã‚ã‚Šâ†’å‚ç…§")
                ),
            })
        else:
            title = re.sub(r"^[\s:ï¼š.\-ï¼ã€ãƒ»]+", "", after_on_line).strip()
            captions.append({
                "è¡Œç•ªå·": lineno,
                "å›³è¡¨ç¨®é¡": kind,
                "å›³è¡¨ç•ªå·": f"{kind}{z2h_numhy(num)}",
                "å›³è¡¨ã‚­ãƒ¼": canon_label(kind, num),
                "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«": title,
                "matched_line": line_txt,
                "matched_line(å¼·èª¿)": highlighted,
                "excerpt": excerpt,
                "åˆ¤å®š": "ã‚¿ã‚¤ãƒˆãƒ«",
                "rule(ç†ç”±)": "ãã®ä»–â†’ã‚¿ã‚¤ãƒˆãƒ«",
            })
    return captions, refs

# =========================
# å…¨ãƒšãƒ¼ã‚¸èµ°æŸ»
# =========================
page_labels, per_page_rows = [], []
for i, ptxt in enumerate(pages_text, start=1):
    label, matched = extract_single_page_label(ptxt)
    page_labels.append(label)
    per_page_rows.append({
        "pdf_page": i,
        "page_label": label or "-",
        "matched_line": matched or "-",
        "has_label": label is not None,
    })
df_per_page_labels = pd.DataFrame(per_page_rows)

caption_rows, ref_rows = [], []
for i, ptxt in enumerate(pages_text, start=1):
    page_label = page_labels[i-1] if i-1 < len(page_labels) and page_labels[i-1] else "-"
    captions, refs = judge_hits_in_page(ptxt, ctx=ctx_chars)
    for h in captions:
        caption_rows.append({"pdf_page": i, "page_label": page_label, **h})
    for r in refs:
        ref_rows.append({"pdf_page": i, "page_label": page_label, **r})

df_captions = pd.DataFrame(caption_rows)
df_refs     = pd.DataFrame(ref_rows)

# =========================
# è¡¨ç¤º
# =========================
st.subheader("ğŸ“‘ å„ãƒšãƒ¼ã‚¸ã®é ãƒ©ãƒ™ãƒ«ï¼ˆ1é =é«˜ã€…1ï¼‰")
st.dataframe(df_per_page_labels, use_container_width=True)

st.subheader("ğŸ–¼ï¸ å›³/è¡¨/å›³è¡¨ è¦‹å‡ºã—ï¼ˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
st.dataframe(df_captions, use_container_width=True)

# =========================
# å›³è¡¨ç•ªå·ã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡ãƒ»é£›ã³ãƒ»é–‹å§‹ç•ªå·ãƒ»ç¶šãåˆ¤å®šï¼‰
# =========================
st.markdown("### âœ… å›³è¡¨ç•ªå·ã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡ãƒ»é£›ã³ãƒ»é–‹å§‹ï¼‰")

if df_captions.empty or "å›³è¡¨ç•ªå·" not in df_captions.columns:
    st.info("å›³è¡¨ç•ªå·ã®æ¤œæŸ»å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆdf_captions ãŒç©ºã§ã™ï¼‰ã€‚")
    cont_rows, dup_rows = [], []
else:
    
    # æœ«å°¾ã®ï¼ˆä»»æ„ãƒ©ãƒ™ãƒ« + i/nï¼‰ã‚„ï¼ˆç¶šãï¼‰ã‚’é™¤å»ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«æœ¬ä½“ã«ã™ã‚‹
    def _title_base(s: str) -> str:
        if s is None:
            return ""
        t = str(s)
        # ä¾‹ï¼š...(ãƒ ã‚¯ãƒ‰ãƒª1/5) / ...(A 2/3) / ...(ãƒ©ãƒ™ãƒ« ï¼“ï¼ï¼•) ãªã©ï¼ˆå…¨è§’æ‹¬å¼§ãƒ»å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
        t = re.sub(r"[\(ï¼ˆ][^()ï¼ˆï¼‰]*?(\d+)\s*[\/ï¼]\s*(\d+)\s*[\)ï¼‰]\s*$", "", t)
        # æœ«å°¾ã®ï¼ˆç¶šãï¼‰ã‚‚é™¤å»ï¼ˆä»»æ„ï¼‰
        t = re.sub(r"[\(ï¼ˆ]\s*ç¶šã\s*[\)ï¼‰]\s*$", "", t)
        # ç©ºç™½æ­£è¦åŒ–
        t = re.sub(r"\s+", " ", t).strip()
        return t

    # æœ«å°¾æ‹¬å¼§ã‹ã‚‰ (i/n) ã‚’æŠ½å‡ºï¼ˆæ‹¬å¼§å†…ã«ä»»æ„ãƒ©ãƒ™ãƒ«è¨±å®¹ã€å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    def _part_tuple(s: str) -> Tuple[Optional[int], Optional[int]]:
        if not s:
            return (None, None)
        m = re.search(r"[\(ï¼ˆ][^()ï¼ˆï¼‰]*?(\d+)\s*[\/ï¼]\s*(\d+)\s*[\)ï¼‰]\s*$", str(s))
        if not m:
            return (None, None)
        return (int(m.group(1)), int(m.group(2)))


    def _is_continuation_group(g: pd.DataFrame) -> bool:
        """åŒå›³è¡¨ã‚­ãƒ¼ã®è¤‡æ•°è¦‹å‡ºã—ãŒã€ç¶šãã€ã‹ã‚’åˆ¤å®š"""
        if g.shape[0] <= 1:
            return False
        g2 = g.sort_values("pdf_page")
        bases = {_title_base(x) for x in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("")}
        if len(bases) != 1:
            return False
        pages = g2["pdf_page"].dropna().astype(int).tolist()
        if not pages:
            return False
        diffs = [b - a for a, b in zip(pages, pages[1:])]
        if not diffs or max(diffs) != 1:   # å®Œå…¨é€£ç¶š
            return False
        parts = [_part_tuple(t) for t in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("")]
        if any(p[0] is None or p[1] is None for p in parts):
            return True  # (i/n) ãŒç„¡ãã¦ã‚‚é€£ç¶šãªã‚‰ç¶šãæ‰±ã„
        nums = [p[0] for p in parts]
        totals = {p[1] for p in parts}
        if len(totals) != 1:
            return False
        return nums == list(range(min(nums), min(nums)+len(nums)))

   
    # ---- ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«åˆ†é¡ ----
    cont_rows, dup_rows = [], []
    for k, g in df_captions.groupby("å›³è¡¨ã‚­ãƒ¼"):
        if len(g) <= 1:
            continue
        g2 = g.sort_values("pdf_page")
        # --- ç¶šãåˆ¤å®š ---
        if _is_continuation_group(g2):
            cont_rows.append({
                "å›³è¡¨ã‚­ãƒ¼": k,
                # ã“ã“ã‚’ä¿®æ­£ï¼šã™ã¹ã¦ã®è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«ã‚’é€£çµã—ã¦å‡ºåŠ›
                "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": " | ".join([str(x) for x in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("").tolist()]),
                "pdfé ä¸€è¦§": ",".join([str(int(x)) for x in g2["pdf_page"].dropna().astype(int).tolist()]),
                "é ãƒ©ãƒ™ãƒ«ä¸€è¦§": ",".join([str(x) for x in g2["page_label"].fillna("").tolist()]),
                "å‚™è€ƒ": "ï¼ˆç¶šãã®ãƒšãƒ¼ã‚¸ã¨ã¿ãªã™ï¼‰"
            })
        # --- çœŸã®é‡è¤‡ ---
        else:
            dup_rows.append({
                "å›³è¡¨ã‚­ãƒ¼": k,
                "è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§": " | ".join([str(x) for x in g2["è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«"].fillna("").tolist()]),
                "pdfé ä¸€è¦§": ",".join([str(int(x)) for x in g2["pdf_page"].dropna().astype(int).tolist()]),
                "é ãƒ©ãƒ™ãƒ«ä¸€è¦§": ",".join([str(x) for x in g2["page_label"].fillna("").tolist()]),
                "å‚™è€ƒ": "ï¼ˆçœŸã®é‡è¤‡ã®å¯èƒ½æ€§ï¼‰"
            })


    # ç”»é¢è¡¨ç¤º
    if cont_rows:
        st.info("ğŸ”µ ä»¥ä¸‹ã¯ **åŒç•ªå·ã®é€£ç¶šãƒšãƒ¼ã‚¸** ã¨åˆ¤å®šã—ã¾ã—ãŸï¼ˆé‡è¤‡æ‰±ã„ã—ã¾ã›ã‚“ï¼‰ã€‚")
        st.dataframe(pd.DataFrame(cont_rows), use_container_width=True)

    if dup_rows:
        st.warning("ğŸŸ  å›³è¡¨ç•ªå·ã®**çœŸã®é‡è¤‡**ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        st.dataframe(pd.DataFrame(dup_rows), use_container_width=True)
    elif not cont_rows:
        st.success("ğŸŸ¢ å›³è¡¨ç•ªå·ã®é‡è¤‡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    # ---- é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆæ¬ ç•ª/é–‹å§‹1ã§ãªã„ï¼‰----
    def _numeric_core_from_key(key: str) -> str:
        return re.sub(r"^(å›³è¡¨|å›³|è¡¨)", "", key)

    def _series_and_index(key: str) -> Tuple[str, Optional[int], str]:
        m = re.match(r"^(å›³è¡¨|å›³|è¡¨)", key)
        kind = m.group(1) if m else ""
        num_part = _numeric_core_from_key(key)
        if "-" in num_part:
            series, last = num_part.rsplit("-", 1)
        else:
            series, last = "", num_part
        try:
            idx = int(last)
        except Exception:
            idx = None
        return series, idx, kind

    from collections import defaultdict
    series_map = defaultdict(list)  # (kind, series) -> [(idx, key, pdf), ...]
    for _, r in df_captions.iterrows():
        key = str(r.get("å›³è¡¨ã‚­ãƒ¼"))
        pdfp = r.get("pdf_page")
        series, idx, kind = _series_and_index(key)
        if idx is None:
            continue
        series_map[(kind, series)].append((int(idx), key, int(pdfp) if pd.notna(pdfp) else 10**9))

    gap_rows, start_rows = [], []
    for (kind, series), items in series_map.items():
        items_sorted = sorted(items, key=lambda x: (x[0], x[2]))
        idxs = [i for i, _, _ in items_sorted]
        starts_at = idxs[0] if idxs else None
        if starts_at is not None and starts_at != 1:
            start_rows.append({
                "ç¨®åˆ¥": kind,
                "ç³»åˆ—": series or "(å˜ä¸€ç•ªå·)",
                "é–‹å§‹ç•ªå·": starts_at,
                "æœŸå¾…": 1,
                "å­˜åœ¨ç•ªå·": ",".join(map(str, idxs))
            })
        if len(idxs) >= 2:
            missing = []
            for a, b in zip(idxs, idxs[1:]):
                if b - a > 1:
                    missing.extend(range(a+1, b))
            if missing:
                gap_rows.append({
                    "ç¨®åˆ¥": kind,
                    "ç³»åˆ—": series or "(å˜ä¸€ç•ªå·)",
                    "æ¬ ç•ª": ",".join(map(str, missing)),
                    "å­˜åœ¨ç•ªå·": ",".join(map(str, idxs)),
                })

    if not gap_rows and not start_rows:
        st.success("ğŸŸ¢ é€£ç•ªã®é£›ã³ã¯ç„¡ãã€å„ç³»åˆ—ã®é–‹å§‹ç•ªå·ã‚‚ 1 ã«ãªã£ã¦ã„ã¾ã™ã€‚")
    else:
        if start_rows:
            st.warning("ğŸŸ  ç³»åˆ—ã®é–‹å§‹ç•ªå·ãŒ 1 ã§ãªã„ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            st.dataframe(pd.DataFrame(start_rows), use_container_width=True)
        if gap_rows:
            st.warning("ğŸŸ  é€£ç•ªã«æ¬ ç•ªï¼ˆé£›ã³ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            st.dataframe(pd.DataFrame(gap_rows), use_container_width=True)

# ===== æœ¬æ–‡å‚ç…§ã®è¡¨ç¤º =====
st.subheader("ğŸ”— æœ¬æ–‡ä¸­ã® å›³/è¡¨/å›³è¡¨ å‚ç…§ï¼ˆexcerptä»˜ï¼‰")
st.dataframe(df_refs, use_container_width=True)

# =========================
# çªãåˆã‚ã›ï¼šå›³è¡¨è¦‹å‡ºã— â†” æœ¬æ–‡å‚ç…§
# =========================
def _index_pages_by_key(df: pd.DataFrame, key_col: str = "å›³è¡¨ã‚­ãƒ¼") -> dict:
    from collections import defaultdict
    idx = defaultdict(set)
    if df is None or df.empty or key_col not in df.columns:
        return {}
    for _, row in df.iterrows():
        k = row.get(key_col)
        p = row.get("pdf_page")
        if pd.notna(k) and pd.notna(p):
            try:
                idx[str(k)].add(int(p))
            except Exception:
                pass
    return {k: sorted(v) for k, v in idx.items()}

cap_idx = _index_pages_by_key(df_captions)
ref_idx = _index_pages_by_key(df_refs)

cap_keys = set(cap_idx.keys())
ref_keys = set(ref_idx.keys())

missing_in_refs = sorted(cap_keys - ref_keys)        # è¦‹å‡ºã—ã¯ã‚ã‚‹ãŒå‚ç…§ãŒãªã„
missing_in_captions = sorted(ref_keys - cap_keys)    # å‚ç…§ã¯ã‚ã‚‹ãŒè¦‹å‡ºã—ãŒãªã„

all_captions_referenced  = (len(missing_in_refs) == 0)
has_refs_without_caption = (len(missing_in_captions) > 0)

st.subheader("ğŸ” çªãåˆã‚ã›çµæœï¼ˆè¦‹å‡ºã— â†” å‚ç…§ï¼‰")

c1, c2 = st.columns(2)
with c1:
    st.markdown("**â‘  å…¨ã¦ã®å›³è¡¨è¦‹å‡ºã—ãŒæœ¬æ–‡ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ**")
    st.write("â†’ **{}**".format("ã¯ã„ï¼ˆå…¨ã¦å¼•ç”¨ã‚ã‚Šï¼‰âœ…" if all_captions_referenced else "ã„ã„ãˆï¼ˆæœªå¼•ç”¨ã‚ã‚Šï¼‰âš ï¸"))
with c2:
    st.markdown("**â‘¡ æœ¬æ–‡ã«å‚ç…§ãŒã‚ã‚‹ãŒè¦‹å‡ºã—ãŒç„¡ã„ã‚‚ã®ã¯ã‚ã‚‹ã‹ï¼Ÿ**")
    st.write("â†’ **{}**".format("ã¯ã„ï¼ˆè¦‹å‡ºã—ãªã—ã®å‚ç…§ã‚ã‚Šï¼‰âš ï¸" if has_refs_without_caption else "ã„ã„ãˆï¼ˆå…¨ã¦è¦‹å‡ºã—ã‚ã‚Šï¼‰âœ…"))

# ç”»é¢è¡¨ç¤ºç”¨ï¼šå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹è¦‹å‡ºã—
def _ref_aggregate_for_view(df_refs: pd.DataFrame):
    if df_refs is None or df_refs.empty:
        return {}, {}, {}, {}
    ref_lbls, ref_pdfs, ref_texts, ref_hi = {}, {}, {}, {}
    for k, grp in df_refs.groupby("å›³è¡¨ã‚­ãƒ¼"):
        grp2 = grp.sort_values(["pdf_page", "è¡Œç•ªå·"], kind="mergesort")
        lbls = grp2["page_label"].dropna().astype(str).unique().tolist()
        pnums = grp2["pdf_page"].dropna().astype(int).unique().tolist()
        texts = grp2["è¡Œãƒ†ã‚­ã‚¹ãƒˆ"].dropna().astype(str).unique().tolist()
        his   = grp2["è¡Œãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)"].dropna().astype(str).unique().tolist()
        ref_lbls[k] = ",".join(lbls)
        ref_pdfs[k] = ",".join(str(x) for x in pnums)
        ref_texts[k] = " | ".join(texts)
        ref_hi[k]    = " | ".join(his)
    return ref_lbls, ref_pdfs, ref_texts, ref_hi

ref_lbls, ref_pdfs, ref_texts, ref_hi = _ref_aggregate_for_view(df_refs)

def _first_caption_row(df_cap: pd.DataFrame, key: str):
    grp = df_cap[df_cap["å›³è¡¨ã‚­ãƒ¼"] == key]
    if grp.empty:
        return {"å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": "", "é ": "", "pdfé ": 10**9}
    row = grp.sort_values("pdf_page").iloc[0]
    return {
        "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": row.get("è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«", "") or "",
        "é ": row.get("page_label", "") or "",
        "pdfé ": int(row.get("pdf_page")) if pd.notna(row.get("pdf_page")) else 10**9,
    }

referenced_keys = sorted(set(cap_idx.keys()) & set(ref_idx.keys()))
rows = []
for k in referenced_keys:
    ci = _first_caption_row(df_captions, k)
    rows.append({
        "å›³è¡¨ã‚­ãƒ¼": k,
        "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": ci["å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«"],
        "é ": ci["é "],
        "pdfé ": ci["pdfé "],
        "å‚ç…§é ãƒ©ãƒ™ãƒ«": ref_lbls.get(k, ""),
        "å‚ç…§pdfé ": ref_pdfs.get(k, ""),
        "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": ref_texts.get(k, ""),
        "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)": ref_hi.get(k, ""),
        "_sort": ci["pdfé "],
    })

df_referenced_view = (
    pd.DataFrame(
        rows,
        columns=["å›³è¡¨ã‚­ãƒ¼","å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«","é ","pdfé ","å‚ç…§é ãƒ©ãƒ™ãƒ«","å‚ç…§pdfé ","å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)","_sort"]
    )
    .sort_values("_sort", kind="mergesort")
    .drop(columns=["_sort"])
)

st.markdown("#### ğŸ”µ å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹è¦‹å‡ºã—ï¼ˆè¦‹å‡ºã—ã‚ã‚Šï¼‹æœ¬æ–‡å‚ç…§ã‚ã‚Šï¼‰")
st.dataframe(df_referenced_view, use_container_width=True)

st.markdown("#### ğŸŸ  æœªå¼•ç”¨ã®è¦‹å‡ºã—ï¼ˆè¦‹å‡ºã—ã¯ã‚ã‚‹ãŒæœ¬æ–‡å‚ç…§ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼‰")
df_missing_caption_refs = pd.DataFrame(
    [{"å›³è¡¨ã‚­ãƒ¼": k, "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸": cap_idx.get(k, []), "å‚ç…§ãƒšãƒ¼ã‚¸": ref_idx.get(k, [])}
     for k in missing_in_refs]
)
st.dataframe(df_missing_caption_refs, use_container_width=True)

st.markdown("#### ğŸ”µ è¦‹å‡ºã—ãªã—ã®å‚ç…§ï¼ˆæœ¬æ–‡ã«å‚ç…§ã¯ã‚ã‚‹ãŒå¯¾å¿œã™ã‚‹è¦‹å‡ºã—ãŒç„¡ã„ï¼‰")
df_orphan_refs = pd.DataFrame(
    [{"å›³è¡¨ã‚­ãƒ¼": k, "å‚ç…§ãƒšãƒ¼ã‚¸": ref_idx.get(k, []), "è¦‹å‡ºã—ãƒšãƒ¼ã‚¸": cap_idx.get(k, [])}
     for k in missing_in_captions]
)
st.dataframe(df_orphan_refs, use_container_width=True)

# =========================
# CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================
with st.sidebar:
    st.markdown("### CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    def _protect_for_excel_csv(x: object) -> object:
        if isinstance(x, str) and re.match(r"^\s*\d{1,2}\s*[-âˆ’ãƒ¼ï¼â€•]\s*\d{1,2}\s*$", x.strip()):
            return f'="{x.strip()}"'
        return x
    for df, name in [
        (df_per_page_labels, "per_page_labels.csv"),
        (df_captions, "figure_table_captions.csv"),
        (df_refs, "figure_table_references.csv"),
    ]:
        if not df.empty:
            df2 = df.copy()
            if "page_label" in df2.columns:
                df2["page_label"] = df2["page_label"].map(_protect_for_excel_csv)
            buf = io.StringIO()
            df2.to_csv(buf, index=False)
            st.download_button(f"ğŸ“¥ {name}",
                               data=buf.getvalue().encode("utf-8-sig"),
                               file_name=name,
                               mime="text/csv",
                               use_container_width=True)

# =========================
# XLSXï¼ˆçªãåˆã‚ã›ï¼‹é‡è¤‡/ç¶šãï¼‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================
from io import BytesIO

def _caption_info_first_by_key(df: pd.DataFrame) -> dict:
    info = {}
    if df is None or df.empty:
        return info
    for k, grp in df.groupby("å›³è¡¨ã‚­ãƒ¼"):
        row = grp.sort_values("pdf_page").iloc[0]
        info[str(k)] = {
            "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": row.get("è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«", "") or "",
            "é ": row.get("page_label", "") or "",
            "pdfé ": int(row.get("pdf_page")) if pd.notna(row.get("pdf_page")) else "",
        }
    return info

def _protect_for_excel(x: object) -> object:
    if isinstance(x, str) and re.match(r"^\s*\d{1,2}\s*[-âˆ’ãƒ¼ï¼â€•]\s*\d{1,2}\s*$", x.strip()):
        return f'=\"{x.strip()}\"'
    return x

# å‚ç…§å´æƒ…å ±é›†ç´„
def _aggregate_ref_info(df_refs: pd.DataFrame):
    if df_refs is None or df_refs.empty:
        return {}, {}, {}, {}
    ref_page_labels, ref_pdf_pages, ref_texts, ref_highlight_texts = {}, {}, {}, {}
    for k, grp in df_refs.groupby("å›³è¡¨ã‚­ãƒ¼"):
        grp2 = grp.sort_values(["pdf_page", "è¡Œç•ªå·"], na_position="last", kind="mergesort")
        labels = [str(x) for x in grp2["page_label"].dropna().unique().tolist()]
        pdfs = [str(int(x)) for x in grp2["pdf_page"].dropna().unique().tolist()]
        ref_page_labels[k] = ",".join(labels)
        ref_pdf_pages[k] = ",".join(pdfs)
        ref_texts[k] = " | ".join(grp2["è¡Œãƒ†ã‚­ã‚¹ãƒˆ"].dropna().astype(str).unique().tolist())
        ref_highlight_texts[k] = " | ".join(grp2["è¡Œãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)"].dropna().astype(str).unique().tolist())
    return ref_page_labels, ref_pdf_pages, ref_texts, ref_highlight_texts

cap_info = _caption_info_first_by_key(df_captions)
ref_page_labels, ref_pdf_pages, ref_texts, ref_highlight_texts = _aggregate_ref_info(df_refs)

def _make_rows(keys, caption_src=True):
    rows = []
    for k in keys:
        ci = cap_info.get(k, {"å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": "", "é ": "", "pdfé ": ""}) if caption_src else {"å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": "", "é ": "", "pdfé ": ""}
        rows.append({
            "å›³è¡¨ã‚­ãƒ¼": k,
            "å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«": ci["å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«"],
            "é ": _protect_for_excel(ci["é "]),
            "pdfé ": ci["pdfé "],
            "å‚ç…§é ãƒ©ãƒ™ãƒ«": ref_page_labels.get(k, ""),
            "å‚ç…§pdfé ": ref_pdf_pages.get(k, ""),
            "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ": ref_texts.get(k, ""),
            "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)": ref_highlight_texts.get(k, ""),
            "_sort_pdf": ci["pdfé "] if caption_src and ci["pdfé "] != "" else (
                min([int(x) for x in ref_pdf_pages.get(k, "").split(",") if x.isdigit()] or [10**9])
            ),
        })
    df = pd.DataFrame(rows, columns=[
        "å›³è¡¨ã‚­ãƒ¼","å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«","é ","pdfé ","å‚ç…§é ãƒ©ãƒ™ãƒ«","å‚ç…§pdfé ","å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ","å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ(å¼·èª¿)","_sort_pdf"
    ])
    return df.sort_values("_sort_pdf", kind="mergesort").drop(columns=["_sort_pdf"], errors="ignore")

referenced_keys = sorted(set(cap_idx.keys()) & set(ref_idx.keys()))
df_referenced_captions_x = _make_rows(referenced_keys, caption_src=True)
df_missing_caption_refs_x = _make_rows(sorted(missing_in_refs), caption_src=True)
df_orphan_refs_x = _make_rows(sorted(missing_in_captions), caption_src=False)

# çœŸã®é‡è¤‡ & ç¶šãåˆ¤å®šã®DFï¼ˆä¸Šã®è¨ˆç®—çµæœã‚’ãã®ã¾ã¾åˆ©ç”¨ï¼‰
dup_df = pd.DataFrame(dup_rows) if dup_rows else pd.DataFrame(
    columns=["å›³è¡¨ã‚­ãƒ¼","è¦‹å‡ºã—ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§","pdfé ä¸€è¦§","é ãƒ©ãƒ™ãƒ«ä¸€è¦§","å‚™è€ƒ"]
)
cont_df = pd.DataFrame(cont_rows) if cont_rows else pd.DataFrame(
    columns=["å›³è¡¨ã‚­ãƒ¼","å›³è¡¨ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæœ¬ä½“ï¼‰","pdfé ä¸€è¦§","é ãƒ©ãƒ™ãƒ«ä¸€è¦§","å‚™è€ƒ"]
)

with st.sidebar:
    st.markdown("### ğŸ”— çªãåˆã‚ã›çµæœï¼ˆXLSXï¼‰")
    output = BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        # ã‚µãƒãƒªãƒ¼
        pd.DataFrame({
            "é …ç›®": ["å…¨ã¦å¼•ç”¨æ¸ˆã¿", "è¦‹å‡ºã—ãªã—å‚ç…§ã‚ã‚Š"],
            "çµæœ": [
                "ã¯ã„ï¼ˆå…¨ã¦å¼•ç”¨ã‚ã‚Šï¼‰âœ…" if all_captions_referenced else "ã„ã„ãˆï¼ˆæœªå¼•ç”¨ã‚ã‚Šï¼‰âš ï¸",
                "ã¯ã„ï¼ˆè¦‹å‡ºã—ãªã—å‚ç…§ã‚ã‚Šï¼‰âš ï¸" if has_refs_without_caption else "ã„ã„ãˆï¼ˆå…¨ã¦è¦‹å‡ºã—ã‚ã‚Šï¼‰âœ…",
            ]
        }).to_excel(writer, sheet_name="ã‚µãƒãƒªãƒ¼", index=False)

        # çªãåˆã‚ã› 3 ã‚·ãƒ¼ãƒˆ
        df_referenced_captions_x.to_excel(writer, sheet_name="å¼•ç”¨è¦‹å‡ºã—", index=False)
        df_missing_caption_refs_x.to_excel(writer, sheet_name="æœªå¼•ç”¨è¦‹å‡ºã—", index=False)
        df_orphan_refs_x.to_excel(writer, sheet_name="è¦‹å‡ºã—ãªã—å‚ç…§", index=False)

        # é‡è¤‡/ç¶šã åˆ¤å®š 2 ã‚·ãƒ¼ãƒˆï¼ˆã”è¦æœ›è¿½åŠ ï¼‰
        dup_df.to_excel(writer, sheet_name="é‡è¤‡ï¼ˆç–‘ã„ï¼‰", index=False)
        cont_df.to_excel(writer, sheet_name="ç¶šãåˆ¤å®š", index=False)

        # é€£ç•ªãƒã‚§ãƒƒã‚¯ï¼ˆä»»æ„ã§å‡ºã™ãªã‚‰ï¼‰
        if 'start_rows' in locals():
            (pd.DataFrame(start_rows) if start_rows else pd.DataFrame(columns=["ç¨®åˆ¥","ç³»åˆ—","é–‹å§‹ç•ªå·","æœŸå¾…","å­˜åœ¨ç•ªå·"])
             ).to_excel(writer, sheet_name="é–‹å§‹ç•ªå·ãƒã‚§ãƒƒã‚¯", index=False)
        if 'gap_rows' in locals():
            (pd.DataFrame(gap_rows) if gap_rows else pd.DataFrame(columns=["ç¨®åˆ¥","ç³»åˆ—","æ¬ ç•ª","å­˜åœ¨ç•ªå·"])
             ).to_excel(writer, sheet_name="æ¬ ç•ªãƒã‚§ãƒƒã‚¯", index=False)

    st.download_button(
        "ğŸ“˜ çªãåˆã‚ã›çµæœï¼ˆå¼•ç”¨/æœªå¼•ç”¨/è¦‹å‡ºã—ãªã—å‚ç…§/é‡è¤‡/ç¶šã/é€£ç•ªï¼‰.xlsx ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=output.getvalue(),
        file_name="figure_table_crosscheck.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )

# =========================
# ãƒ‡ãƒãƒƒã‚°
# =========================
if show_debug:
    st.divider()
    st.markdown("### ğŸ§ª Debug")
    st.code(f"EXTRACT_RE = {EXTRACT_RE.pattern}")
    st.caption("ãƒãƒƒãƒã‚’å«ã‚€1è¡ŒæŠ½å‡ºï¼‹âŸªå¼·èª¿âŸ«ï¼‹excerptä»˜ãã€‚")
